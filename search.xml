<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[paperMemo-图片质量评价指标]]></title>
    <url>%2Farticle%2Fe3c77110%2F</url>
    <content type="text"><![CDATA[SSIM]]></content>
      <categories>
        <category>Scientific Research</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paperMemo-3dShapeMeasurement]]></title>
    <url>%2Farticle%2F4b571488%2F</url>
    <content type="text"><![CDATA[主题本次调研主题是 “3D形状测量” 学科融合一、摄影测量 摄影测量是指运用摄影机和胶片组合测量方针物的形状、大小和空间位置的技术，它使用光学摄影机获取的像片，经过处理以获取被摄物体的形状、大小、位置、特性及其相互关系。 ​ 重视的是几何量的量测信息（物体的位置、大小和形状等），首要任务是用于测绘各种比例尺的地形图、树立数字地面模型，为各种地理信息体系和土地信息体系供给基础数据。摄影测量学要解决的两大问题是几何定位和影像解译。几何定位就是断定被摄物体的大小、形状和空间位置。几何定位的根本原理源于测量学的前方交会办法，它是根据两个已知的摄影站点和两条已知的摄影方向线，交会出构成这两条摄影光线的待定地面点的三维坐标。影像解译就是断定影像对应地物的性质。 当被测物体的尺寸或摄影间隔小于100米时的摄影测量称之为近景摄影测量。随着数字传感器技术的发展，尤其是CCD器件和CMOS器件的迅速发展，使用CCD(或CMOS)像机不需要胶片就可直接获得被测物的数字影像，这种直接基于数字影像的近景摄影测量称为数字近景摄影测量。 二、计算机视觉测量 计算机视觉是使用计算机及相关设备对生物视觉的一种模仿。 ​ 首要重视的是对物体进行描绘、识别和理解，它的首要任务就是经过对采集的图片或视频进行处理以获得相应场景的三维信息，就像人类和许多其他类生物每天所做的那样。是一门关于怎么运用照相机和计算机来获取咱们所需的，被摄影对象的数据与信息的学识。形象地说，就是给计算机设备上眼睛（照相机）和大脑（算法），让计算机能够感知环境。 机器视觉体系是计算机学科的一个分支，是指经过机器视觉产品（即图画吸取设备，分CMOS和CCD两种）将被吸取方针转换成图画信号，传送给专用的图画处理体系，根据像素分布和亮度、颜色等信息，改变成数字化信号；图画体系对这些信号进行各种运算来抽取方针的特征，进而根据判别的成果来操控现场的设备动作。 三、摄影测量和视觉测量的差异 1、起点不同导致根本参数物理含义的差异：摄影测量中的外部定向是断定影像在空间相对于物体的位置与位置（将物体先平移再旋转），而计算机视觉则是物体相对于影像的位置与位置来描绘问题（将摄像机先旋转再平移）。 2、因为两者不同的起点导致根本公式的差异：摄影测量中最为根本的是共线方程，而视觉测量中最为根本的公式是用齐次坐标表明的投影方程。 3、数学处理算法的不同：摄影测量渊源于测绘学科，基于非线性迭代的最小二乘法平差求解贯穿于数字近景摄影测量的全过程，而计算机视觉着重矩阵分解，总是设法将非线性问题转换为线性问题，尽可能避免求解非线性方程。 四 现有商业化测量产品新拓三维 SmartRay 五 研究趋势 虽然数字近景摄影测量与计算机视觉有各种各样的差异，但在重视点方面，和理论基础方面是共同的，并且随着最近20年的发展，人工智能，智能城市，大数据等在各个范畴的应用，让一切都有了不同的改变。 ​ 学术会议和出版论文集等交流方式让学科间的交流逐渐增加，两个学科的交叉也越来越多。比方，数字近景摄影测量中的许多根本概念与办法来自影像处理与计算机视觉（如数字图画处理的某些算法、编码标志的自动识别）；反过来，摄影测量中的一些特征理论和办法又为视觉测量所采用（如全体光束法平差算法、像机自标定原理和办法等），而两者的结合也给学科及人类科技发展带来了协助。所以，视觉系统和摄影测量这两种学科逐渐相互融合并优势互补是发展的必然趋势。 文献2010 Fast Phase-based Stereo Matching Method for 3D Shape Measurement作者：Dong Li ; Huijie Zhao ; Hongzhi Jiang 来源： 2010 International Symposium on Optomechatronic Technologies文章链接：https://ieeexplore.ieee.org/document/5687348源码链接：无 方法： 三维形状测量模型，连同校准板，如图所示。DLP投影机的分辨率是1024x768。两台黑白CCD相机的分辨率都是2048x2048。校准板由铝合金制成。由于标定板具有较高的平整度表面，因此可以利用标定板的三维重建来评价三维形状测量精度。 提出了一种基于相位的立体匹配方法，该方法利用极线校正，将双立体几何变换为极标准几何。利用相移技术计算相位，通过沿水平极线搜索最近的相位，简化了立体匹配。（类似于双目测距） 利用DLP投影仪将正弦条纹图形投影到物体表面，利用两台CCD相机捕捉变形的条纹图形图像。 实施极线矫正。极线校正是基于线性针孔模型,根据公式构造极线矫正前后像平面坐标对应关系 采用双线性插值方法计算经过校正的绝对相位图 进行立体匹配。对于左侧图像上的一个点，利用所描述的外极标准几何特征，沿水平外极线搜索最近的相位，即右侧图像上具有相同垂直坐标的一条直线。 结果： 贡献： 提出了一种利用极线校正的快速相位立体方法。与之前的方法相比，该方法将立体匹配时间缩短了20%。在高平整度平面的三维重建中，三维形状测量精度可达0.0178mm。因此，该方法适用于速度快、精度高的应用场合。 缺点： 重建效果较差，丢失了较多的细节 2019 A Calibration Method of 3D Shape Measurement System Using 3D Scanner, Turn-table and Arm-robot作者： Hiroyuki Ukida ; Tomoyo Sasao ; Kenji Terada 来源： 2019 58th Annual Conference of the Society of Instrument and Control Engineers of Japan (SICE)文章链接：https://ieeexplore.ieee.org/document/8860071源码链接：暂无 方法： 所提出的系统是由一个三维扫描仪，一个手臂机器人和一个转台。 为了准确地测量三维形状，提出了一种测量系统设备位移的方法来校准。 固定机械手臂与转台中心的距离 对于臂-机器人坐标系的每个轴，臂-机器人与已知位移的轴平行移动。这个动作在给定的时间内迭代。 以已知角度旋转转台，每次移动时用3D扫描仪估计球体标记的中心前沿的坐标。 得到“球中心轨迹坐标”。从球面标记的测量中心坐标分布出发，用最小二乘法估计一个由中心坐标支持的平面。该平面为转台的参考平面，其表面法向量为转台的旋转轴。 将所有球面标记中心坐标投影到估计的参考平面，并从投影坐标估计该平面上的圆。估算出投影轨迹坐标的直线，然后根据直线的倾斜度估算出每个轴的旋转角度。将三维扫描仪坐标系中的一个坐标转换为arm-robot坐标系中的坐标 坐标映射，然后将各个面结合到同一个坐标中。 结果： 贡献： 针对传统木偶形状三维测量系统 提出了一种三维形状测量系统的标定方法。 缺点： 由于操作人员和操作环境的影响，手动操作可能会产生较大的误差 所估计的深度图中也会影响位置估计 2020 TOWARDS UNDERSTANDING SPECIATION BY AUTOMATED EXTRACTION AND DESCRIPTION OF 3D FORAMINIFERA（有孔虫类） STACKS作者： Wenshu Zhang ; Thomas Ezard ; Alex Searle-Barnes ; 来源：2020 IEEE Southwest Symposium on Image Analysis and Interpretation (SSIAI)文章链接： https://ieeexplore.ieee.org/document/9094611源码链接：暂无 方法： 在分析浮游有孔虫标本时，巨大的三维数据量限制了对遗传物种形成的理解，提出了一种端到端的神经网络架构来解决这一问题。观察到的化石是浮游有孔虫，这是一种单细胞生物，大量生活在世界海洋中。每个孔的大小和形状在其生命旅程的每个阶段都有完整的记录。在本研究中，我们分析了各种个体有孔虫来研究它们之间的差异，并与人工标记的ground truth进行比较。从图像序列中自动重建每个样本的独立腔室，(ii)使用形状特征来描述不同类型的物种。通过处理包含9GB点的3D样本的数据集，表明物种形成现在确实可以被分析，从形态学特征的自动分析导致了对生命起源的新见解。 结果： 贡献： 利用计算机视觉演算法来重建有孔虫形状，可视化断层扫描的内部特征，分析形状意味着理解空间排列，以了解影响物种形成的重要因素。 提出了一个端到端自动测量有孔虫形状的框架。 缺点： 标本的数量和形态的多样性不够，导致易出现预估错误。 待看 3D Shape Measurement of Translucent Objects Using Laser Rangefinder （半透明形状） Improving 3D reconstruction accuracy in wavelet transform profilometry by reducingshadow effects （提高精度） 3D Environment Measurement and Reconstruction Based on LiDAR （利用激光雷达测量环境） Reconstruction of Realistic 3D Surface Model and 3D Animation from Range Images Obtained by Real Time 3D Measurement System ECG: Edge-aware Point Cloud Completion with Graph Convolution（2020，图神经网络生成点云边缘） PointGrow: Autoregressively Learned Point Cloud Generation with Self-Attention（2020，点云自我生成）]]></content>
      <categories>
        <category>Scientific Research</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-GAN]]></title>
    <url>%2Farticle%2Ff3e3799%2F</url>
    <content type="text"><![CDATA[理解生成器与判别器相互博弈的过程。 经过多次反复训练迭代之后，最终希望能够达到生成样本分布拟合于真实样本分布的状态，并且判别器分辨不出样本是生成的还是真实的（判别概率均为0.5） 生成模型大概有两种玩法： 密度（概率）估计：就是说在不了解事件概率分布的情况下，先假设随机分布，然后通过数据观测来确定真正的概率密度是怎样的。 样本生成：这个就更好理解了，就是手上有一把训练样本数据，通过训练后的模型来生成类似的「样本」。 在应用上，这套 GAN 理论最火的构架是 DCGAN（深度卷积生成对抗网络/Deep Convolutional Generative Adversarial Network），反卷积，DCGAN目的是创造图片，其实就类似于把一组特征值慢慢恢复成一张图片。 参考链接通俗理解生成对抗网络GAN 交叉熵损失函数 GAN学习指南：从原理入门到制作生成Demo 独家 | GAN之父NIPS 2016演讲现场直击：全方位解读生成对抗网络的原理及未来（附PPT）]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-强化学习]]></title>
    <url>%2Farticle%2Fda29b4e7%2F</url>
    <content type="text"><![CDATA[强化学习基本概念强化学习被认为是实现人工智能的通用框架 强化学习使得机器人具备在环境中自主动作的能力 每个动作都能影响机器人在环境中的状态 动作的好坏由状态对应的激励信号决定 累计激励： 目标：学习选择动作的策略使得将来的累计激励最大化 监督学习与强化学习监督学习目标 f(x-&gt;y)x:数据 y:预测目标存在y的标注信息且y对x无影响 强化学习目标 π(s-&gt;a)s:状态 a:动作不存在a的标注信息且a对s有影响 V、Q函数V函数衡量了机器人在状态s期望获得的累计激励 Q函数衡量了机器人在状态s采取动作a之后期望获得的累计激励 Policy函数Policy函数代表某种选择动作的策略π(s)，或条件概率分布π(a|s)，负责根据状态s选择最优的动作a a=π(s) 基于Q函数的策略：• 选择动作使得累计激励的期望最大 π*(s) = argmaxQ(s,a) 强化学习的主要模式Value-based RL(基于价值的) 学习Q函数Q(s,a) 选择使Q函数取最大值的动作 Policy-based RL(基于策略的) 学习Policy函数π(a|s) 从Policy函数采样动作 贝尔曼方程 Q函数的递归关系采用贝尔曼方程 【缺图】 基于Q函数的强化学习的弊端复杂性方面：• 只能对简单离散的动作空间进行建模• 无法应对连续的动作空间(例如：方向盘的转动角度)灵活性方面：• 选择动作的策略Policy由Q函数直接决定（硬策略）• 无法对策略的随机性进行建模 深层策略网络Deep Q-Network: 先学习Q函数，然后估计最佳策略Policy Gradient: 直接学习最佳策略 训练过程： 基于当前策略运行一段时间(直至产生激励) 若为高激励，则增加选择当前动作的概率 若为低激励，则降低选择当前动作的概率 模拟学习基于带标签的数据训练策略网络 优点：网络收敛快 缺点：探索能力不足，容易过拟合 反（逆向）强化学习强化学习：激励函数已知 反强化学习：激励函数未知，结合专家示例进行学习（与对抗学习GAN存在本质关联） 策略对应生成器 激励对应鉴别器]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-聚类]]></title>
    <url>%2Farticle%2Fbe746f28%2F</url>
    <content type="text"><![CDATA[基本概念在“无监督学习” 任务中研究最多、 应用最广。 聚类目标：将数据集中的样本划分为若干个通常不相交的子集（“簇” ， cluster） 聚类既可以作为一个单独过程（用于找寻数据内在的分布结构） ， 也可作为分类等其他学习任务的前驱过程。 应用：1，分析数据内在结构；2，作为有监督学习的预处理部分，发掘数据的隐藏模式 监督信号类别 任务标注 中间激励 无 输出 离散 分类 聚类 连续 回归 降维 策略 模仿学习 强化学习 聚类性能指标（聚类结果的“簇内相似度”（intra-cluster similarity） 高，且“簇间相似度”（inter-clustersimilarity）低 ） 外部指标：将聚类结果与某个“参考模型”（只用于测试评判） 进行比较 内部指标（划分依据）：基于簇内和簇间相似度直接考察聚类结果 a=SS （一致）、b=SD（不一致） c=DS、d=DD（若一致） 外部指标 ([0,1]区间内,越大越好 ) Jaccard系数（Jaccard Coefficient, JC） FM指数(Fowlkes and Mallows Index, FMI) Rand指数（Rand Index, RI） 内部指标 考虑聚类结果的簇C划分 ，定义：簇 C内样本间的平均距离(簇内距离) 、簇 C 内样本间的最远距离(簇内距离)、簇C_i与簇 C_j最近样本间的距离(簇间距离)、簇 C_i与簇 C_j中心点间的距离(簇间距离) DB指数(Davies-Bouldin Index, DBI) (簇内距离) Dunn指数(Dunn Index, DI) (簇间距离) 距离度量性质:非负性、同一性、对称性、直递性 闵可夫斯基距离(Minkowski distance)p=2: 欧氏距离(Euclidean distance).p=1： 曼哈顿距离(Manhattan distance) 分类基于原型的聚类(Prototype-based clustering)• 主要特点：先对原型进行初始化，然后进行迭代更新• 常见算法： k-Means算法，高斯混合模型基于密度的聚类(Density-based clustering)• 主要特点：从样本密度的角度来考察样本间的关系• 常见算法： DBSCAN算法层次聚类(Hierarchical clustering)• 主要特点：在不同层次对样本进行划分，形成树形的聚类结构• 常见算法： 逐次聚合算法 k-Means算法E值在一定程度上刻画了簇内样本围绕簇均值向量的紧密程度，值越小，则簇内样本相似度越高。 算法流程(迭代优化)：人工选取簇的个数，初始化每个簇的均值向量(中心)repeat 簇划分(聚类)； 更新每个簇的中心 until 当前均值向量均未更新 不足： 簇的数目k需要人为指定；对簇中心的初始化比较敏感；破坏数据内在结构 ps: 硬聚类：样本只从属于一个簇，如K-means 软聚类：样本以该概率从属于多个簇,如高斯混合模型、EM模型 混合模型是概率化的k-Means算法 基于密度的聚类 DBSCAN算法（举例）簇的定义：密度相连样本组成的集 算法的核心：由核心对象搜寻密度可达的所有样本 Agglomerative(逐次聚合)算法算法过程(自底向上的层次聚类)：• 首先，将样本中的每一个样本看做一个初始聚类簇；• 然后在算法运行的每一步中找出距离最近的两个聚类簇进行合并，该过程不断重复，直到达到预设的聚类簇的个数。 • 基于层次的聚类• 生成不同层次的簇-树状图]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-领域自适应]]></title>
    <url>%2Farticle%2F2d45afe4%2F</url>
    <content type="text"><![CDATA[迁移学习(transfer learning)给定源域的数据和任务，提高目标域的任务性能 领域自适应(domain adaptation)• 任务相同• 数据域不同(光照，姿势，图像风格)• 源域数据有标签， 目标域数据没有标签(或标签很少) 易混淆概念无监督领域自适应 Unsupervised DA：• 源域有标签，目标域没有标签监督领域自适应 Supervised DA (fine-tune)：• 源域有标签，目标域有少量标签无监督迁移学习 Unsupervised transfer learning：• 源域和目标域都没有标签，比较少见半监督学习 Semi-supervised learning：• 没有数据域或任务的差别• 来源于同一个数据域的少量标签和大量无标签数据 领域自适应方法分类基于样本的方法 Instance-based method：• 改变源域标签数据的样本权重• 与目标域数据相近的源域数据有更高的权重 基于特征的方法 Feature-based method：• 将源域与目标域数据映射到共通的特征空间 提取源域和目标域的共通特征• 部分特征是和数据域相关的• 其它特征是数据域之间共通的 基于伪标签的方法 Pseudo-label-based method：• 以目标域数据中可信度高的预测结果作为伪标签来训练模型 伪标签• 目标域中可信度高的预测通常是对的• 可信度比较高的样本可以作为伪标签对模型进行自训练]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-RNN（比较浅）]]></title>
    <url>%2Farticle%2Fc91fc4b%2F</url>
    <content type="text"><![CDATA[面对序列信号（语音、视频）如何建模无法保留时序信息； 无法建模长期依赖关系 ； 通用性差； 序列模型需要满足：• 可处理变长输入序列• 能够学习长期的依赖关系• 保留序列本身的时序信息 RNNRecurrent Neural Network(递归神经网络) 误差函数对于ℎ0的梯度包含𝑊ℎℎ和非线性激活函数的多次方计算：• 𝑊ℎℎ过大导致梯度爆炸(exploding gradient)• 𝑊ℎℎ过小会导致梯度消失(vanishing gradient) 思路：设计更复杂的递归单元(recurrent unit/cell)来保留长期关系]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[环境配置版本备忘录]]></title>
    <url>%2Farticle%2F724e4936%2F</url>
    <content type="text"><![CDATA[Labtop深度学习环境 NVIDIA Driver Version: 430.39 cuda 10.1 cudann 7.6.4 python 3.7.5 参考文章：Pytorch在win10下搭建cuda版本的环境 ​ Pytorch在Windows下的环境搭建以及模型训练 Desktop(in laboratory)]]></content>
      <categories>
        <category>tiny knowledge</category>
      </categories>
      <tags>
        <tag>tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-决策树]]></title>
    <url>%2Farticle%2Fae9520f1%2F</url>
    <content type="text"><![CDATA[剪枝分类器剪枝分类器是指，对于d次维的输入变量x，任意选定其中一维，通过将其与给定阈值相比较来进行分类的线性分类器 剪枝分类器的要素：• 一个输入维度• 一个阈值 决策树决策树是最符合人类决策机制的模型之一 决策树： 剪枝分类器经过一层一层累积，形成树状的结构 • 分叉节点：剪枝分类器• 叶节点：判断结果 预测流程 数据样本从根节点进入决策树 数据节点在分支节点进行判定，根据判定结果导入该节点的子节点 数据样本若到达分支节点，重复第2步；若到达叶节点，得到最终判定结果 叶节点的输出判定• 每个叶节点保存了一部分训练数据的子集• 训练数据子集的类型分布决定了叶节点的输出概率 决策树学习决策树学习过程就是树枝不断分裂长成大树的过程 （剪枝分类器的组合） 决策树学习：不断根据属性划分训练数据的过程 关键在于如何选择最优划分属性。一般而言，随着划分过程不断进行，我们希望经过分支结点划分后的样本尽可能属于同一类别，即“纯度” (purity)越来越高 经典的属性划分方法：– 信息增益– 增益率– 基尼指数 评价指标-信息熵“信息熵” 是度量样本集合纯度最常用的一种指标，假定当前样本集合D中第k类样本所占的比例为 ， 则的信息熵定义为 信息增益 增益率基尼指数剪枝处理 为什么剪枝– “剪枝” 是决策树学习算法对付“过拟合” 的主要手段– 可通过“剪枝” 来一定程度避免因决策分支过多， 以致于把训练集自身的一些特点当做所有数据都具有的一般性质而导致的过拟合 剪枝的基本策略– 预剪枝– 后剪枝 剪枝依据-决策树泛化性能– 留出法：预留一部分数据用作“验证集” 以进行性能评估– 交叉验证法 预剪枝 决策树生成过程中， 对每个结点在划分前先进行估计， 若当前结点的划分不能带来决策树泛化性能提升， 则停止划分并将当前结点记为叶结点， 其类别标记为训练样例数最多的类别 预剪枝优缺点 优点– 降低过拟合风险– 显著减少训练时间和测试时间开销 缺点– 欠拟合风险：有些分支的当前划分虽然不能提升泛化性能， 但在其基础上进行的后续划分却有可能导致性能显著提高。 预剪枝基于“贪心” 本质禁止这些分支展开， 带来了欠拟合风险 后剪枝 先从训练集生成一棵完整的决策树， 然后自底向上地对非叶结点进行考察， 若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升， 则将该子树替换为叶结点 后剪枝的优缺点 优点– 后剪枝比预剪枝保留了更多的分支， 欠拟合风险小， 泛化性能往往优于预剪枝决策树 缺点– 训练时间开销大：后剪枝过程是在生成完全决策树之后进行的，需要自底向上对所有非叶结点逐一考察 连续属性的处理与离散属性不同， 若当前结点划分属性为连续属性，连续属性还可作为后代结点的划分属性 1.png) 2.png) 多变量决策树• 分支节点根据多个属性的线性组合（一个节点由多个属性和对应权重决定）来进行判定 集成学习指将多个性能较低的“弱” 分类器通过适当组合而形成高性能的“强” 分类器的方法 多个模型的组合性能高于单个模型的性能-》模型之间的多样性越大，组合性能越好 Boosting学习法对多个弱学习器依次进行学习；每个弱学习器是对前一个弱学习器的强化 通过样本权重的变换来生成多样化的训练数据 Bagging学习法对多个弱学习器进行独立学习；通过训练样本的随机采样来生成多样化的模型 随机森林(random forest)随机森林 = bagging学习 + 决策树 随机性的目的：确保决策树组合的多样性• 数据集的随机性(bagging学习法)• 分支节点属性的随机性]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并行算法分析]]></title>
    <url>%2Farticle%2F515eae05%2F</url>
    <content type="text"><![CDATA[并行与并发的区别 并行是指多个事件在同一时刻发生; 而并发是指多个事件在同一时间间隔内发生。 分布式计算与并行计算的区别 SMP与 AMP的区别 并行计算机的互联方式 先序树遍历如何在PRAM模型上实现并行化 1，构建单链表 2，做前缀求和 3，每个顶点的输出顺序的值 注意点共享内存访问冲突问题的解决（利用忙等待、互斥）]]></content>
      <categories>
        <category>Parallel Algorithm</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机视觉]]></title>
    <url>%2Farticle%2F746b0d51%2F</url>
    <content type="text"><![CDATA[定义计算机利用相机和计算能力来感知现实世界 2.5维：在平面上浮凸出来的一个空间，可以想象是浮雕所处于的一种空间（伪3D） 2.5维：在平面上浮凸出来的一个空间，可以想象是浮雕所处于的一种空间（伪3D） 2.5D是从一个视角去看的，像素之间没有距离的概念，3D是从多视角去看的，有距离的概念 实现方案传统算法 HOG、金字塔、SIFT 神经网络 CNN 数据集 IMAGENET（李飞飞团队）https://v.qq.com/x/page/z0399f36dv1.html有关介绍 分类几何视觉（3D\2D） 语义识别（分类、检测、提取） 基础图像的采样与量化 如何训练神经网络关键Activation FunctionsData PreprocessingWeight InitializationBatch NormalizationBabysitting the Learning ProcessHyperparameter OptimizationoptimizationRegularizationTransfer Learning 环境CPU vs GPUDeep Learning FrameworksCaffe / Caffe2Theano / TensorFlowTorch / PyTorch TensorFlow is a safe bet for most projects. Not perfect but has huge community, wide usage. Maybe pair with high-level wrapper (Keras, Sonnet, etc) I think PyTorch is best for research. However still new, there can be rough patches. Use TensorFlow for one graph over many machines Consider Caffe, Caffe2, or TensorFlow for production deployment Consider TensorFlow or Caffe2 for mobile 激活函数 红色区域存在饱和(f(x)*f`(x)=0)神经元,黄色区域均为非饱和神经元 sigmoid激活函数1.饱和神经元将会使得梯度消失 2.Sigmoid输出不是以零为中心的 3.exp() is a bit compute expensive 由于使用sigmoid激活函数会造成神经网络的梯度消失和梯度爆炸问题，所以许多人提出了一些改进的激活函数，如：用ReLU、Leaky-ReLU、P-ReLU、R-ReLU、Maxout等替代sigmoid函数。 tanh(x)激活函数1.值均在[-1,1]之间 2.以零为中心点（这点很优秀） 3.仍然会在饱和时使得梯度消失 ReLU激活函数1.非饱和(在正区域)2.非常计算效率3.在实践中，收敛速度比sigmoid/tanh快得多(例如6x)4.实际上比sigmoid更合理 但是不是以零为中心点，并且容易出现死亡神经元 Leaky-ReLU激活函数1.不饱和2.计算效率高3.在实践中比sigmoid/tanh收敛得快得多!(例如6 x)4.不存在死亡神经元 Parametric Rectifier (PReLU) f(x) = max(ax, x) ELU激活函数1.继承ReLU的所有好处2.接近于零的平均输出3.负饱和状态与Leaky-ReLU相比，对噪声有一定的鲁棒性 但是计算中需要exp(),意味着计算量较高 Maxout激活函数1.没有点积的基本形式——&gt;非线性？2.泛化ReLU和Leaky-ReLU3.线性!不饱和!没有死亡神经元! 实际使用Use ReLU. Be careful with your learning ratesTry out Leaky ReLU / Maxout / ELUTry out tanh but don’t expect muchDon’t use sigmoid 数据预处理使用PCA和数据增强使得数据以0为中心 就是尽可能的让输入和输出服从相同的分布，这样就能够避免后面层的激活函数的输出值趋向于0 网络参数初始化 很小的随机数（在小型神经网络中表现可以，但在深层网络存在问题） 何凯明对ReLU的初始化方法 批归一化位置：通常插入在全连接层或卷积层之后，在非线性之前。 作用：使得数据的分布更符合高斯或正态分布 开始训练尝试输出每个epoch的损失值(是否下降、下降速率)、精准度 尝试学习率按（一定规律）衰减 找到合适的超参（1）交叉验证集 （2）随机查找 or 网格查找(大概确定范围) （3） 正则化 （4）可视化工具 梯度下降随机梯度下降小批量SGD 会有噪声 动量梯度下降 牛顿动量 以下算法查看大佬解析深度学习优化算法解析(Momentum, RMSProp, Adam) Adam适应性梯度算法添加了基于每个维度的历史平方和的梯度元素的缩放 RMSPropL-BFGS- Usually works very well in full batch, deterministic mode i.e. if you have a single, deterministic f(x) then L-BFGS will probably work very nicely - Does not transfer very well to mini-batch setting. Gives bad results. Adapting L-BFGS to large-scale, stochastic setting is an active area of research. 实际使用1.Adam is a good default choice in most cases 2.If you can afford to do full batch updates then try outL-BFGS (and don’t forget to disable all sources of noise) 正则化主因：数据链不够大 or 多样性不足 目的：提高模型泛化能力 1.给损失函数增加正则项 2.Dropout（随机删除） 总结Activation Functions (use ReLU)Data Preprocessing (images: subtract mean)Weight Initialization (use Xavier init)Batch Normalization (use)Babysitting the Learning processHyperparameter Optimization(random sample hyperparams, in log space when appropriate) Optimization ​ - Momentum, RMSProp, Adam, etc - Regularization ​ - Dropout, etc -Transfer learning ​ -Use this for your projects! 任务小作业（小组的形式 各自合作共同大项目） 复现一篇相关论文并完成10页的实验报告（类似论文） 1、presentation部分按小组完成一个计算机视觉方向的文献（多篇）阅读，讲解并提交一份文献综述报告；2、project 每个人完成一个（会下发题目），展示并讲解，提交源代码和实验报告。 所选论文Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting 论文链接：http://openaccess.thecvf.com/content_ECCV_2018/html/Wei_Liu_Learning_Efficient_Single-stage_ECCV_2018_paper.html 源代码：https://github.com/liuwei16/ALFNet Is Faster R-CNN Doing Well for Pedestrian Detection? 论文链接：https://arxiv.org/abs/1607.07032 源代码：https://github.com/zhangliliang/RPN_BF/tree/RPN-pedestrian ​ https://github.com/longcw/faster_rcnn_pytorch ​ https://github.com/CharlesShang/TFFRCNN 参考链接https://www.cnblogs.com/wangxiaocvpr/p/5747095.html Is Faster R-CNN Doing Well for Pedestrian Detection?论文阅读 Learning Efficient Single-stage Pedestrian Detectors by Asymptotic Localization Fitting 行人检测/人体检测综述 行人检测（Pedestrian Detection）论文整理 激活函数及其作用以及梯度消失、爆炸、神经元节点死亡的解释 深度学习优化算法解析(Momentum, RMSProp, Adam)]]></content>
      <categories>
        <category>Computer Vision</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[并行计算机组成]]></title>
    <url>%2Farticle%2F517840f3%2F</url>
    <content type="text"><![CDATA[定义并行计算机就是由多个处理单元组成的计算机系统，这些处理单元相互通信和协作，能快速，高效地求解大型复杂问题。 需求满足在平台金字塔顶端对性能需求最强的应用程序，对运算速度的无止境追求 发展历程 云计算（着重在软件-》网格运算）是并行计算（硬件）的一种发展形势 考虑到功耗，cpu散热，摩尔定律中晶体管的数量（当前还在上升）、CPU频率（大概3.8GHZ，超频4.2GHZ）受限。 结构并行计算机体系机构= 计算机体系结构 + 计算机通信 编程模型： 共享存储、消息传递、数据并行 计算机性能执行时间（execution time）- 响应时间 性能 = 1/执行时间 吞吐率（Throughput） 即带宽-单位时间内完成的任务数量 程序执行时间：是指用户的响应时间 (访问磁盘和访问存储器的时间， CPU时间， I/O时间以及操作系统的开销)CPU时间：它表示CPU的工作时间，不包括I/O等待时间和运行其它任务的时间。 Amdahl定律 - 加速比（只改进一部分，则系统所获得的加速比有上限） 并行计算的提升，受到了程序中必须串行执行部分的限制 经典的Amdahl定律并不适用于规模可扩展的系统，加速比曲线理论与实际不一致。 为什么CPU不能通过成千上万个线程来加速？CPU的架构和GPU不一样，CPU就算是采用超线程技术，一个核也只能开两个线程，4核就8个线程，如果强行开10000个线程实际上也是软系统层面的虚拟线程 参考资料CMU并行体系结构与编程的网址 学校教材]]></content>
      <categories>
        <category>Parallel Computing</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[英文摘要写作]]></title>
    <url>%2Farticle%2Fcb8609cd%2F</url>
    <content type="text"><![CDATA[摘要简介 摘要( abstract) 也称为内容提要，通常在学术论文中都必须附有摘要，其位子应放在论文的正文之前，对整个论文内容的概述。无论对专业读者还是对非专业读者而言，摘要都是一个非常重要的文件。 摘要如果和论文一起发表，则被称为一次性出版物摘要，主要用于帮助读者评价文章内容及其潜在作用，使读者不必阅读全文就可以了解论文的内容。除此之外，摘要也可以被单独收入文摘机构出版的摘要期刊如：生物学文摘(Biological Abstract)、化学文摘（Chemical Abstract）等，称为二次性出版物摘要。此类脱离论文独立成篇的摘要主要用于方便读者检索文摘、收集信息，帮助研究者寻找新的研究领域。 摘要的定义摘要的英文术语：有两个词汇，一个是 abstract, 一个是 summary 根据美国国家标准学会 ( American National Standards Institute ) 于1971年通过并颁布的《美国国家文摘写作标准》 （American National Standards for Writing Abstracts）规定，abstract 不应与 summary 混同。 Abstract 对一篇论文的主要内容以精炼的文字进行高度概括，使读者不必阅读论文全文即可迅速了解论文内容，或者让读者对即将阅读的文章有思想准备，或者让读者判断是否有通读全文的必要。文中只对论文信息进行浓缩，而不加主观评论或解释，可以脱离原文而独立成篇。字数通常在100 – 150 个词左右，更确切地说，约为原文长度的1% - 5%(有的杂志规定摘要平均为全文的 3% - 5% )。现在越来越多的用法是 abstract。尤其是放在索引资料中一律要用abstract 这个术语，在论文的题目下也通常要用这个词。 Summary (概要) 与 abstract 无明显差别。严格地说，summary 一般附在论文的后面，对论文的主要结论和成果进行再叙述。其前提是读者已经通读的全文，通过summary 来巩固论文的主要论点和成果。在某些论文中，用summary取代正文中的conclusion部分。Summary是论文的“缩影”，可以概括论文的全部内容，只是在删繁就简上下功夫，字数长短不一，少则两三句话，多则500个单词甚至更长.美国的一些高校规定,说是论文提要(summary)以250词左右为宜，而博士论文提要以350词左右为宜。博士会议论文的提要一般规定为300 – 500词或1000个印刷符号。 至于究竟采用什么形式，要根据征稿简则而定。一般说来，国际学术会议论文集要求按Summary方式来写摘要，而正式出版发行的刊物要求不禁一致。对于个别论文还见有前面为Abstract,结尾又有一个Summary，这多半是由于文章过长，内容又多，后面的Summary相当于该文的缩写。 摘要的种类摘要分为两类，一类是说明性摘要(Descriptive/Indicative Abstract),一类是资料性摘要（Informative Abstract）。 1.2.1 说明性摘要(Descriptive/Indicative Abstract) 如同迈克尔.艾利（Michael Alley）所说，“一篇说明性摘要是段落形式的目录，是读者手中的一份简要地图。”从这句话中可以清楚地了解说明性摘要的作用。说明性摘要只向读者指出论文的主要议题是什么，不涉及具体的研究方法和结果，但无法给读者提供更多的详细信息。它一般是用于综述性文章，也用于讨论、评论性文章，尤以介绍某学科近期发展动态的论文居多。常出现“…is studied”, “…is investigated”, “…is discussed”字样。时态多用现在时或现在完成时。其篇幅也较短，大多在100-150字之间。以下是一篇说明性摘要的样例： Ten widespread diseases that are hazards in isolated construction camps can be prevented by removing or destroying the breeding places of flies, mosquitoes and rats, and by killing their adult forms. 由于说明性摘要仅限于陈述论文的主要议题且篇幅较小，主要用于评述性论文。 1.2.2 资料性摘要（Informative Abstract） 资料性摘要适用于专题研究论文和实验报告型论文。 资料性摘要的优点是比说明性摘要能提供多得多的信息，它应该尽量完整和准确地体现原文的具体内容，特别要强调指出研究的方法和结果，结论等。其篇幅较长，大多在150-250字之间。根据原文长度，也有多达500字的。通常，这一类的摘要反映了论文的基本面貌，能够代替阅读论文全文。 Ten widespread diseases that are hazards in isolated construction camps can be prevented by removing or destroying the breeding places of flies, mosquitoes and rats, and by killing their adult forms. The breeding of flies is controlled by proper disposal of decaying organic matter, and of mosquitoes by destroying or draining pools, or spraying them with oil. For rats, only the indirect methods of rat-resistant houses and protected food supplies are valuable. Control of adult forms of both insects and rodents requires use of poisons. Screens are used for insects. Minnows can be planted to eat mosquito larvae. 如何写摘要摘要的位置摘要的位置是确定的，一般在作者工作单位的下方。如： Cultural Differences Between China and U.S.A. (标题) ​ Xu Ying (署名) Hunan University （Changsha，Hunan， 410082） （工作单位） Abstract: （摘要） Key words: (关键词) 写作要点 长度：有专家认为150-200个词之间；文章长度的五分之一。 有些刊物会规定摘要的篇幅不能超过一定的字数，如：在80-100之间，再投稿应查询。 若刊物没有规定长度时，可参阅已发表的文章长度。 参加国际会议的论文摘要有字数限制，一般要求200-500个词之间，约1000个印刷符号。 （美国化学文献、医学文献的论文摘要规定在200个词以内。） 不要重复论文中的句子。 避免例举大堆数据。 一般只是一个段落，不要将其分为数段。 不要使用祈使句、感叹句、公式、表格等。 完成论文后再写摘要。 一般使用第三人称或被动语态。 语言需简明扼要。 下面请看一篇论文摘要 This paper deals with the English syllabus for graduate students in China. The paper first reviews the history of the graduate English teaching, then discusses the shortcomings in the syllabus and finally proposes some suggestions for its revision. Key words: syllabus, graduate English teaching 阅读下面文章，然后写出一段80个词左右的摘要。最后再参阅提供的英语摘要。 These days, there is a common belief among parents that schools are no longer taking any notice of students’ spelling. But, no school I have taught in has ever ignored spelling or considered it unimportant as a basic skill. There are, however, vastly different ideas about how to teach it, or how much importance it must be given over general language development and writing ability. The problem is , how to encourage a child to express himself freely and confidently in writing without holding him back with the complexities of spelling. If spelling becomes the only focal point of his teacher’s interest, clearly a bright child will be likely to “play safe”. He will tend to write only words within his spelling range, choose to avoid adventurous language. That’s why teachers often encourage the early use of dictionaries and pay attention to content rather than technical ability. I was once shocked to read on the bottom of a sensitive piece of writing about a personal experience. “This work is terrible! There are too many spelling errors and your writing is hard to read.” It may have been a sharp criticism of the pupil’s technical abilities in writing, but it was a sad remark from the teacher who had omitted to read the essay, which contained some beautiful expressions of the child’s deep feelings. The teacher was not wrong to draw attention to the errors, but if his attention had centered on the child’s ideas, an expression of his disappointment with the presentation would have given the pupil more motivation to seek improvement. 摘要的内容 摘要的写作必须准确、明晰、简洁，概述与细节描述之间需要相互平衡，相互补充。内容取舍的标准首先是对论文本身重点的理解，其次应该考虑到读者阅读的方式。资料性摘要的内容通常包括： 背景知识或文献回顾 （Background Information / Literature Review） 研究的主要目的和范围 (Principal Purpose) 研究方法 （Methodology） 研究的主要结果（Results） 结论和建议 （Conclusions and Recommendation） 例1： This article discusses some possible roles for self-access pathways, particularly in cultures which have no tradition of self-study. It suggests how pathways might influence the design and running of self-access centre, and gives an illustration of how pathways were designed and employed in a centre in China. Feedback is based on a mini-survey distributed to thirty users. ( ELT Joural Vol.51/1 January 1997 Oxford Univ. Press, 1997) 例2： The science taught in the classroom should be reasonably up-to – date. What is taught should place emphasis first on the principles and major concepts of science rather than on the applications of scientific knowledge. The instructional techniques comprise laboratory work which is introduced in such a manner as to emphasize science as a process— to reveal through practice that science involves inquiry, discovery, and experimentation. The paper suggests that college science programs should be revised with a view to preparing teachers to handle science in secondary and elementary schools. 常用表达方法描述目的，介绍相关知识 This The paper thesis article study survey project research investigation present study work advances the view that… advocates … analyzes … argues that … contains deal with … discusses … develops … explains the reason why … expresses … focuses ( attention ) on the fact that … holds that … includes … investigates the features of … makes a comparative study upon the … offers … presents … proposes that … reviews … states … supports … The chief major main primary principal aim goal objective object proposal purpose of this of the paper study project research survey work is to investigate … discuss … evaluate … examine … determine … measure … reveal the cause of … This research is designed study project investigation The experiments on … were made The author attempts intends The author’s endeavor is to determine … measure the amount of … evaluate … calculate … obtain the result of … obtain some knowledge of … explain the reason why … outline the framework of … 描述观察角度： a. …from the angle of … b. … in the light of the context that … c. to view something at a different angle / from various angles d. from the point of view of … e. from the perspective of … 描绘方法： a. Detailed information has been acquired by the authors using … b. This is theory based on the idea that … c. Several sets of experiments have been performed to test the validity of … d. The method used in our study is known as … e. The technique the author adopted is referred to as … f. The approach taken in the investigation is called … g. The experiments consisted of four steps, which are described in … 描述结果： a. The results show / indicate that … b. The results are as follows: c. The analysis of the samples indicates that … d. We found that … e. Data suggested that … f. It is shown that … g. Based on / upon the outcome ​ findings ​ results of the research , points out believes concludes declares that … suggests recommends asks that … (从句中用虚拟语气) h. the author i. the results / findings / observations have shown that … j. The outcome has proved that … k. The data obtained seem to be very similar to those reported earlier by … 描述结论： a. In conclusion, the result shows … b. To sum up, we have revealed … c. It can be concluded / acknowledged that … d. The examination / investigation proves that … e. In summing up it may be stated that … f. All the preliminary results throw light on the nature of … g. These findings of the research have naturally led the author to the conclusion that … 练习Read the following passages first and then write an abstract of 60 to 80 words. Passage One The use of the word “imitation” reminds me that I should make some more comments on the risk of people imitating what they see on TV in the Way of crime of violence. First there was always a risk of children acting out according to what they saw from a TV program, which could be dangerous. For example I remember a woman who was a head of a primary school telling me that she had happened to look out of her window when the children were in the playground and had seen them putting a small boy on a chair with a rope round his neck under the branch of a tree; fortunately she was in time to stop them before the child was hanged. I remember a film of no particular merit in which the hero who was imprisoned had escaped by killing his guard, the technique of doing shown in detail. This was the kind of scene which we should cut for the reasons. In films for young people and adults we always tried to keep off the screen any details of criminal techniques, such as how to open a locked door with a piece of wire, or how to open a safe; if we were consulted before production I used to advise that the details should not be shown. When I gave talks in prisons about film censorship I invariably had full support for this; since fathers who were in prison for criminal offenses did not want their children to embark on crime. Every time I gave a talk in a prison someone used to mention the French film Rififi made by Jules Dassin in 1954. This remarkable film showed in great detail a robbery of a jeweler’s shop, the robbery sequence lasting about half an hour and being backed only by natural sound — one of the most brilliant film sequences of all time. I remember our discussions at the time. We took into account the fact that the robbery was accomplished only with the use of elaborate and obviously expensive equipment, and that only the most experienced and skilled criminals could possibly imitate it; we believed therefore that it was relatively safe. When talking in prisons some years later I learned that there had been several robberies in which the techniques had been copied, so perhaps we were wrong. Passage Two We have recently heard a great deal about the bad effects of computers on our social and economic organizations. In industry, computers mean automation, and automation means unemployment. Computers in the United States have already begun to displace workers whose tasks are simple. The variety of jobs,formerly done only by humans, that the machine can perform more rapidly, accurately, and economically, increases with each new generation of computers. If we follow this trend, we will be faced with mass unemployment for all but a handful of highly trained professionals, who will then be more powerful and overworked than they are now. What can we do about it? It is foolish to dream of reversing history. We cannot pass laws forbidding the advancement of science and technology. The computing machines are here, and they will grow because engineers want to build them, and politicians want their help in the process of government. In short, they will develop and become popular because they enable us to complete tasks that could never before have been undertaken, no matter how many unskilled laborers we might have set to work. Computers will continue to increase our intelligence for just the same reason that engines continue to strengthen our muscles. The question we must ask in not whether we shall have computers or not have computers, but rather, since we are going to have them, how we can make the most human and intelligent use of them. 说明以上内容是郝老师上课总结给我们的，分享给大家用于学习。 版权归属于她，请勿盗用。 我的摘要模板资料性摘要 背景知识或文献回顾 （Background Information / Literature Review） 研究的主要目的和范围 (Principal Purpose) 研究方法 （Methodology） 研究的主要结果（Results） 结论和建议 （Conclusions and Recommendation） ps:针对考试：注意答题纸，除了剩余空间，背面也要写4行，不要太多。若文章较难（看不懂），将每段的首句、尾句写入summary中。以上几点，范文出现几点，写几点。 ​ The article advances the view that …(文献回顾,文章讲了啥) ​ The chief aim of this research is to investigate/discuss that… （研究目的），from the angle of … （角度）.The approach taken in the investigation is called…(研究方法). It is shown that … ​ It can be concluded that … （研究结论）, There are some recommendation given by the authors. First of all,(建议1).In addition,(建议2).What’s more,(建议3).（建议）]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[操作系统总结]]></title>
    <url>%2Farticle%2F8ce3e425%2F</url>
    <content type="text"><![CDATA[《计算机操作系统》复习大纲 第一章 绪论 1.掌握操作系统的基本概念、主要功能、基本特征、主要类型； 2.理解分时、实时系统的原理； 第二章 进程管理 1.掌握进程与程序的区别和关系； 2.掌握进程的基本状态及其变化； 3.掌握进程控制块的作用； 4.掌握进程的同步与互斥； 5.掌握多道程序设计概念； 6.掌握临界资源、临界区、掌握信号量，PV操作的动作； 第三章 处理机调度 1.掌握作业调度和进程调度的功能； 2.掌握简单的调度算法：先来先服务法、时间片轮转法、优先级法； 3.掌握评价调度算法的指标：吞吐量、周转时间、平均周转时间、带权周转时间和平均带权周转时间； 4.掌握死锁；产生死锁的必要条件；死锁预防的基本思想和可行的解决办法； 5.掌握进程的安全序列，死锁与安全序列的关系； 第四章 存储器管理 1.掌握用户程序的主要处理阶段； 2.掌握存储器管理的功能；有关地址、重定位、虚拟存储器、分页、分段等概念； 3.掌握分页存储管理技术的实现思想； 4.掌握分段存储管理技术的实现思想； 5.掌握页面置换算法。 第五章 设备管理 1.掌握设备管理功能； 2.掌握常用设备分配技术； 3.掌握使用缓冲技术的目的； 第六章 文件管理 1.掌握文件、文件系统的概念、文件的逻辑组织和物理组织的概念； 2.掌握目录和目录结构；路径名和文件链接； 3.掌握文件的存取控制；对文件和目录的主要操作 第七章 操作系统接口 1.掌握操作系统接口的种类； 2.掌握系统调用的概念、类型和实施过程。 计算机操作系统复习知识点汇总 第一章 1、操作系统的定义、目标、作用 操作系统是配置在计算机硬件上的第一层软件，是对硬件系统的首次扩充。 设计现代OS的主要目标是：方便性，有效性，可扩充性和开放性. OS的作用可表现为： a. OS作为用户与计算机硬件系统之间的接口；（一般用户的观点） b. OS作为计算机系统资源的管理者；（资源管理的观点） c. OS实现了对计算机资源的抽象. 2、脱机输入输出方式和SPOOLing系统（假脱机或联机输入输出方式）的联系和区别 脱机输入输出技术Off-Line I/O是为了解决人机矛盾及CPU的高速性和I/O设备低速性间的矛盾而提出的.它减少了CPU的空闲等待时间，提高了I/O速度. 由于程序和数据的输入和输出都是在外围机的控制下完成的，或者说，它们是在脱离主机的情况下进行的，故称为脱机输入输出方式；反之，在主机的直接控制下进行输入输出的方式称为联机（SPOOLing）输入输出方式 假脱机输入输出技术也提高了I/O的速度，同时还将独占设备改造为共享设备，实现了虚拟设备功能。 3、多道批处理系统需要解决的问题 处理机管理问题、内存管理问题、I/O设备管理问题、文件管理问题、作业管理问题 4**、OS具有哪几个基本特征?它的最基本特征是什么? a. 并发性Concurrence,共享性Sharing,虚拟性Virtual,异步性Asynchronism. b. 其中最基本特征是并发和共享. c. 并发特征是操作系统最重要的特征，其它三个特征都是以并发特征为前提的。 5、并行和并发 并行性和并发性是既相似又有区别的两个概念， 并行性是指两个或多个事件在同一时刻发生； 而并发性是指两个或多少个事件在同一时间间隔内发生。 6、操作系统的主要功能，各主要功能下的扩充功能 a. 处理机管理功能： 进程控制，进程同步，进程通信和调度. b. 存储管理功能： 内存分配，内存保护，地址映像和内存扩充等 c. 设备管理功能：缓冲管理，设备分配和设备处理，以及虚拟设备等 d. 文件管理功能：对文件存储空间的管理，目录管理，文件的读，写管理以及共享和保护 7、操作系统与用户之间的接口 a. 用户接口：它是提供给用户使用的接口，用户可通过该接口取得操作系统的服务 b. 程序接口：它是提供给程序员在编程时使用的接口，是用户程序取得操作系统服务的唯一途径。 第二章 1、进程的定义、特征，进程实体的组成 进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位。 进程具有结构特征、动态性、并发性、独立性和异步性。 进程实体由程序段、相关的数据段和PCB三部分构成。 2、进程的三种基本状态及其转换 运行中的进程可能具有就绪状态、执行状态、阻塞状态三个基本状态。 进程三个基本状态转换图— P38 3**、引入挂起状态的原因，具有挂起状态的进程转换** a. 终端用户的请求 b. 父进程请求 c. 负荷调节的需要 d. 操作系统的需要 具有挂起状态的进程转换图— P39 4、创建进程的主要步骤 a. 为一个新进程创建PCB，并填写必要的管理信息。 b. 把该进程转入就绪状态并插入就绪队列之中。 5、进程控制块（PCB）的作用 PCB是进程实体的一部分，是操作系统中最重要的记录型数据结构。PCB中记录了操作系统所需的用于描述进程情况及控制进程运行所需的全部信息。因而它的作用是使一个在多道程序环境下不能独立运行的程序含数据，成为一个能独立运行的基本单位，一个能和其它进程并发执行的进程。 为什么说PCB是进程存在的唯一标志? 在进程的整个生命周期中，系统总是通过其PCB对进程进行控制，系统是根据进程的PCB而不是任何别的什么而感知到该进程的存在的，所以说，PCB是进程存在的唯一标志。 6、进程控制块的组织方式 链接方式、索引方式 7、原语的定义、组成、作用 原语是由若干条指令组成的，用于完成一定功能的一个过程，与一般过程的区别在于：它们是“原子操作”，它是一个不可分割的基本单位，在执行过程中不允许中断。原子操作在管态下执行，常驻内存。 原语的作用是为了实现进程的通信和控制，系统对进程的控制如不使用原语，就会造成其状态的不稳定性，从而达不到进程控制的目的。 8、引起创建进程的事件 用户登录、作业调度、提供服务、应用请求 9、引起进程终止的事件 正常结束、异常结束、外界干预 10、引起进程阻塞和唤醒的事件 请求系统服务、启动某些操作、新数据尚未到达、无新工作可做 11、临界资源和临界区 临界资源是指每次仅允许一个进程访问的资源。 属于临界资源的硬件有打印机、磁带机等,软件有消息缓冲队列、变量、数组、缓冲区等。 诸进程间应采取互斥方式，实现对这种资源的共享。 每个进程中访问临界资源的那段程序称为临界区（Critical Section）不论是硬件临界资源，还是软件临界资源，多个进程必须互斥地对它进行访问。 12、同步机制应遵循的规则 空闲让进、忙则等待、有限等待、让权等待 13、进程通信的类型 高级通信机制可归结为三类：共享内存系统、消息传递系统以及管道通信系统。 14、线程的定义、属性在多线程OS中，通常是在一个进程中包含多个线程，每个线程都是作为利用CPU的基本单位，是花费最小开销的实体。 线程具有下述属性：（1）轻型实体—线程中的实体基本上不拥有系统资源，只是有一点必不可少的、能保证其独立运行的资源。 （2）独立调度和分派的基本单位 （3）可并发执行。（4）共享进程资源。 15、进程和线程的比较 a. 调度性。在传统的操作系统中，拥有资源的基本单位和独立调度、分派的基本单位都是进程，在引入线程的OS中，则把线程作为调度和分派的基本单位，而把进程作为资源拥有的基本单位； b. 并发性。在引入线程的OS中，不仅进程之间可以并发执行，而且在一个进程中的多个线程之间，亦可并发执行，因而使OS具有更好的并发性； c. 拥有资源。无论是传统的操作系统，还是引入了线程的操作系统，进程始终是拥有资源的一个基本单位，而线程除了拥有一点在运行时必不可少的资源外，本身基本不拥有系统资源，但它可以访问其隶属进程的资源； d. 系统开销。由于创建或撤销进程时，系统都要为之分配和回收资源，如内存空间等，进程切换时所要保存和设置的现场信息也要明显地多于线程，因此，操作系统在创建、撤销和切换进程时所付出的开销将显著地大于线程。 第三章 1、高级调度与低级调度的区别 高级调度又称为作业调度或长程调度，调度对象是作业，作业调度往往发生于一个（批）作业运行完毕，退出系统，而需要重新调入一个（批）作业进入内存时，故作业调度的周期长；低级调度又称为进程调度和短程调度，调度物件为进程（或内核级线程），进程调度的运行频率最高，是最基本的一种调度，多道批处理、分时、实时三类OS中必须配置这种调度。 引入中级调度的主要目的：是为了提高系统资源的利用率和系统吞吐量 2、低级调度的功能 保存处理机的现场信息、按某种算法选取进程、把处理器分配给进程 3、进程调度方式 （1）非抢占方式—实现简单、系统开销小、适用于大多数的批处理系统环境 （2）抢占方式——原则：优先权原则、短作业（进程）优先原则、时间片原则 4、同时具有三级调度的调度队列模型当在OS中引入中级调度后，人们可把进程的就绪状态分为内存就绪和外存就绪，类似的阻塞状态也可以同样划分。 5、三大调度算法 在ＯＳ中调度实质是一种资源的分配。 先来先服务和短作业（进程）优先调度算法、高优先权优先调度算法、基于时间片的轮转调度算法。 6**、高响应比优先调度算法 优先权＝等待时间＋要求服务时间＼要求服务时间 响应比＝等待时间＋要求服务时间＼要求服务时间＝响应时间＼要求服务时间 7**、最低松弛度优先调度算法即LLF算法 该算法是根据任务紧急（或松弛）的程度，来确定任务的优先级。涉及到计算题，参照课本Ｐ１０２仔细研究。 8**、何谓死锁？产生死锁的原因和必要条件是什么？ a.死锁是指多个进程因竞争资源而造成的一种僵局，若无外力作用，这些进程都将永远不能再向前推进； b.产生死锁的原因有二，一是竞争资源，二是进程推进顺序非法； c.必要条件是: 互斥条件，请求和保持条件，不剥夺条件和环路等待条件。 9、处理死锁的基本方法** （１）预防死锁—破坏产生死锁的四个必要条件中的一个或几个条件 （２）避免死锁—破坏产生死锁的四个必要条件 （３）检测死锁—通过系统设置的检测机构，及时检测出死锁的发生 （４）解除死锁—撤销或挂起一些进程 10、预防死锁的方法 a.摒弃”请求和保持”条件 b.摒弃”不剥夺”条件 c.摒弃”环路等待”条件 11 解除死锁 a资源剥夺法，b撤销进程 第四章 1、存储器按存储量、速度怎么划分？ 对于通用计算机而言，存储层次至少应具有三级：最高层为CPU寄存器、中间为主存、最底层为辅存，较高档点的根据具体功能还可细分为：寄存器；高速缓存、主存储器、磁盘缓存；固定硬盘、可移动存储介质等6层。 主存储器（简称内存或主存）：容量一般为数十MB到数GB，其访问速度远低于CPU执行指令的速度。为此引入寄存器和高速缓存，寄存器访问速度最快，价格昂贵，容量不大；高速缓存容量大于或远大于寄存器，从几十KB到几十MB，访问速度快于主存储器。 2、程序的装入方式 绝对装入方式、可重定位装入方式、动态运行时装入方式 3、程序的链接方式分类 静态链接、装入时动态链接、运行时动态链接 4**、对换的定义、分类、实现** 对换是把内存中暂时不能运行的进程或者暂时不用的程序和数据调到外存上，以便腾出足够的内存空间，再把已具备运行条件的进程或进程所需要的程序和数据调入内存。 以整个进程为单位，称为“整体对换”或“进程对换”；以“页”或“段”为单位，分别称为“页面对换”和“分段对换”，又称为“部分对换” 为了实现进程对换，系统必须能实现三方面的功能：对换空间的管理、进程的换出，以及进程的换入。 5、页面与页表 分页存储管理是将一个进程的逻辑地址空间分成若干个大小相等的片，称为页面或页 由于进程的最后一页经常装不满一块而形成不可利用的碎片，称为“页内碎片”。 系统为每个进程建立一张页面映像表，简称页表。页表的作用是实现从页号到物理块号的地址映射。 6**、分页系统的地址变换机构** 涉及到图形，分别是P132和P133 7、分段存储管理方式的引入原因 引入分段存储管理方式，主要是为了满足用户和程序员的一些需要： 方便编程、信息共享、信息保护、动态增长、动态链接 8、分段系统的基本原理 在分段存储管理方式中，作业的地址空间被划分为若干个（二维）段，每个段定义了一组逻辑信息，逻辑地址由段号和段内地址组成。每个段在表中占有一个表项，其中记录了该段在内存中的起始地址（又称为“基址”）。段表是用于实现从逻辑段到物理内存区的映射。 9、分段和分页的主要区别 a. 分页和分段都采用离散分配的方式，且都要通过地址映射机构来实现地址变换，这是它们的共同点； b. 对于它们的不同点有三，第一，从功能上看，页是信息的物理单位，分页是为实现离散分配方式，以消减内存的外零头，提高内存的利用率，即满足系统管理的需要，而不是用户的需要；而段是信息的逻辑单位，它含有一组其意义相对完整的信息，目的是为了能更好地满足用户的需要； c. 页的大小固定且由系统确定，而段的长度却不固定，决定于用户所编写的程序； d. 分页的作业地址空间是一维的，而分段的作业地址空间是二维的. 10、虚拟存储器的特征及其内部关联 a. 虚拟存储器具有多次性，对换性和虚拟性三大主要特征； b. 其中所表现出来的最重要的特征是虚拟性，它是以多次性和对换性为基础的，而多次性和对换性又必须建立在离散分配的基础上。 11、最佳置换算法和先进先出置换算法 涉及到关键的作图和计算答题，参照课本P150 12、最近最久未使用（ＬＲＵ）置换算法 13、请求分段系统的地址变换过程 涉及到关键的考试内容，请参考课本P156 图4-33仔细研究 14、分段保护 采取以下措施保证信息安全：越界检查、存取控制检查、环保护机构 第五章 １、I/O设备按使用特性、传输速率、信息变换、共享属性如何分类 按设备的使用特性分类：存储设备（又称外存、后备存储器、辅助存储器）；输入输出设备（又可具体划分：输入设备（键盘、鼠标、扫描仪、视频摄像、各类传感器）、输出设备（打印机、绘图仪、显示器、数字视频显示设备、音响输出设备）、交互式设备） 按传输速率分类：低速设备（键盘、鼠标、语音的输入输出设备）；中速设备（行式打印机、激光打印机）；高速设备（磁带机、磁盘机、光盘机）。 按信息交换的单位分类：块设备（磁盘）；字符设备（交互式终端、打印机） 按设备的共享属性分类：独占设备；共享设备（磁盘）；虚拟设备 2、设备控制器的组成 设备控制器由以下三部分组成：（1）设备控制器与处理机的接口，该接口用于实现CPU与设备控制器之间的通信，提供有三类信号线：数据线、地址线和控制线。（2）设备控制器与设备的接口，可以有一个或多个接口，且每个接口连接一台设备。每个接口都存在数据、控制和状态三种类型的信号。（3）I/O逻辑，用于实现对设备的控制。其通过一组控制线与处理机交互，处理机利用该逻辑向控制器发送I/O命令，I/O逻辑对收到的命令进行译码。 3、I/O通道设备如何引入 虽然在ＣＰＵ和I/O设备之间增加了设备控制器后，已能大大减少CPU对I/O的干预，但当主机配置的外设很多时，CPU的负担仍然很重，为此，在ＣＰＵ和设备控制器之间又增设了通道。 I/O通道是一种特殊的处理机，它具有执行I/O指令的能力，并通过执行通道（I/O）程序来控制I/O操作。 4**、有哪几种I/O控制方式？各适用于何种场合？** I/O控制方式：程序I/O方式、中断驱动I/O控制方式、DMAI/O控制方式、I/O通道控制方式。程序I/O方式适用于早期的计算机系统中，并且是无中断的计算机系统；中断驱动I/O控制方式是普遍用于现代的计算机系统中；DMA I/O控制方式适用于I/O设备为块设备时在和主机进行数据交换的一种I/O控制方式；当I/O设备和主机进行数据交换是一组数据块时通常采用I/O通道控制方式，但此时要求系统必须配置相应的通道及通道控制器。 5、DMA控制器的组成 DMA控制器由三部分组成：主机与DMA控制器的接口、DMA控制器与块设备的接口、I/O控制逻辑。 6**、为了实现主机与控制器之间成块数据的直接交换，需设置ＤＭＡ控制器中四类寄存器 DR：数据寄存器，暂存从设备到内存或从内存到设备的数据 MAR：内存地址寄存器 DC：数据计数器，存放本次CPU要读或写的字（节）数 CR：命令\状态寄存器，接收从CPU发来的I/O命令，或相关控制信息，或设备状态 7**、缓冲的引入原因 操作系统引入缓冲机制的主要原因可归结为以下几点：（1）缓和CPU与I/O设备间速度不匹配的矛盾；（2）减少对CPU的中断频率，放宽对中断响应时间的限制；（3）提高CPU与I/O设备之间的并行性。 8、缓冲池的组成、工作方式 三个队列：空缓冲队列、输入队列、输出队列 四种工作缓冲区：（1）用于收容输入数据的工作缓冲区；（2）用于提取输入数据的工作缓冲区；（3）用于收容输出数据的工作缓冲区；（2）用于提取输出数据的工作缓冲区； 缓冲区工作方式参照图P176 图5—15 9**、SPOLLing系统的定义、组成、特点** SPOOLing系统是对脱机I/O工作的模拟，其必须有高速随机外存（通常采用磁盘）的支持。SPOOLing系统主要有以下四个部分： （1）输入井和输出井，为磁盘上开辟的两大存储空间，分别模拟脱机输入/出时的磁盘，并用于收容I/O设备输入的数据和用户程序的输出数据； （2）输入缓冲区和输出缓冲区，在内存中开辟，分别用于暂存由输入设备和输出井送来的数据； （3）输入进程SPi和输出进程SPo，分别模拟脱机输入/出时的外围控制机，用于控制I/O过程； （4）I/O请求队列，由系统为各个I/O请求进程建立的I/O请求表构成的队列。 SPOLLing系统的特点：提高了I/O的速度；将独占设备改造为共享设备；实现了虚拟设备功能。 第六章 1、文件的定义、属性 文件是指由创建者所定义的、具有文件名的一组相关信息的集合，可分为有机构文件和无结构文件。 文件的属性包括：文件类型、文件长度、文件的物理位置、文件的建立时间 2、文件类型按用途、文件中数据的形式、存取控制属性、组织形式和处理方式如何划分？ 按用途分类：系统文件、用户文件、库文件 按文件中数据的形式分类：源文件、目标文件、可执行文件 按存取控制属性分类：只执行文件、只读文件、读写文件 按组织形式和处理方式划分：普通文件、目录文件、特殊文件 3、有结构文件按不同方式组织形成哪几种文件？ 顺序文件、索引文件、索引顺序文件 4、顺序文件的适用场合、优缺点 最佳适用场合是在对诸记录进行批量存取时。 批量存取时对顺序文件的存取速率是所有逻辑文件中最高的；只有顺序文件能存储在磁带上，并能有效地工作。 在交互应用场合，顺序文件表现出来的性能很差；如果想增加或删除一个记录都比较困难。 5、对目录管理的要求有哪些？ 对文件目录的管理有以下要求： a 实现“按名存取” b 提高对目录的检索速度 c 文件共享 d 允许文件重名]]></content>
      <categories>
        <category>Computer Operating</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu下深度学习环境与blog搭建]]></title>
    <url>%2Farticle%2F46397481%2F</url>
    <content type="text"><![CDATA[本机系统：Ubuntu18.04 本机显卡型号查询 1lspci | grep -i nvidia 查看该显卡是否支持cuda https://developer.nvidia.com/cuda-gpus 更新NVIDIA驱动添加 Graphic Drivers PPA 123sudo add-apt-repository ppa:xorg-edgers/ppa #添加ppa源sudo add-apt-repository ppa:graphics-drivers/ppa #添加ppa源sudo apt-get update 12ubuntu-drivers devicessudo apt install nvidia-driver-440(所推荐的驱动包) or 打开 Software &amp; Updates，选择 Additional Drivers，一般需要加载一定时间，会出现多个驱动，选择最新的也就是版本号最大的 NVIDIA-Driver，点击应用，需要等待一点时间生成应用，完成便成功安装了驱动 查看NVIDIA驱动版本 12nvidia-smi # 显示驱动版本440和驱动的CUDA版本10.2（和运行CUDA不同） or 1sudo dpkg --list | grep nvidia-* or 1cat /proc/driver/nvidia/version CUDA与cuDNN只是深度学习环境，装了conda就没必要单独装cuda什么的了，虚拟环境有指令可以包含进去。 但是CUDA的其他文件可能没有, 比如nvcc.因此我想安装完整的CUDA,。另外, 如果驱动满足, 这里的cudatoolkit版本&lt;=系统, 也能正常使用. 完整的CUDA安装CUDA Toolkit Archive （Ubuntu18.04 下载runfile版本） cuda10.1(选择这个版本的原因，是因为cudnn在2020.5.26都没有对应的版本) 123wget http://developer.download.nvidia.com/compute/cuda/10.1/Prod/local_installers/cuda_10.1.243_418.87.00_linux.runsudo sh cuda_10.1.243_418.87.00_linux.run 安装完后，在.bashrc文件末尾添加环境变量 1234567sudo gedit ~/.bashrcexport PATH=$PATH:/usr/local/cuda-10.1/bin export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-10.1/lib64 export LIBRARY_PATH=$LIBRARY_PATH:/usr/local/cuda-10.1/lib64 source ~/.bashrc 测试 1234567nvcc --version #出现Cuda compilation tools, release 10.1, V10.1.243cd /usr/local/cuda/samples/1_Utilities/deviceQuery sudo make./deviceQuery#出现deviceQuery, CUDA Driver = CUDART, CUDA Driver Version = 10.2, CUDA Runtime Version = 10.1, NumDevs = 1Result = PASS https://share.weiyun.com/zMGMSFEE ## cuDNN cuDNN是用于神经网络的GPU库, 有些python包依赖cuDNN才能运行. 官网说明 官网下载runtime/dev/doc三个版本的deb文件，依次安装。 cuDNN Archive（目前还没有10.2的对应版本） 安装后查看版本： 1cat /usr/include/cudnn.h | grep CUDNN_MAJOR -A 2 用sample验证CUDNN（需要之前安装dev和doc两个deb） 修改cudnn.h的include “…”为include &lt;…&gt;，否则下面编译会报错 123cd /usr/src/cudnn_samples_v7/mnistCUDNN # 如果没有操作权限就cp到可操作的位置make clean &amp;&amp; make./minstCUDNN 安装Miniconda点击Anaconda 镜像使用帮助，下载Miniconda （Miniconda3-py37_4.8.2-Linux-x86_64.sh）就可以了。 12345678910111213141516# 进入下载目录# cd Downloads/cd 下载# 执行安装 bash Miniconda3-py37_4.8.2-Linux-x86_64.sh 遇到Do you accept the license terms? [yes|no]回车q键退出阅读licenseyesMiniconda3 will now be installed into this location:/home/用户名/miniconda3回车默认Do you wish the installer to initialize Miniconda3by running conda init? [yes|no]yes 完成后关闭该终端，再一次打开终端 测试是否安装成功 12345678conda --version#添加国内镜像，进入清华镜像源anaconda页面conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/freeconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/mainconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forgeconda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda conda config --set show_channel_urls yes conda常用命令pytorchGPU为所创建的虚拟环境名字 创建虚拟环境 1conda create -n pytorchGPU python=3.7.6 激活虚拟环境 1conda activate pytorchGPU 退出当前虚拟环境 1conda deactivate 删除虚拟环境操作1conda remove -n pytorchGPU --all 1234567891011conda info ＃查看conda的信息（是否有镜像库）conda clean -i ＃清除缓存索引conda list #查看安装了哪些包。conda env list 或 conda info -e #查看当前存在哪些虚拟环境conda update conda #检查更新当前condapython --version #查看python版本 对虚拟环境中安装额外的包使用命令conda install -n your_env_name [package]即可安装package到your_env_name中 删除环境中的某个包使用命令conda remove --name your_env_name package_name 即可 配置深度学习环境 Anaconda Cloud官网提供了各种包的安装命令，可以搜索并安装到我们创建的虚拟环境中，例如搜索pytorch，执行便可安装 再一次检查镜像源： 1sudo gedit ~/.condarc 配置文件修改如下： channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/pro/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/msys2/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/bioconda/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/menpo/ - https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/simpleitk/ - defaults show_channel_urls: true 12conda info ＃查看conda的信息（是否有镜像库）conda clean -i ＃清除缓存索引 安装GPU版pytorch，在官网http://pytorch.org/选一个你的当前的配置 12conda install pytorch torchvision cudatoolkit=10.1 #去掉-c pytorch安装的时候才会默认从清华源下载相应的包 测试12345678910111213141516#在VScode里面测试import torchimport torchvisionfrom torch.backends import cudnn# TEST 出现tensor([1.], device='cuda:0')x = torch.Tensor([1.0])xx = x.cuda()print(xx)# CUDA cuDNN test ,出现２个trueprint(torch.cuda.is_available())print(cudnn.is_acceptable(xx))#输出当前GPU型号gnlook = torch.cuda.current_device()print(torch.cuda.get_device_name(gnlook)) ## 出现错误 Torch not compiled with CUDA enabled 不知道是什么原因，清华源下载的是cpu版pytorch 我认为是没有检测到安装好的英伟达驱动程序，所以检查好驱动版本后重启电脑，重新安装pytorch（删掉这个虚拟环境，再来）。 IDE-VScode推荐Visual Studio Code，轻量快速，在终端激活环境，便可直接运行程序 ##安装 1.进入官网，直接下载压缩包。（我的是64位） https://code.visualstudio.com/Download 12cd 下载sudo dpkg -i code_1.35.0-1559611369_amd64.deb 解压完成，在全部应用的区域就可以看到VS Code的图标了，直接点击打开 配置ubuntu18.04+VSCode+Python安装 ubuntu18.04- +vscode c++使用时的三个配置文件 pylint路径设置VSCode中pytorch出现’torch’ has no member ‘xxx’的错误 setting中 python.linting.pylintPath : /home/pabebe/miniconda3/envs/pytorchGPU/bin/pylint 保存后便无报错。 美化VSCode配置FiraCode字体 下载字体 到FiraCode字体的GitHub页面 找到下面的Download链接下载最新字体 解压缩下载文件，并进入ttf文件夹 选中所有字体文件，右键选择安装 配置字体 打开vscode的配置页面，并搜索font 修改editor.fontFamily配置项的内容为：&apos;Fira Code Retina&apos;, &apos;Microsoft Yahei UI&apos;。由于Fira Code字体不支持中文，这里配置微软雅黑为第二字体 配置后重启vscode 开机自动挂载windows磁盘挂载磁盘的信息是保存在 /etc /fstab 这个文件里面,我们注意到其中又这么一行注释 这个就是对下面没有注释的信息的解释 ：分别表示:原来在文件系统的位置，加载点位置，类型，参数等，其中以#是注释符号，相当于C语言中的 // 注释 123#先备份sudo cp /etc/fstab /etc/fstab_backup df 先用 df命令查看要自动挂载磁盘信息（查看之前先手动挂载想要开机自动挂载的磁盘，要不然显示不出来） 123456sudo gedit /etc/fstab#修改如下# window D disk/dev/sdb2 /media/pabebe/D ntfs defaults,locale=zh_CN.UTF-8 0 0# window E disk/dev/sdb3 /media/pabebe/E ntfs defaults,locale=zh_CN.UTF-8 0 0 保存，退出，重启系统 Ubuntu18.04开机自启动脚本准备好你的sh脚本文件路径:~/VScodeWorkspace/mountDisk.sh 给予权限 1chmod 777 ~/VScodeWorkspace/mountDisk.sh 挂载磁盘脚本-mountDisk.sh 123456789101112131415161718192021222324252627282930313233343536373839404142#!/bin/sh# 挂载硬盘到指定目录（放在 /【根目录】/media/用户名/下）mymount()&#123; echo "check and create mount directory!" if [ ! -d "/media/pabebe/d" ] then mkdir /media/pabebe/D fi echo "start mount win disk!!" sudo mount /dev/sdb2 /media/pabebe/D echo "mount over!Have fun"&#125;myumount()&#123; echo "start umounting win disk!!" sudo umount /media/pabebe/D echo "all down bye!"&#125;cd /#mymount #创建博客桌面快捷键if [ -e "/home/pabebe/桌面/blog" ]then sudo rm /home/pabebe/桌面/blogfiln -s /media/pabebe/D/Pabebezz.github.io/source/_posts /home/pabebe/桌面/blogecho "blog 可以快捷访问啦"exit 0#echo "mount or umount win disk?please type m/u"# read M_U# if [ "$M_U" = "m" ]; then# mymount# else# myumount# fi#shel l脚本需要注意的地方就是if这个条件这里[]和then要是在同一行那么就得加上;，#还有就是 中间的判断要和中括号有空格，比如[ 判断条件 ] 创建一个service文件进入/etc/systemd/system/，创建一个autoMountDisk.service文件，内容如下： 12345678[Unit]Description=for quickly access blog source #这里填简介[Service]ExecStart=/home/pabebe/VScodeWorkspace/mountDisk.sh # 这里填sh文件路径 [Install]WantedBy=multi-user.target 启动12345678# 重新加载配置文件sudo systemctl daemon-reload # service文件改动后重新转载sudo systemctl enable autoMountDisk.service #设置开机启动# 不重启，立即启动服务sudo systemctl start autoMountDisk.service＃查看运行状态sudo systemctl status autoMountDisk.service 关于service文件里的一些选项，在这里有详细的说明。 ## 问题 autoMountDisk.service: Failed to execute command: Exec format error autoMountDisk.service: Failed at step EXEC spawning /home/pabebe/VScodeWorkspace code=exited, status=203/EXEC 解决方案: systemctl执行脚本时需要知道脚本的解释器 在脚本的开头加上 1#!/bin/sh Bolgubuntu下搭建Hexo+GitHub博客 Ubuntu16 升级nodejs版本 参考链接Ubuntu 搭建深度学习环境（直接安装anaconda，就不需要再繁琐安装cuda、cudann了） Pycharm没有菜单栏 conda安装Pytorch下载过慢解决办法(11月26日更新ubuntu下pytorch1.3安装方法) ubuntu自带截图工具–方便好用 ubuntu18.04 使用systemd方式添加开机运行sh脚本 ubuntu 18.04开机自启动脚本 Ubuntu 18.10 下安装CUDA10/CUDA10.1 systemctl自定义service执行shell脚本时报错code exited status 203 EXEC ubuntu14.04开机自动挂载windows磁盘的配置方法]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu16.04下深度学习环境搭建]]></title>
    <url>%2Farticle%2F46397481%2F</url>
    <content type="text"><![CDATA[深度学习环境搭建ubuntu16.04+1080ti+cuda10.0+cudnn环境配置 Pycharm没有菜单栏 本文英伟达显卡驱动版本：430.64 cuda 10.0.130 cudann 7.6.4 Bolgubuntu下搭建Hexo+GitHub博客 Ubuntu16 升级nodejs版本 ubuntu自带截图工具–方便好用]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[英语笔试笔记]]></title>
    <url>%2Farticle%2F2a1314f5%2F</url>
    <content type="text"><![CDATA[时间分配写作（25分钟写完，5分钟看听力文章） 听力（30分钟） 阅读理解：先仔细阅读，再长篇阅读，最后段落翻译。选词填空能做就做。 高级词替换thinkhave been convinced that… be of the opinion that…cling to the perspective that… ( 坚持认为… )maintain contend assertargue assume claim many a sea ofmultitudes ofimmense amounts ofnumerous innumerable plentiful people we usprivate individuals (人们)youngsters and teenagers（年轻人们）all children and adults （所有孩子和成年人们-》所有人）experts and professors （专家和教授们）parents kids offspring （父母孩子及其子孙后代）businessman （商人们）youngsters on campus （大学生们） veryexceedingly（ adv. 非常；极其；极度地；极端 ）distinctly（ adv. 明显地；无疑地，确实地 ）strikingly（ adv. 显著地，突出地，引人注目地 ）more thanextraordinarily（ adv. 极其，极端地；奇怪地 ）outstandingly （ adv. 非常，极其；优异，极好地；格外地 ） importantsignificantcrucialcriticalindispensableplay a crucial role in sth. Andsimilarlyequallylikewiseat the same time 万用句型（1）主语从句这可以写在文章任何位置，用于拉长句子。 显而易见、众所周知 It proves self-evident thatIt has been found thatIt seems beyond dispute thatIt seems universally acknowledged thatIt has been widely accepted thatIt becomes generally agreed that （2）定语从句这可以写在任何一句话的后面，用于补充说明。 显而易见、众所周知 a which is really beyond dispute.b which has been widely accepted.c which has provoked the public’s widespread concern. 例如 He is a good student,which has provoked the public’s widespread concern. （3）万能状语这可以放在句子的任意位置 as every one can see（众所周知） with the rapid advance of science and technology（随着科学和技术的快速发展） in our contemporary society（在当今社会，如今 = today） in the general routine of everyday living（在我们日常生活中） （4）插入语 to be frank（坦白说） as a matter of fact （事实上） from my perspective（在我看来） needless to say（显而易见）in my judgment（在我看来） to tell the truth（坦白说） （5）强调句型It is… that… 直接套进去就好，注意时态。 作文中所有的句子都可以写成强调句型，但是不能强调谓语，其他句子成分都可以被强调. 例：I met a crazy dog in the street yesterday. 转换为： 强调 昨天 It was yesterday(所强调的部分) that I met a crazy dog in the street . 强调 我 It was I(所强调的部分) that met a crazy dog in the street yesterday. 强调 在街上 It was in the street (所强调的部分) that I met a crazy dog yesterday. 加工重点句作文老师一定会看的四句话： 第一段、第二段、第三段的第一句、第一段的最后一句 一个句子用一个句型加工就好了。 作文这些模板句里面的词用上面的高级词替换掉，然后再换掉整个句型，生成自己的模板 （1）谚语警句类第一段：引出主题（一句话）+ 解释你对这句话的理解 （二句话） 第一句 A Nowadays（换掉）, there remains（换掉） an increasing（换掉） interest in the topic（换掉） about…; B Recently the issue of…has been in the limelight / brought into focus; C What is your idea as to the topic about…? It is my belief that …; D It looks beyond dispute that the issue about … has caused wide publicattention. 第二、三句 the meaning of the saying seems that … 比如：不要草率做决定the meaning of the saying seems that if you hope to do something successfully, please think it carefully. That is to say, it is foolish to decide it quickly. 第二段： 举例 A Although（换掉） so abundant cases can support（换掉） my simple view（换掉）, the following one is most favorable（换掉）.B Examples to prove the view are abundant. The most persuasive one is thecase of sb.C Such impressive cases/stories are not rare in our daily life, yet the followingone is definitely typical. …之后举一个详细的例子【我（我叔叔）有一个善良热情（责任感强、有耐心）的朋友（邻居）xxx,他怎么怎么样 要切题】。 第三段： 总结 第一句 Under no circumstances can we fail to pour attention into the importance/seriousness of the fact that It is really high time that due attention cannot have failed to paid to theissue. So crucial/grave is sth that it should have caused our attention. It is the fact of sth that really has a great influence on our study and life. 第二、三句话具体措施（国家政府、家长老师、个人三方面措施）for one thing / for another;on one hand / on the other hand; Eg. Write an essay on happiness by referring to the saying “Happiness is notthe absence of problems, but the ability to deal with them.” You can citeexamples to illustrate your points and then explain how you can develop yourability to deal with problem and be happy.措施一父母采取措施Parents are supposed to spend more time educating their kids to do sthEg. to be happy facing difficulties.to put eggs in different baskets.（鸡蛋放在不同的篮子里）措施二Awareness about sth could be cultivated to make ourselves lead a healthy andfavorable life. 最后一句：喊口号！1) Only by taking these action can people have a more brilliant and gloriousfuture.2) So shouldn’t human beings pay much attention to the meaningfulsaying/problem?3) So under no account could people divert attention from the issue of sth.4) The more actively people face the issue, the more happily they will lead theirlife. （2）图画图表类第一段 第一二句主语从句引出描述图画或图表 It seems beyond dispute that in the vivid cartoon/chart …A son is telling his father that he is … while his father is saying that … 第三句总结图画中心思想 Simple as the cartoon looks, its meaning behind is really so far reaching:If you desire to do something great, you have to do it from small things. 第二段原因分析或举例 The majority of people would agree that sth has caused serious problems. It is superficially a simple phenomenon, but when subjected to analysis, ithas its fundamental reasons. There stand at least two reasons, from my perspective, for the presentphenomenon. However, recognizing a problem is the first step in finding a solution.to begin with 原因一in addition 原因二in the end 原因三 第三段解决问题 第一句 1.Therefore, it is imperative for us, people in all walks, to take drasticmeasures to reverse this disturbing trend.2.If we do not desire the trend to become a reality in the future, positive stepsmust be taken to put an end to sth right now.3.My suggestion, to put an end to the negative situation, are the followingsteps.4.The most important thing, confronted the current situation, is not to say, butinstead to do. 第二、三句话具体措施 for one thing / for another;on one hand / on the other hand;措施一父母采取措施Parents are supposed to spend more time educating their kids to do sth.Eg. to do little things well.措施二Awareness about sth could be cultivated to make ourselves lead a healthy andfavorable life.Eg. doing current things 第四句 1) Only by taking these action can people have a more brilliant and gloriousfuture.2) So shouldn’t human beings pay much attention to the meaningfulsaying/problem?3) So under no account could people divert attention from the issue of sth.4) The more actively people face the saying/issue, the more happily they willlead their life （3）论述类这类文章是论述自己的观点，跟第一类警句文章雷同，所以不再赘述. 举例：信息大爆炸是件好事吗？国企 or 外企国内读大学 or 国外读大学工作 or 考研就业 or 创业 （4）书信类举例：求职信1.大学快要毕业了，需要找工作，写一封求职信说明申请工作的原因和自己能胜任的理由2.推荐一个旅游景点 称呼文中已给出便照写.文中未给出：dear sir or madam /dear Mr. president/ professor/ editor 第一段:自我介绍+写作目的自我介绍：I am senior from the department of in university.写作目的：I am, to be frank, writing the letter in order to…(文中一定给出） 第二段:按文中要求来写比如推荐旅游景点There, to begin with, are so many beautiful flowers and trees on campus. You,undoubtedly, will enjoy it.In addition, the university is quite famous among Chinese for it has a historyabout 100 years long.Consequently, a sports meeting is held there and you could think about takingpart in it and make some new friends with some foreigners. 第三段a. 文中已经做出要求一句话来写文中要求的内容第二句话表示感谢或期待回信b. 未给出要求，则是：表示感谢+ 期待回信表示感谢：A. My thanks to you for your generous assistance are beyond words.B. Words fail me when I desire to express my sincere gratitude to you for yourkind consideration my requirement/application/complaint.C. I take the opportunity to show my heartfelt appreciation for your generousassistance you rendered me.17期待回信：A I am looking forward to your reply.B I look forward to a favorable reply at your earliest convenience.C Your prompt attention to my ……. would be highly appreciated.落款yours truly,Liming 我的警句类模板It is in the topic about… that there remains an growing interest in our contemporary society. (两句话解释警句) the meaning of the saying seems that…,which is distinctly beyond dispute. That is to say,… Adequately as immense amounts of cases can sustain my standpoint, the following one, from my perspective,is most favorable.（之后举一个详细的例子）【我（我叔叔）有一个善良热情（责任感强、有耐心）的朋友（邻居）xxx,他怎么怎么样 要切题】。 Therefore, it is indeed high time that due attention cannot have failed to paid to the issue.（国家政府、家长老师、个人三方面措施）First of all,(措施1).In addition,(措施2).What’s more,(措施3). The more actively private individuals confront the issue, the more happily they will lead their life. 我的图画类模板图表类 It seems beyond dispute that…(描述图画/表) in the vivid cartoon/chart. Simple as the cartoon looks, its meaning behind is really so far reaching.(点出背后的寓意或问题) Superficially as the phenomenon is, it has its fundamental reasons when subjected to analysis. There stand at least three reasons, from my perspective, for the present phenomenon. First of all,(原因1).In addition,( 原因2).What’s more,( 原因3). Therefore, it is indeed high time that due attention cannot have failed to paid to the issue. (若字数不够，加些措施).The more actively private individuals confront the issue, the more happily they will lead their life. 我的论述类模板It is in the topic about… that there remains an growing interest in our contemporary society. some people think that(观点1).whereas others argue that(观点2). As far as I am concerned, I agree with the opinion that(自己的观点).…which is distinctly beyond dispute. /in the general routine of everyday living. Adequately as immense amounts of cases can sustain my standpoint, the following one, from my perspective,is most favorable.（之后举一个详细的例子）【我（我叔叔）有一个善良热情（责任感强、有耐心）的朋友（邻居）xxx,他怎么怎么样 要切题】。 或者说明原因 Superficially(tough) as the phenomenon(choose) is, it has its fundamental reasons when subjected to analysis(inclined to sth). There stand at least three reasons, from my perspective, for the present phenomenon. First of all,(原因1).In addition,( 原因2).What’s more,( 原因3). Therefore, it is indeed high time that due attention cannot have failed to paid to the issue.（国家政府、家长老师、个人三方面措施）First of all,(措施1).In addition,(措施2).What’s more,(措施3). The more actively private individuals confront the issue, the more happily they will lead their life. 我的书信类模板Dear sir ​ I am student from the department of…. I am, to be frank, writing the letter in order to…(文中一定给出） 中间尽量和前面的模板挂钩 Words fail me when I desire to express my sincere gratitude to you for your kind consideration my requirement/application/complaint. I look forward to a favorable reply at your earliest convenience. yours truly, XXX 翻译①以意群为单位读句子， 确定句子时态②确定句子的主干、从而确定句型和语态③先翻译主干，非主干部分通过定语、状语、同位语的方式呈现④注意句子间的逻辑关系，加逻辑关系词 英语多被动 多长句 将汉语的短句改成长句方法如下: （1）非谓语动词谁的意思最重要,谁做谓语.其他动词做非谓语 （2）从句 （3）连词 综合举例 选词填空 传统阅读10min一篇 先读第一段及第二段的首句,了解文章中心(小于2min) 然后”出题顺序与行文顺序一致” 带着问题找答案,直接定位. 题型分类: 主旨题(和传统的主旨题不一样) :无论题干问什么 四个选项中当中和文章中心最接近的选项为正确答案,这题即为主旨题. 细节题 :除主旨题以外都是细节题 [定位句(关键词(时间地点人名)\ 顺序)+前后句] 细节题的正确答案一定来自文中的某句话(大概率是匹配字数最多). 长篇阅读15min (1)看大标题与小标题(了解文章中心 推测大概内容以及作者态度) (2)题的定位词 (3)如上词都没有,则常用动词 adj adv (4)文章中心词不能拿来定位 (5)重叠选项,得出答案(注意找出明显定位词后,阅读一下该句意思是否和题干一致) (6)有时间的话查漏补缺 . 听力 主旨题: 选项多为n \ 动名词 \ 概括性的词 数字题: 涉及到时间\金钱\数量 大部分即听即得,有时候要算. 观点态度题: 细节题:所听即所得 一定要预读 寻找中心词，推测文章大概内容 纵横对比，猜题目问题]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[自动化脚本]]></title>
    <url>%2Farticle%2F1fbb7b95%2F</url>
    <content type="text"><![CDATA[自动开关机设置12345::在1800s（即30分钟）后自动关机::shutdown -s -t 1800shutdown -s -t 10800::取消自动关机shutdown -a AutoXue123456::连接夜神模拟器adb connect 127.0.0.1:62001::开始自动运行e:cd E:\AutoXue-masterpython -m xuexi technologyXue123456::连接mumu模拟器adb connect 127.0.0.1:7555::开始自动运行e:cd E:\technologyPower-masterpython adb.py 完整版1234567891011121314151617181920212223242526::打开appium、夜神模拟器e:cd E:\AutoXuepython startAppiumAndNox.py::连接夜神模拟器adb connect 127.0.0.1:62001::挑战答题cd E:\AutoXue\2python -m xuexi::视听学习cd E:\AutoXue\3python videoArticle.py::打开appium、夜神模拟器::start /d "C:\Program Files\Appium\" Appium.exe::start /d "D:\Nox\bin\" Nox.exe::视听学习::E:::cd E:\AutoXue\4::python startApp.py::python adb.py cmd 运行Anaconda环境示例 1234#这句话是关键，调用Anaconda环境CALL D:\Anaconda3\Scripts\activate.bat D:\Anaconda3#在指定虚拟环境内运行python程序activate generalPython &amp;&amp; D: &amp;&amp; cd D:\PyCharm Community Edition 2019.3.4\workspace\clockIN\ &amp;&amp; python signIN.py &amp;&amp;pause win10定时任务https://blog.csdn.net/weixin_41712808/article/details/81567328 excel成绩表中自动将等级转换为分数12345678910111213141516171819202122232425262728293031323334353637383940414243Private Sub Worksheet_BeforeDoubleClick(ByVal Target As Range, Cancel As Boolean) StdGradeColumn = 1 'standard中的等级列 StdScoreColumn = 2 'standard中的等级对应的分数列 GradeNumber = 5 'standard中的等级个数，自己可以任意设置多个等级 TotalNum = 86 '学生总数 StartNum = 2 '开始行号 For i = StartNum To TotalNum + StartNum - 1 GradeColumn = 2 '等级列 ScoreColumn = 7 '分数列，这列的分数将自动计算 For k = 0 To 3 Set std = Worksheets("管理信息作业完成情况").Cells(i, GradeColumn + k) For j = 2 To GradeNumber + 1 SelectI = 1 Set Table = Worksheets("standard").Cells(j, StdGradeColumn) If StrComp(std.Value, Table.Value, 1) = 0 Then '比较文本 SelectI = j Exit For End If Next j Worksheets("管理信息作业完成情况").Cells(i, ScoreColumn + k) = Worksheets("standard").Cells(SelectI, StdScoreColumn) Next k Next i calColumn = 6 '总结果 GradeColumn = 7 '计算第7列至第10列 For i = StartNum To TotalNum + StartNum - 1 score = 0 For k = 0 To 3 score = score + Worksheets("管理信息作业完成情况").Cells(i, GradeColumn + k).Value Next k Worksheets("管理信息作业完成情况").Cells(i, calColumn) = score Next i End Sub QQ企业邮箱附件批量下载123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105import poplibimport emailimport timefrom email.parser import Parserfrom email.header import decode_headerimport tracebackimport sysimport telnetlibclass c_step4_get_email: # 字符编码转换 @staticmethod def decode_str(str_in): value, charset = decode_header(str_in)[0] if charset: value = value.decode(charset) return value # 解析邮件,获取附件 @staticmethod def get_att(msg_in): # import email attachment_files = [] for part in msg_in.walk(): # 获取附件名称类型 file_name = part.get_filename() if file_name: h = email.header.Header(file_name) # 对附件名称进行解码 dh = email.header.decode_header(h) filename = dh[0][0] if dh[0][1]: # 将附件名称可读化 filename = c_step4_get_email.decode_str(str(filename, dh[0][1])) # 下载附件 data = part.get_payload(decode=True) # 在指定目录下创建文件，注意二进制文件需要用wb模式打开 att_file = open('./work/' + filename, 'wb') attachment_files.append(filename) att_file.write(data) # 保存附件 att_file.close() return attachment_files @staticmethod def run_ing(): # 如果是QQ企业邮箱则输入：登录密码 ，普通邮箱输入授权码 # POP3服务器需要对应的地址和端口号 # 输入邮件地址, 口令和POP3服务器地址: email_user = 'XXXXXX' # 此处密码是授权码,用于登录第三方邮件客户端 password = 'XXXXXX' pop3_server = 'pop.exmail.qq.com' # 连接到POP3服务器,有些邮箱服务器需要ssl加密，可以使用poplib.POP3_SSL try: telnetlib.Telnet('pop.exmail.qq.com', 995) server = poplib.POP3_SSL(pop3_server, 995, timeout=10) except: time.sleep(5) server = poplib.POP3(pop3_server, 110, timeout=10) # 身份认证: server.user(email_user) server.pass_(password) # 返回邮件数量和占用空间: print('Messages: %s. Size: %s' %server.stat()) # list()返回所有邮件的编号: resp, mails, octets = server.list() # 可以查看返回的列表类似[b'1 82923', b'2 2184', ...] print(mails) index = len(mails) # 遍历邮件 for i in range(1, index + 1): resp, lines, octets = server.retr(i) # lines存储了邮件的原始文本的每一行, # 邮件的原始文本: msg_content = b'\r\n'.join(lines).decode('utf-8') # 解析邮件: msg = Parser().parsestr(msg_content) # 获取邮件时间,格式化收件时间 print(msg.get("Date")[0:24]) date1 = time.strptime(msg.get("Date")[0:24], '%a, %d %b %Y %H:%M:%S') # 邮件时间格式转换 date2 = time.strftime("%Y%m%d", date1) # 如果收件时间在20200520之后就下载附件 if date2 &gt; '20200520': # 获取附件 c_step4_get_email.get_att(msg) server.quit() print("下载附件完成")if __name__ == '__main__': origin = sys.stdout # 在log文件里查看输出的信息 f = open('./log.txt', 'w') sys.stdout = f try: c_step4_get_email.run_ing() except Exception as e: s = traceback.format_exc() print(e) tra = traceback.print_exc() sys.stdout = origin f.close()]]></content>
  </entry>
  <entry>
    <title><![CDATA[英语口语考试笔记]]></title>
    <url>%2Farticle%2F851220dd%2F</url>
    <content type="text"><![CDATA[六级口语考试 话题六级口试从头到尾只围绕一个话题进行深度讨论 自我介绍个人基本信息（姓名、籍贯…）、教育信息（所在大学专业与年级…）、个人拓展信息（性格、兴趣爱好、特长等…）、其他信息 自我介绍模板 自我介绍补充句 填充词 高能加分句 我的自我介绍1（It is my pleasure to introduce myself. ）My name is Wang zezu.（I'm from Changsha,Hunan Province.）I am a graduate student from HunNan University,majoring in Computer Science.I am highly proficient in computer programming. In my spare time,I like listening music,especially pop music.（That's pretty much about me.Thank you very much!） 1220秒My name is Wang zezu.I am a graduate student from HunNan University,majoring in Computer Science.I am highly proficient in computer programming. In my spare time,I like listening music,especially pop music. 语音基础部分元音：气流从口腔流出不受阻碍的音 问答题话题从3个方面进行展开： （1）现象 （2）原因 （3）结论 注意事项 同意对方观点12I agree with you...You can say that again... 高级补充 不同意对方观点12I have something different to say...I may not agree with you. 高级补充 引出自己观点12In my opinionwhat I want to say is that... 询问别人的观点12what's your opinion...what do you think... 强势自信碾压句型在论述完成自己观点加上以下句型 123Am I making myself clear?Are you asking me something about...?Have I given enough information? 万能模板第一次问答模板 一句话回答+拓展 最后一部分回答模板 回答（原因、论点）-》解释说明（1-2句）-》举例说明（1-2句） Topic Sentence -&gt;Supporting Ideas&gt; Examples 拓展句型三大模型WH 句型what who when where why how I + 动词 + 名词 +With my … + During my time off / when I’m feeling happy/down/bored + at school/home…/to many foreign countries + to enrich myself/kill time/loosen up/have fun/enjoy myself/expand my horizons 个人经历拓展I remenber when I was a kid ,I did something … .I thought it was so I started to(用于衬托喜好).It made me into a _ person. opinion扩展模式12345As far as I'm concerned..I strongly support the idea that..As far as I know..Personally I think..In my view.. 卡片陈述题模板1123There are some ideas concerning(问题)Firstly,(举例1).Secondly,(举例2).Thirdly,(举例3).in a word,(总结观点). 模板2123When asked about(问题),the majority of people say that(观点1).But as for me,(自己的观点).So how to solve the problem is worth paying attention to.First of all,(措施1).In addition,(措施2).What's more,(措施3).It's high time that we did something to(解决问题). 模板3123Nowadays,people are becoming increasingly aware of the significance of(主题词).From my point of view, (陈述观点).In the first place,(分论点1).In the second place,(分论点2)Taking account of all these factors,we may draw the conclusion that(重述观点) . 模板41234Different people have different views on(谈论主题).some people think that(观点1).whereas others argue that(观点2).as far as I am concerned, I agree with the opinion that(自己的观点).For one thing,I firmly believe that(原因1).For another,(原因2).Taking all these factors into consideration,we may come to the conclusion that (自己的观点). 我的模板1234Nowadays,people are becoming increasingly aware of the significance of(主题词)/When talked about(问题),the majority of people say that(观点1).But as for me,(自己的观点)./as far as I am concerned, I agree with the opinion that(自己的观点).So how to solve the problem is worth paying attention to.First of all,(措施1).In addition,(措施2).What's more,(措施3).It's high time that we did something to(解决问题). 万能句型12__ can enhance（提升/加强） someone's___ （能力）__ can cultivate （培养/锻炼/陶冶） someone's___（能力） 1__ keeps someone posted about(知道) the lastest___ （了解最新的..） 1___is good/bad for someone's ___（发展好坏） 1___is good way to express someone's(某种情感) towards___ 1___brings someone___(名词) 1___adds spice(调味料) to someone's life. 1___ helps someone get a better picture of ..对什么有更好的认识 1___is a necessary part of life. 1___has a positive/negative influence/effect on ___. 1___can fullfill people's___ 个性化素材准备 小组讨论1 问候 ，2 陈述自己的观点， 3 问问题 hello, I am glad to do the pair work with you. anything else to say about this topic 考试话题分布 卡片问题预测大学教育 大学生活 职场工作 找工作 男女平等 退休生活 尊敬的人 学习或生活习惯 城市的发展建设 废物循环利用 应对压力 禁言 常考素材 讨论预测机器人 过度医疗 扶老人 家庭关系 高科技课堂 校园养动物 电子支付 成为领导好还是被领导好]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络总结]]></title>
    <url>%2Farticle%2F72b93d83%2F</url>
    <content type="text"><![CDATA[协议体系OSI，TCP/IP，五层协议的体系结构OSI分层 （7层）：物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。 TCP/IP分层（4层）：网络接口层、网际层、运输层、应用层。 五层协议 （5层）：物理层、数据链路层、网络层、运输层、应用层。 各层协议及功能 功能 协议 设备 应用层 允许访问OSI环境的手段，提供各种应用程序接口（应用协议数据单元APDU） FTP、DNS、Telnet、SMTP、HTTP、WWW、NFS 表示层 对数据进行翻译、加密和压缩 （表示协议数据单元PPDU） JPEG、MPEG、ASII 会话层 建立、管理和终止会话 （会话协议数据单元SPDU） NFS、SQL、NETBIOS、RPC 传输层 提供端到端的可靠报文传递和错误恢复 （段Segment） TCP、UDP、SPX 网络层 负责数据包从源到宿的传递和网际互连 （包Packet） 寻址、路由选择 IP、ICMP、ARP、RARP、OSPF、IPX、RIP、IGRP 路由器 数据链路层 将比特组装成帧和点到点的传递（帧Frame） 差错校验、链路管理、媒介访问控制 PPP、FR、HDLC、VLAN、MAC 网桥，交换机 物理层 通过媒介传输比特,确定机械及电气规范 （比特Bit） RJ45、CLOCK、IEEE802.3 中继器，集线器，网关 主机间通信过程 交换机、路由器、网关的概念以及用途1）交换机在计算机网络系统中，交换机是针对共享工作模式的弱点而推出的。交换机拥有一条高带宽的背部总线和内部交换矩阵。交换机的所有端口都挂接在这条背部总线上，当控制电路收到数据包以后，处理端口会查找内存中的地址对照表以确定目的MAC（网卡的硬件地址）的NIC（网卡）挂接在哪个端口上，通过内部 交换矩阵迅速将数据包传送到目的端口。目的MAC若不存在，交换机才广播到所有的端口，接收端口回应后交换机会“学习”新的地址，并把它添加入内部地址表中。 交换机工作于数据链路层。交换机内部的CPU会在每个端口成功连接时，通过ARP协议学习它的MAC地址，保存成一张 ARP表。在今后的通讯中，发往该MAC地址的数据包将仅送往其对应的端口，而不是所有的端口。因此，交换机可用于划分数据链路层广播，即冲突域；但它不能划分网络层广播，即广播域。 交换机被广泛应用于二层网络交换，俗称“二层交换机”。 交换机的种类有：二层交换机、三层交换机、四层交换机、七层交换机分别工作在OSI七层模型中的第二层、第三层、第四层和第七层，并因此而得名。 2）路由器路由器（Router）是一种计算机网络设备，提供了路由与转送两种重要机制，可以决定数据包从来源端到目的端所经过的路由路径（host到host之间的传输路径），这个过程称为路由；将路由器输入端的数据包移送至适当的路由器输出端(在路由器内部进行)，这称为转送。路由工作在OSI模型的第三层——即网络层，例如网际协议。 路由器的一个作用是连通不同的网络，另一个作用是选择信息传送的线路。 路由器与交换器的差别，路由器是属于OSI第三层的产品，交换器是OSI第二层的产品(这里特指二层交换机)。 3）网关网关（Gateway），网关顾名思义就是连接两个网络的设备，区别于路由器（由于历史的原因，许多有关TCP/IP 的文献曾经把网络层使用的路由器（Router）称为网关，在今天很多局域网采用都是路由来接入网络，因此现在通常指的网关就是路由器的IP），经常在家庭中或者小型企业网络中使用，用于连接局域网和Internet。 网关也经常指把一种协议转成另一种协议的设备，比如语音网关。 在传统TCP/IP术语中，网络设备只分成两种，一种为网关（gateway），另一种为主机（host）。网关能在网络间转递数据包，但主机不能 转送数据包。在主机（又称终端系统，end system）中，数据包需经过TCP/IP四层协议处理，但是在网关（又称中介系统，intermediate system）只需要到达网际层（Internet layer），决定路径之后就可以转送。在当时，网关gateway）与路由器（router）还没有区别。 在现代网络术语中，网关（gateway）与路由器（router）的定义不同。网关（gateway）能在不同协议间移动数据，而路由器（router）是在不同网络间移动数据，相当于传统所说的IP网关（IP gateway）。 网关是连接两个网络的设备，对于语音网关来说，他可以连接PSTN网络和以太网，这就相当于VOIP，把不同电话中的模拟信号通过网关而转换成数字信号，而且加入协议再去传输。在到了接收端的时候再通过网关还原成模拟的电话信号，最后才能在电话机上听到。 对于以太网中的网关只能转发三层以上数据包，这一点和路由是一样的。而不同的是网关中并没有路由表，他只能按照预先设定的不同网段来进行转发。网关最重要的一点就是端口映射，子网内用户在外网看来只是外网的IP地址对应着不同的端口，这样看来就会保护子网内的用户。 4）网络接口卡（网卡）的功能（1）进行串行/并行转换。 （2）对数据进行缓存。 （3）在计算机的操作系统安装设备驱动程序。 （4）实现以太网协议。 5）网桥的作用网桥是一个局域网与另一个局域网之间建立连接的桥梁 计算机网络性能指标速率: 连接在网络上的主机在数字信道上传送数据位数的速率, b/s kb/s Mb/s Gb/s 吞吐量：是单位时间内通过网络的总数据量b/s Mb/s 时延：是数据（一个报文或分组，甚至比特）从网络或链路的一段传送到另一端所需要的时间。 发送时延：是从发送数据帧的第一个比特算起，到该帧的最后一个比特发送完毕所需的时间。发送时延=数据帧长度(b) /信道带宽(b/s) [信道带宽就是数据率] 传播时延：电磁波在信道中需要传播一定的距离而花费的时间。往返时延等于两倍的端到端传播时延传播时延=信道长度(m)/电磁波在信道上的传播速率(m/s) 处理时延：主机或路由器处理所收到的分组的时间。排队时延：分组在输入队列中排队等待处理，在输出队列中等待转发，就形成了排队时延。 总时延=发送时延+传播时延+处理时延+排队时延 信道利用率： TCP每发送一个窗口，需要进行等待确认信息回来，所以每发送完一个窗口，最快需要经过一个往返时延才可以发送下一个窗口（确认信息很小不考虑发送时延），所以在一个传输轮次中，包含一个发送时延和一个往返时延，而传输的数据量是一个窗口的大小（这里不考虑TCP、IP首部和帧的构成)所以最大吞吐量为一个窗口的大小除以一个传输轮次的时间 多种协议ARP 地址解析协议1：首先，每个主机都会在自己的ARP缓冲区中建立一个ARP列表，以表示IP地址和MAC地址之间的对应关系。 2：当源主机要发送数据时，首先检查ARP列表中是否有对应IP地址的目的主机的MAC地址，如果有，则直接发送数据，如果没有，就向本网段的所有主机发送ARP数据包，该数据包包括的内容有：源主机IP地址，源主机MAC地址，目的主机的IP地址。 3：当本网络的所有主机收到该ARP数据包时，首先检查数据包中的IP地址是否是自己的IP地址，如果不是，则忽略该数据包，如果是，则首先从数据包中取出源主机的IP和MAC地址写入到ARP列表中，如果已经存在，则覆盖，然后将自己的MAC地址写入ARP响应包中，告诉源主机自己是它想要找的MAC地址。 4：源主机收到ARP响应包后。将目的主机的IP和MAC地址写入ARP列表，并利用此信息发送数据。如果源主机一直没有收到ARP响应数据包，表示ARP查询失败。广播发送ARP请求，单播发送ARP响应。 RARP 逆地址解析协议作用是完成硬件地址到IP地址的映射，主要用于无盘工作站，因为给无盘工作站配置的IP地址不能保存。工作流程：在网络中配置一台RARP服务器，里面保存着IP地址和MAC地址的映射关系，当无盘工作站启动后，就封装一个RARP数据包，里面有其MAC地址，然后广播到网络上去，当服务器收到请求包后，就查找对应的MAC地址的IP地址装入响应报文中发回给请求者。因为需要广播请求报文，因此RARP只能用于具有广播能力的网络 ICMP协议 因特网控制报文协议它是TCP/IP协议族的一个子协议，用于在IP主机、路由器之间传递控制消息。 TFTP协议 简单文件传输协议TCP/IP协议族中的一个用来在客户机与服务器之间进行简单文件传输的协议，提供不复杂、开销不大的文件传输服务。 HTTP协议 超文本传输协议是一个属于应用层的面向对象的协议，由于其简捷、快速的方式，适用于分布式超媒体信息系统。 DHCP协议 动态主机配置协议是一种让系统得以连接到网络上，并获取所需要的配置参数手段。一个局域网的网络协议，使用UDP协议工作，用途：给内部网络或网络服务供应商自动分配IP地址，给用户或者内部网络管理员作为对所有计算机作中央管理的手段。 NAT协议网络地址转换属接入广域网(WAN)技术，是一种将私有（保留）地址转化为合法IP地址的转换技术， CSMA/CD协议 带有冲突检测的载波侦听多路存取 ， 是一个无法进行双全工的协议 。 先听后发 边听边发， 冲突停发 随机重发 CSMA/CD总线网中的计算公式： 最短数据帧长(bit)/数据传输速率(Mbps)=2*(两站点间的最大距离(m)/传播速度) 信号传播时延(μs)= 两站点间的距离(m)÷信号传播速度(200m/μs)，并且：数据传输时延 (s)=数据帧长度(bit)÷数据传输速率(bps) IP地址分类A类地址：以0开头， 第一个字节范围：0~127（1.0.0.1 - 126.255.255.254）； 缺省子网掩码：255.0.0.0 B类地址：以10开头， 第一个字节范围：128~191（128.0.0.1 - 191.255.255.254）； 缺省子网掩码：255.255.0.0 C类地址：以110开头， 第一个字节范围：192~223（192.0.0.1- 223.255.255.254）； 缺省子网掩码：255.255.255.0 10.0.0.0—10.255.255.255， 172.16.0.0—172.31.255.255， 192.168.0.0—192.168.255.255。（Internet上保留地址用于内部） 网络地址：IP地址与子网掩码相与 主机地址：将掩码取反，然后与运算） 子网划分自定义子网掩码(用于划分子网) 将一个网络划分为若干子网，希望每个子网拥有不同的网络地址或子网地址。因为IＰ是有限的，实际上我们是将主机地址分为两个部分：子网网络地址、子网主机地址。形式如下： 未做子网划分的ip地址：网络地址＋主机地址做子网划分后的ip地址：网络地址＋（子网网络地址＋子网主机地址） 等分成N个子网，子网掩码往右移动logN位。 网络前缀 掩码 点分十进制的地址化成二进制记法，1的个数就是网络前缀的个数。 TCP的三次握手与四次挥手三次握手（1）server处于Listen状态，表示服务器端的某个SOCKET处于监听状态，可以接受连接了； （2）当client端socket执行connect连接时，首先发送SVN报文到server，进入SVN_SENT状态，等待server发送ACK； （3）server接受到SVN进入SVN_RCVD状态，（很短暂，一般查询不到），发送SVN+ACK给client端； （4）client端接受到server的ACK，发送ACK给server，server接收到后进入established状态，client也进入established状态。 四次挥手：（1）client发起断开连接，给server发送FIN，进入FIN_WAIT1状态，表示client想主动断开连接；（2）server接受到FIN字段后，会继续发送数据给client端，并发送ACK给client端，表明自己知道了，但是还没有准备好断开，请等我的消息；（3） 当server确定自己的数据已经发送完成，就发送FIN到client；（4）client接受到来自server的FIN，发送ACK给server端，表示可以断开连接了，再等待2ms，没有收到server端的数据后，表示可以正常断开连接。 TCP/IP补充： 最大传输单元 （MTU, IP层下面数据链路层所限定的帧格式中数据字段的最大长度，与IP数据报首部中的总长度字段有关系 ）限制了数据帧的最大长度。以太网的MTU为1500字节，最小帧为64字节。 以太网规定帧间最小间隔为9.6 微秒 ,相当于96比特时间；以太网取51.2微秒做争用期 ,相当于512比特时间. 在IP网络中 DNS（域名系统） 是负责主机IP地址与主机名称之间的转换协议，ARP（地址解析协议） 是负责IP地址与MAC地址之间的转换协议。 TCP报文 一个TCP报文段分为首部和数据两部分。首部由固定部分和选项部分组成，固定部分是20字节。TCP首部的最大长度为60。首部固定部分字段： IP数据报由首部 和数据 两部分组成。首部由固定部分和可选部分 组成。首部的固定部分有20字节。可选部分的长度变化范围为1——40字节。固定部分的字段： 字段名 位数（bit） 字段名 位数 版本 4 Ipv4 首部长度 4（表示的最大数为15个单位，一个单位表示4字节） 服务类型 8 以前很少用 总长度 16 （首部和数据部分的总长度，因此数据报的最大长度为65535字节，即64KB，但是由于链路层的MAC都有一定的最大传输单元，因此IP数据报的长度一般都不会有理论上的那么大，如果超出了MAC的最大单元就会进行分片） 标识 16 （相同的标识使得分片后的数据报片能正确的重装成原来的数据报） 标志 3 （最低位MF=1表示后面还有分片，MF=0表示这是若干个数据报片的最后一个中间位DF=0才允许分片） 片偏移 片偏移指出较长的分组在分片后，某片在原分组中的相对位置，都是8字节的偏移位置 生存时间 数据报在网络中的生存时间，指最多经过路由器的跳数 协议 8 （指出该数据报携带的数据是何种协议，以使得目的主机的IP层知道应将数据部分上交给哪个处理程序）如ICMP=1 IGMP=2 TCP=6 EGP=8 IGP=9 UDP=17 Ipv6=41 OSPF=89 首部校验和 这个部分只校验首部，不包括数据部分，计算方法：将首部划分为多个16位的部分，然后每个16位部分取反，然后计算和，再将和取反放到首部校验和。接收方收到后按同样的方法划分，取反，求和，在取反，如果结果为零，则接收，否则就丢弃 源地址 32 目的地址 32 UDP数据报 用户数据报UDP由首部和数据部分组成。首部只有8个字节，由4个字段组成，每个字段都是两个字节。 字段名 字节 字段名 字节 源端口 2 目的端口 2 长度 2 检验和 2 （检验首部和数据，加12字节的伪首部） RTT与RTORTT: 发送一个数据包到收到对应的ACK，所花费的时间 RTO: 发送数据包，启动重传定时器，重传定时器到期所花费的时间，称为RTO 对于segment的重传，重传的时间RTO设定是非常重要的，如果设置太短，可能会导致并没有丢包而重传，如果设置太长了，可能因为等待ACK而浪费掉很多时间，牺牲传输的效率。 [RFC6298] 习题内有例题 第一次RTO计算方法, 假设RTT = R SRTT = R RTTVAR = R/2 RTO = SRTT + max(G, K*RTTVAR) , K = 4 后续的RTO计算,假设当前的RTT为R’ RTTVAR = (1 - beta)RTTVAR + beta|SRTT - R’| 计算平滑RTT和真实RTT的差距，切记这个地方的SRTT是上一次的SRTT SRTT = (1 - alpha)SRTT + alphaR’ 计算平滑RTT RTO = SRTT + max(G, K*RTTVAR) alpha = 1/8 beta = 1/4 流量控制利用滑动窗口实现流量控制，如果发送方把数据发送得过快，接收方可能会来不及接收，这就会造成数据的丢失。所谓流量控制就是让发送方的发送速率不要太快，要让接收方来得及接收。 TCP为每一个连接设有一个持续计时器(persistence timer)。只要TCP连接的一方收到对方的零窗口通知，就启动持续计时器。若持续计时器设置的时间到期，就发送一个零窗口控测报文段（携1字节的数据），那么收到这个报文段的一方就重新设置持续计时器。 拥塞控制及其方法防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。拥塞控制所要做的都有一个前提：网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素。 拥塞控制代价：需要获得网络内部流量分布的信息。在实施拥塞控制之前，还需要在结点之间交换信息和各种命令，以便选择控制的策略和实施控制。这样就产生了额外的开销。拥塞控制还需要将一些资源分配给各个用户单独使用，使得网络资源不能更好地实现共享。 几种拥塞控制方法：慢开始(slow-start )、拥塞避免(congestion avoidance )、快重传( fastretransmit )和快恢复( fastrecovery )。 慢开始和拥塞避免发送方维持一个拥塞窗口cwnd ( congestion window )的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞窗口。 发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数。 慢开始算法：当主机开始发送数据时，如果立即把大量数据字节注入到网络，那么就有可能引起网络拥塞，因为现在并不清楚网络的负荷情况。因此，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是说，由小到大逐渐增大拥塞窗口数值。通常在刚刚开始发送报文段时，先把拥塞窗口 cwnd 设置为一个最大报文段MSS的数值。而在每收到一个对新的报文段的确认后，把拥塞窗口增加至多一个MSS的数值。用这样的方法逐步增大发送方的拥塞窗口 cwnd ，可以使分组注入到网络的速率更加合理。 每经过一个传输轮次，拥塞窗口 cwnd 就加倍。一个传输轮次所经历的时间其实就是往返时间RTT。不过“传输轮次”更加强调：把拥塞窗口cwnd所允许发送的报文段都连续发送出去，并收到了对已发送的最后一个字节的确认。 另，慢开始的“慢”并不是指cwnd的增长速率慢，而是指在TCP开始发送报文段时先设置cwnd=1，使得发送方在开始时只发送一个报文段（目的是试探一下网络的拥塞情况），然后再逐渐增大cwnd。 为了防止拥塞窗口cwnd增长过大引起网络拥塞，还需要设置一个慢开始门限ssthresh状态变量（如何设置ssthresh）。慢开始门限ssthresh的用法如下： 当 cwnd &lt; ssthresh 时，使用上述的慢开始算法。 当 cwnd &gt; ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。 当 cwnd = ssthresh 时，既可使用慢开始算法，也可使用拥塞控制避免算法。 拥塞避免算法：让拥塞窗口cwnd缓慢地增大，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍。这样拥塞窗口cwnd按线性规律缓慢增长，比慢开始算法的拥塞窗口增长速率缓慢得多。 无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有收到确认），就要把慢开始门限ssthresh设置为出现拥塞时的发送方窗口值的一半（但不能小于2）。然后把拥塞窗口cwnd重新设置为1，执行慢开始算法。这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。过程图如下： 快速重传：那就是收到3个相同的ACK。TCP在收到乱序到达包时就会立即发送ACK，TCP利用3个相同的ACK来判定数据包的丢失，此时进行快速重传，快速重传做的事情有： 把ssthresh设置为cwnd的一半 把cwnd再设置为ssthresh的值(具体实现有些为ssthresh+3) 重新进入拥塞避免阶段。 快速恢复 当收到3个重复ACK时，把ssthresh设置为cwnd的一半，把cwnd设置为ssthresh的值加3，然后重传丢失的报文段，加3的原因是因为收到3 再收到重复的ACK时，拥塞窗口增加1。 收到新的数据包的ACK时，把cwnd设置为第一步中的ssthresh的值。原因是因为该ACK确认了新的数据，说明从重复ACK时的数据都已收到，该恢复过程已经结束，可以回到恢复之前的状态了，也即再次进入拥塞避免状态。 常见问题TCP和UDP的区别？ 区别 TCP UDP 1.连接 面向连接 面向非连接 2.可靠性 可靠 非可靠 3.有序性 有序 不保证有序 4.速度 慢 快 5.量级 重量级 轻量级 6.拥塞控制或流量控制 有 没有 7 面向对象 面向字节流，无记录边界 面向报文，有记录边界 8 传播方式 只能单播 可以广播或组播 9.应用场景 效率低，准确性高 效率高，准确性低 对应协议 方式 功能 端口号 FTP TCP 定义了文件传输协议 21 Telnet TCP 一种用于远程登陆的端口，用户可以以自己的身份远程连接到计算机上，可提供基于DOS模式下的通信服务。 23 SMTP TCP 邮件传送协议，用于发送邮件。 25 POP3 TCP 它是和SMTP对应，POP3用于接收邮件。 110 HTTP TCP 是从Web服务器传输超文本到本地浏览器的传送协议。 DNS UDP 用于域名解析服务，将域名地址转换为IP地址 53 SNMP UDP 简单网络管理协议，是用来管理网络设备的。由于网络设备很多，无连接的服务就体现出其优势 161 TFTP UDP 简单文件传输协议 69 为什么TIME_WAIT状态还需要等2*MSL秒之后才能返回到CLOSED状态呢？（Max SegmentLifetime，最大分段生存期） 因为虽然双方都同意关闭连接了，而且握手的4个报文也都发送完毕，按理可以直接回到CLOSED状态（就好比从SYN_SENT状态到ESTABLISH状态那样），但是我们必须假想网络是不可靠的，你无法保证你最后发送的ACK报文一定会被对方收到，就是说对方处于LAST_ACK状态下的SOCKET可能会因为超时未收到ACK报文，而重发FIN报文，所以这个TIME_WAIT状态的作用就是用来重发可能丢失的ACK报文。 为什么TCP连接要建立三次连接？为了防止失效的连接请求又传送到主机，因而产生错误。如果使用的是两次握手建立连接，假设有这样一种场景，客户端发送了第一个请求连接并且没有丢失，只是因为在网络结点中滞留的时间太长了，由于TCP的客户端迟迟没有收到确认报文，以为服务器没有收到，此时重新向服务器发送这条报文，此后客户端和服务器经过两次握手完成连接，传输数据，然后关闭连接。此时此前滞留的那一次请求连接，网络通畅了到达了服务器，这个报文本该是失效的，但是，两次握手的机制将会让客户端和服务器再次建立连接，这将导致不必要的错误和资源的浪费。 如果采用的是三次握手，就算是那一次失效的报文传送过来了，服务端接受到了那条失效报文并且回复了确认报文，但是客户端不会再次发出确认。由于服务器收不到确认，就知道客户端并没有请求连接。 为什么TCP断开连接要4次挥手？TCP协议是一种面向连接的、可靠的、基于字节流的传输层通信协议，是一个全双工模式： 1、当主机A确认发送完数据且知道B已经接受完了，想要关闭发送数据口（当然确认信号还是可以发），就会发FIN给主机B。 2、主机B收到A发送的FIN，表示收到了，就会发送ACK回复。 3、但这是B可能还在发送数据，没有想要关闭数据口的意思，所以FIN与ACK不是同时发送的，而是等到B数据发送完了，才会发送FIN给主机A。 4、A收到B发来的FIN，知道B的数据也发送完了，回复ACK， A等待2MSL以后，没有收到B传来的任何消息，知道B已经收到自己的ACK了，A就关闭链接，B也关闭链接了。 确保数据能够完成传输。 如果已经建立了连接，但是客户端突然出现故障了怎么办？TCP还设有一个保活计时器，显然，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75分钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。 在浏览器中输入www.baidu.com后执行的全部过程 从网络模型的角度来分析问题的，主要涉及应用层：DNS,HTTP,传输层：TCP,网络层：IP和路由选择协议：RIP,OSPF(内部网关协议),BGP(外部网关协议）和数据链路层：ARP。 1、客户端浏览器通过DNS解析到www.baidu.com的IP地址220.181.27.48，通过这个IP地址找到客户端到服务器的路径。客户端浏览器发起一个HTTP会话到220.161.27.48，然后通过TCP进行封装数据包，输入到网络层。 2、在客户端的传输层，把HTTP会话请求分成报文段，添加源和目的端口，如服务器使用80端口监听客户端的请求，客户端由系统随机选择一个端口如5000，与服务器进行交换，服务器把相应的请求返回给客户端的5000端口。然后使用IP层的IP地址查找目的端。 3、客户端的网络层主要做的是通过查找路由表确定如何到达服务器，期间可能经过多个路由器，这些都是由路由器来完成的工作，我不作过多的描述，无非就是通过查找路由表决定通过那个路径到达服务器。 4、客户端的链路层，包通过链路层发送到路由器，通过邻居协议查找给定IP地址的MAC地址，然后发送ARP请求查找目的地址，如果得到回应后就可以使用ARP的请求应答交换的IP数据包现在就可以传输了，然后发送IP数据包到达服务器的地址。 HTTP的长连接和短连接HTTP的长连接和短连接本质上是TCP长连接和短连接。HTTP属于应用层协议. 短连接:浏览器和服务器每进行一次HTTP操作，就建立一次连接，但任务结束就中断连接。 长连接:当一个网页打开完成后，客户端和服务器之间用于传输HTTP数据的 TCP连接不会关闭，如果客户端再次访问这个服务器上的网页，会继续使用这一条已经建立的连接。Keep-Alive不会永久保持连接，它有一个保持时间，可以在不同的服务器软件（如Apache）中设定这个时间。实现长连接要客户端和服务端都支持长连接。 TCP短连接: client向server发起连接请求，server接到请求，然后双方建立连接。client向server发送消息，server回应client，然后一次读写就完成了，这时候双方任何一个都可以发起close操作，不过一般都是client先发起 close操作.短连接一般只会在 client/server间传递一次读写操作 TCP长连接: client向server发起连接，server接受client连接，双方建立连接。Client与server完成一次读写之后，它们之间的连接并不会主动关闭，后续的读写操作会继续使用这个连接。 RIP使用UDP，OSPF使用IP，而BGP使用TCP。这样做有何优点？为什么RIP周期性地和临站交换路由器由信息而BGP却不这样做？答：RIP只和邻站交换信息，使用UDP无可靠保障，但开销小，可以满足RIP要求； OSPF使用可靠的洪泛法，直接使用IP，灵活、开销小； BGP需要交换整个路由表和更新信息，TCP提供可靠交付以减少带宽消耗； RIP使用不保证可靠交付的UDP，因此必须不断地（周期性地）和邻站交换信息才能使路由信息及时得到更新。但BGP使用保证可靠交付的TCP因此不需要这样做。 运输层协议与网络层协议的区别？网络层协议负责的是提供主机间的逻辑通信 运输层协议负责的是提供进程间的逻辑通信 数据链路层协议可能提供的服务？成帧、链路访问、透明传输、可靠交付、流量控制、差错检测、差错纠正、半双工和全双工。最重要的是帧定界（成帧）、透明传输以及差错检测。 静态路由和动态路由有什么区别？静态路由是由管理员手工配置的，适合比较简单的网络或需要做路由特殊控制。而动态路由则是由动态路由协议自动维护的，不需人工干预，适合比较复杂大型的网络。 路由器能够自动地建立自己的路由表，并且能够根据实际实际情况的变化适时地进行调整。动态路由机制的运作依赖路由器的两个基本功能：对路由表的维护；路由器之间适时的路由信息交换。 在Linux环境中怎么配置一条默认路由？在linux上可以用“route add default gw&lt;默认路由器 IP&gt;”命令配置一条默认路由。 习题TCP拥塞控制TCP的拥塞窗口cwnd大小与传输轮次n的关系如下所示： （1）试画出如图5-25所示的拥塞窗口与传输轮次的关系曲线。 （2）指明TCP工作在慢开始阶段的时间间隔。 （3）指明TCP工作在拥塞避免阶段的时间间隔。 （4）在第16轮次和第22轮次之后发送方是通过收到三个重复的确认还是通过超市检测到丢失了报文段？ （5）在第1轮次，第18轮次和第24轮次发送时，门限ssthresh分别被设置为多大？ （6）在第几轮次发送出第70个报文段？ （7）假定在第26轮次之后收到了三个重复的确认，因而检测出了报文段的丢失，那么拥塞窗口cwnd和门限ssthresh应设置为多大？ （4）在第16轮次和第22轮次之后发送方是通过收到三个重复的确认还是通过超时检测到丢失了报文段？ 在第16轮次是通过收到三个重复的确认（因为cwnd减少到原来的一半，继续进行拥塞避免操作）而检测到丢失了报文段，第22轮次是通过超时检测（因为cwnd重新赋值为1，继续进行慢开始操作）而检测到丢失了报文段。 （5）在第1轮次，第18轮次和第24轮次发送时，门限ssthresh分别被设置为 32 、21、13 （6）在第几轮次发送出第70个报文段？第7，因为（1+2+4+8+16+32+33）&gt; 70 （7）假定在第26轮次之后收到了三个重复的确认，因而检测出了报文段的丢失，那么拥塞窗口cwnd和门限ssthresh应设置为 8/2=4 TCP报文主机A向主机B连续发送了两个TCP报文段，其序号分别为70和100。试问： （1） 第一个报文段携带了多少个字节的数据？ （2） 主机B收到第一个报文段后发回的确认中的确认号应当是多少？ （3） 如果主机B收到第二个报文段后发回的确认中的确认号是180，试问A发送的第二个报文段中的数据有多少字节？ （4） 如果A发送的第一个报文段丢失了，但第二个报文段到达了B。B在第二个报文段到达后向A发送确认。试问这个确认号应为多少？ 解：（1）第一个报文段的数据序号是70到99，共30字节的数据。 （2）确认号应为100. （3）80字节。 （4）70 TCP超时重传假定TCP在开始建立连接时，发送方设定超时重传时间是RTO=6s。 （1）当发送方接到对方的连接确认报文段时，测量出RTT样本值为1.5s。试计算现在的RTO值。 （2）当发送方发送数据报文段并接收到确认时，测量出RTT样本值为2.5s。试计算现在的RTO值。 解：根据RFC6298 RTT(1) = 1.5s SRTT(1) = RTT(1) = 1.5s RTTVAR(1) = 1.5/2 = 0.75 RTO = 1.5 + 4*0.75 = 4.5s 所以现在RTO的值为4.5s 接着计算第二步 RTT (2) = 2.5 RTTVAR(2) = 3/4 0.75 + 1/4 |1.5 - 2.5| = 13/16 SRTT(2) = 7/8 1.5 + 1/82.5 = 1.625 RTO = 1.625 + 4 * 13/16 = 4.875 IP分片1一个UDP用户数据的数据字段为8192季节。在数据链路层要使用以太网来传送。试问应当划分为几个IP数据报片？说明每一个IP数据报字段长度和片偏移字段的值。 答：6个 数据字段的长度：前5个是1480字节，最后一个是800字节。 片偏移字段（ 片偏移就是某片在原分组的相对位置，以8个字节为偏移单位）的值分别是：0，185（1480/8），370 （2960/8），555 （4440/8），740 （5920/8）和925 （7400/8）. 因为UDP用户数据报的数据字段为8192字节，所以数据报文的长度是8192+upd首部8字节=8200。 所以第6个数据报片使8200-1480*5=800 注意：链路层具有最大传输单元MTU这个特性，它限制了数据帧的最大长度 以太网的MTU为1500字节，一般IP首部为20字节，UDP首部为8字节，数据的净荷（payload）部分预留是1500-20-8=1472字节。如果数据部分大于1472字节，就会出现分片现象。 1一个数据报长度为4000字节（固定首部长度）。现在经过一个网络传送，但此网络能够传送的最大数据长度为1500字节。试问应当划分为几个短些的数据报片？各数据报片的数据字段长度、片偏移字段和MF标志应为何数值？ 答：IP数据报固定首部长度为20字节 总长度(字节) 数据长度(字节) MF 片偏移 原始数据报 4000 3980 0 0 数据报片1 1500 1480 1 0 数据报片2 1500 1480 1 185 数据报片3 1040 1020 0 370 IP数据报1主机A发送IP数据报给主机B，途中经过了5个路由器。试问在IP数据报的发送过程中总共使用了几次ARP？ 答：6次，主机用一次，每个路由器各使用一次。 ARQ协议 假定使用连续ARQ协议中，发送窗口大小事3，而序列范围[0,15],而传输媒体保证在接收方能够按序收到分组。在某时刻，接收方，下一个期望收到序号是5. 试问： （1）在发送方的发送窗口中可能有出现的序号组合有哪几种？ （2）接收方已经发送出去的、但在网络中（即还未到达发送方）的确认分组可能有哪些？说明这些确认分组是用来确认哪些序号的分组。 路由选择1234567891011121314151617假定网络中的路由器B的路由表有如下的项目（这三列分别表示“目的网络”、“距离”和“下一跳路由器”）N1 7 AN2 2 CN6 8 FN8 4 EN9 4 F现在B收到从C发来的路由信息（这两列分别表示“目的网络”和“距离” ）：N2 4N3 8N6 4N8 3N9 5试求出路由器B更新后的路由表（详细说明每一个步骤）。 解：路由器B更新后的路由表如下： N1 7 A 无新信息，不改变 N2 5 C 相同的下一跳，更新 N3 9 C 新的项目，添加进来 N6 5 C 不同的下一跳，距离更短，更新 N8 4 E 不同的下一跳，距离一样，不改变 N9 4 F 不同的下一跳，距离更大，不改变 1234567891011121314151617181920212223242526272829设某路由器建立了如下路由表：目的网络 子网掩码 下一跳------------------------------------------------------------------128.96.39.0 255.255.255.128 接口0128.96.39.128 255.255.255.128 接口1128.96.40.0 255.255.255.128 R2192.4.153.0 255.255.255.192 R3*（默认） - R4现共收到5个分组，其目的站IP地址分别为：（1）128.96.39.10（2）128.96.40.12（3）128.96.40.151（4）192.4.153.17（5）192.4.153.90试分别计算其下一跳。 解：（1）分组的目的站IP地址为：128.96.39.10。先与子网掩码255.255.255.128相与，得128.96.39.0，可见该分组经接口0转发。 （2）分组的目的IP地址为：128.96.40.12。 ① 与子网掩码255.255.255.128相与得128.96.40.0，不等于128.96.39.0。 ② 与子网掩码255.255.255.128相与得128.96.40.0，经查路由表可知，该项分组经R2转发。 （3）分组的目的IP地址为：128.96.40.151，与子网掩码255.255.255.128相与后得128.96.40.128，与子网掩码255.255.255.192相与后得128.96.40.128，经查路由表知，该分组转发选择默认路由，经R4转发。 （4）分组的目的IP地址为：192.4.153.17。与子网掩码255.255.255.128相与后得192.4.153.0。与子网掩码255.255.255.192相与后得192.4.153.0，经查路由表知，该分组经R3转发。 （5）分组的目的IP地址为：192.4.153.90，与子网掩码255.255.255.128相与后得192.4.153.0。与子网掩码255.255.255.192相与后得192.4.153.64，经查路由表知，该分组转发选择默认路由，经R4转发。 网络前缀12以下的地址前缀中哪一个地址和2.52.90.140匹配？请说明理由。（1）0/4；（2）32/4；（3）4/6；（4）80/4 答：（1）2.52.90.140与11110000 00000000 00000000 00000000逐比特相“与”和0/4匹配 （2）2.52.90.140与11110000 00000000 00000000 00000000逐比特相“与”和32/4不匹配 （3）2.52.90.140与11110000 00000000 00000000 00000000逐比特相“与”和4/6不匹配 （4）2.52.90.140与11110000 00000000 00000000 00000000逐比特相“与”和80/4不匹配 CRC校验1生成多项式:G(X)=X4+X3+1，要求出二进制序列10110011的CRC校验码。 （1）G(X)=X4+X3+1,二进制比特串为11001;(有X的几次方，对应的2的几次方的位就是1) （2）因为校验码4位，所以10110011后面再加4个0，得到101100110000，用“模2除法”(其实就是异或)即可得出结果； 余数就是 FCS（帧校验序列 ，俗称帧尾 ） （3）CRC^101100110000得到101100110100。发送到接收端； （4）接收端收到101100110100后除以11001(以“模2除法”方式去除),余数为0则无差错； 码分多址CDMA通信1234567共有4个站进行码分多址CDMA通信。4个站的码片序列为：A：（-1 –1 –1 +1 +1 –1 +1 +1） B：（-1 –1 +1 -1 +1 +1 +1 -1）C：（-1 +1 –1 +1 +1 +1 -1 -1） D：（-1 +1 –1 –1 -1 –1 +1 -1）现收到这样的码片序列：（-1 +1 –3 +1 -1 –3 +1 +1）。问哪个站发送数据了？发送数据的站发送的1还是0？ 答：S·A=（＋1－1＋3＋1－1＋3＋1＋1）／8=1， A发送1 S·B=（＋1－1－3－1－1－3＋1－1）／8=－1， B发送0 S·C=（＋1＋1＋3＋1－1－3－1－1）／8=0， C无发送 S·D=（＋1＋1＋3－1＋1＋3＋1－1）／8=1， D发送1 数据链路层11、假定站点A和B在同一个10Mb/s以太网网段上。这两个站点之间的传播时延为225比特时间。现假定A开始发送一帧，并且在A发送结束之前B也发送一帧。如果A发送的是以太网所容许的最短的帧，那么A在检测到和B发生碰撞之前能否把自己的数据发送完毕？换言之，如果A在发送完毕之前并没有检测到碰撞，那么能否肯定A所发送的帧不会和B发送的帧发生碰撞？（提示：在计算时应当考虑到每一个以太网帧在发送到信道上时，在MAC帧前面还要增加若干字节的前同步码和帧定界符） 答： 设在t=0时A开始发送，在t=（64+8(MAC帧要加8个字节同步码和帧开始定界符)）*8=576比特时间，A应当发送完毕。t=225比特时间，B就检测出A的信号。只要B在t=224比特时间之前发送数据，A在发送完毕之前就一定检测到碰撞，就能够肯定以后也不会再发送碰撞了。如果A在发送完毕之前并没有检测到碰撞，那么就能够肯定A所发送的帧不会和B发送的帧发生碰撞（当然也不会和其他站点发生碰撞）。 12、在上题中的站点A和B在t=0时同时发送了数据帧。当t=255比特时间，A和B同时检测到发生了碰撞，并且在t=255+48=273比特时间完成了干扰信号的传输。A和B在CSMA/CD算法中选择不同的r值退避。假定A和B选择的随机数分别是rA=0和rB=1。试问A和B各在什么时间开始重传其数据帧？A重传的数据帧在什么时间到达B？A重传的数据会不会和B重传的数据再次发生碰撞？B会不会在预定的重传时间停止发送数据？ 答：t=0时，A，B开始传输数据； ​ t=225比特时间，A和B同时检测到发生碰撞； t=225+48=273比特时间，完成了干扰信号的传输； 开始各自进行退避算法： A： 因为rA=0，则A在干扰信号传输完之后立即开始侦听 t=273+225（传播时延）=498比特时间，A检测到信道开始空闲 t=498+96（帧间最小间隔）=594比特时间，A开始重传数据 —–第一问A的重传时间 t=594+225 （传播时延）=819比特时间，A重传完毕 —-第二问A重传的数据帧到达B的时间 B： 因为rB=1，则B在干扰信号传输完之后1倍的争用期，即512比特时间才开始侦听 t=273+512=785比特时间，B开始侦听 若侦听空闲，则 t=785+96（帧间最小间隔）=881比特时间，B开始重传数据 若侦听费空闲，则继续退避算法 又因为t=819比特时间的时候，A才重传数据完毕，所以B在785比特时间侦听的时候，肯定会侦听信道非空闲，即B在预定的881比特时间之前侦听到信道忙， 所以，第四问的答案：B在预定的881比特时间是停止发送数据的。 即第三问A重传的数据不会和B重传的数据再次发生碰撞。 计算机网络性能计算1通信信道带宽为1Gb／s，端到端时延为10ms。TCP的发送窗口为65535字节。试问:可能达到的最大吞吐量是多少? 信道的利用率是多少? 往返时延等于两倍的端到端传播时延，即20ms=0.02s发送时延等于窗口数据量除以带宽，即655358/10^9秒TCP每发送一个窗口，需要进行等待确认信息回来，所以每发送完一个窗口，最快需要经过一个往返时延才可以发送下一个窗口（确认信息很小不考虑发送时延），所以在一个传输轮次中，包含一个发送时延和一个往返时延，而传输的数据量是一个窗口的大小（这里不考虑TCP、IP首部和帧的构成)所以最大吞吐量为一个窗口的大小除以一个传输轮次的时间，即655358/(65535*8/10^9+0.02)=25.54Mbit/s信道利用率为25.54Mbit/s/1000Mbit/s=2.55% 参考文章计算机网络——计算机网络常见面试题总结 在浏览器中输入URL后执行的全部过程的个人总结 计算机网络考试 复习时你应该要看的几道题！ TCP-IP详解: RTT和RTO的计算方法 计算机网络学习笔记——第三章课后题答案详解 子网划分及子网掩码计算方法 子网划分详解]]></content>
      <categories>
        <category>Computer Networking</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[国际交流实用英文写作]]></title>
    <url>%2Farticle%2F37c8b7ce%2F</url>
    <content type="text"><![CDATA[书信齐头式、改良齐头式、半齐头式或缩进式 Main Section 示例 邮件格式与书信类似 内容 示例 The Opening(前言) （1）直抒主要想法 （2）在第一段再次重述并阐释目的 The Body(主体) （1）讲述主要内容 （2）提供更多信息，逻辑地解释、讨论主题 The Closing(结尾) （1）展示欺骗与关心 （2）请求~~ （3）表示感谢 Skillpositive and polite negative formal(正式)（1）选择正确的词句 （2）句子结构尽量采用高级表达，如非谓语结构…]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[win10【uefi+gpt】安装or删除双系统Ubuntu16.04/18.04]]></title>
    <url>%2Farticle%2Fb234d6bf%2F</url>
    <content type="text"><![CDATA[相关环境Win10【原先是legacy bios引导+mbr分区】后来重装系统后改为【uefi引导+gpt分区】 Ubuntu16.04 (考虑到稳定性，程序员永远别用最新的…) 随着时间推移 18.04也ok，操作类似 微星主板（头一回看见有可视化界面的Bios） Win10安装在SSD上，Ubuntu安装在HD上 所以这篇文章本质上是（uefi+gpt）版的win10安装双系统Ubuntu16.04，若不和本文相符，绕道… labtop备注F2 进入bois 修改后到没有选项的首页 按Esc 选 Yes 按 Enter F12 进入 boot选项界面 备份一切有关搭建的举动，备份好重要的数据，再进行操作。 查看Win10启动方式（重要，避坑）按Win图标键+X键，点命令提示符(管理员) 输入bcdedit回车，然后找path开头的行，结尾是exe的话就是Legacy，结尾是efi的话就是UEFI 方法二 ：Dos命令框运行输入命令：msinfo32 传统即为legacy ，UEFI就会显示UEFI 所以我的win10就是legacy BIOS启动方式 但是后来重装变了，UEFI的好处的不用再考虑磁盘主分区个数的问题，读取数据也加快了。 确定磁盘分区表格式（重要，避坑） 制作U盘启动盘ps: 本文最后是制作UEFI的启动盘，方法类似 （1）准备4G以上U盘一个 （2）下载软碟通（UltraISO）、Ubuntu16.04的镜像文件、以及分区助手（DiskGenius）。 ​ 链接：https://pan.baidu.com/s/1RlF49Ol9XNQf7_vwyTfIiw​ 提取码：ejh3 如果你还是要参考本文，请制作bios模式不是UEFI的启动盘。。现在大多数默认UEFI （3）按如下经验开始制作 使用UltraISO（软碟通）制作U盘启动盘完整教程 准备Ubuntu的磁盘区（1）想清楚你准备划多少G给ubuntu，因为确定之后就不要扩展/压缩卷了，因为容易将磁盘变成动态盘(有坑) （2） 采用压缩卷的方式，我划了300G（300*1024），因为后期Ubuntu要训练深度学习所以划多一些。 （3）划重点 动态磁盘转基本磁盘 因为Ubuntu不识别动态磁盘（跳坑了几天….） 可以看到上边“磁盘0”（也就是我即将放置Ubuntu的地方）为动态磁盘（绿色标识），再后续安装的过程导致识别不出该机械盘的分区情况。 解决办法： 分区助手（DiskGenius）有“动态盘转基本磁盘”的功能，但是操作之前，最好也备份数据。 进入BIOS设置U盘启动微星主板的Bios和一般的不一样，so 仅供参考 ‘设置U盘先启动“的含义是指调整Bios读取路径的优先级，先读取U盘的操作系统镜像，by the way 启动模式最好也设置成UEFI 保存后自动进入U盘选择界面 解决：efi usb device has been blocked by the current security policy问题描述：U盘装系统或者其他操作时，是因为BIOS安全策略，出现上述错误无法进入后续步骤。解决方法：按F2（Fn+F2）进入BIOS，在secure Boot 中security选择disable。解决！ 安装安装好后不要升级为18.04，不然你会后悔的！！！ 第一个可以不用勾 （重点）其他选项千万不要选”共存“ 之前是legacy的时候，因为U盘默认UEFI启动的，所以在ubuntu安装的时候出现了不兼容的情况，此时千万不要点”在UEFI模式下进行“，因为这样会回不去Window系统，最终可能导致两个系统都崩了。。 （重点）分区在Ubuntu磁盘划分情况，与window的分区情况大致相同。 如出现不一致或者不识别磁盘，找度娘。 看过了网络上大多数文章，才确定的分区，这一步确实很重要。 而且有很多人都说错了或者文章太老，不适合现在了。 我的分区如下（引用参考文章的，我觉得他说的比较清楚）： 1、swap交换空间（我选择的大小是16G） swap是linux的虚拟内存，具体分的空间大小因个人电脑的内存而定，2g电脑分4g，4g内存的电脑分4~6个g即可，8g电脑分8~10g即可。 大小：与电脑内存一致即可，最小不能低于电脑的一半。 新分区类型：主分区（或逻辑分区，推荐） 新分区的位置：空间起始位置 用于：交换空间 2、efi系统分区（我选择的大小是1G，你想再多点也想，该区适用于升级的） 大小：512MB，系统的引导文件都在这里。最好不要小于256MB 新分区类型：逻辑分区 新分区的位置：空间起始位置 用于：EFI系统分区 3、“/”（主要活动区） 大小：剩余的全部空间 新分区类型：逻辑分区 新分区的位置：空间起始位置 用于：Ext4日志文件系统 挂载点：/ 在安装启动引导器的设备一栏，选择efi系统分区所对应的设备。 这样分区的原因与好处： “/home”的作用是作为默认工作目录，下载的文件以及用户平时保存的工作文档都会存储在该目录下，因此应分配足够大的空间。“/usr”为所有软件的默认安装目录，因此也要分配足够大的空间。 “/usr”、“efi系统分区”、“/home”，“/usr”都是挂载在“/”下的目录。安装双系统时只建立“/”分区，则ubuntu系统的“efi系统分区”、“/home”、“/usr”都可以使用“/”的全部储存空间，这样不容易出现使用一段时间后“/home”，“/usr”存储空间不足的问题 efi系统分区包含系统引导文件，它的作用和boot引导分区一样，但是boot引导是默认grub引导的，而efi显然是UEFI引导的。不要按照那些老教程去选boot引导分区，也就是最后你的挂载点里没有“/boot”这一项，否则你就没办法UEFI启动两个系统了。 swap分区 efi分区 / 分区 引导器挂载在efi所在分区。 后续细节创建用户，用于Ubuntu登入 键盘布局（中国或美国随意） 检验点击“立即重启”后，拔出U盘 此时可能会出现直接进入window的情况，重启，在Bios出现的那瞬间按下F11（或许F12）出现Bios目录 删除（1）下载 EasyUEFI （2）在（window下）该软件下删除Ubuntu引导（打开EasyUEFI-》管理EFI启动项-》选择Ubuntu-》选择右边工具栏里的“删除”图标），之后重启电脑，此时应该默认window启动。 （3）在window磁盘管理器下删除给ubuntu的分区 重启后，再检查一下是否还存在Ubuntu引导文件 若还在，跟这篇文章来 ：彻底删除Ubuntu EFI分区及启动项 用记事本这个骚操作学到了嘿嘿~ 疑问（1）为什么本系统里面存在win7的引导，但是并没有看见呢？ 重装了一遍系统之后就没有了，猜测这台电脑可能是从win7升上来的。 安装Ubuntu18.04后的优化https://launchpad.net/~rikmills 设置 root密码12sudo passwd root #输入密码并确认 取消自动挂起与锁屏setting-&gt;电源 关闭“无操作时屏幕变暗,自动挂起” -&gt;息屏”从不” etting-&gt;隐私 关闭”锁屏” 文件夹创建桌面快捷方式12# ln -s [绝对路径] ~/桌面/快捷方式名称ln -s /data/long.com/ ~/桌面/long 修改grub配置win10直接从bios读时间，ubuntu是biso+8个小时。win10用的rtc ，ubuntu用的utc，我的ubuntu时间是准的。 是用同一个时间计算方式，以window为准。 12345678timedatectl set-local-rtc true sudo gedit /etc/default/grub #选择grub启动等待时间，默认是10秒GRUB_TIMEOUT=5 #(任何数值）#修改win10和ubuntu18.04双系统启动顺序并统一时间GRUB_DEFAULT=2#保存并退出sudo update-grub 重启电脑 更新源ubuntu下安装软件，一般都是通过命令sudo apt-get install package来安装想要的软件，这个命令一般都是从一个叫做/etc/apt/source.list文件里找下载地址，而默认的ubuntu也就是新安装的ubuntu，下载软件包都是从ubuntu官网下载,也就是这个source.list包含的是ubuntu官网配置的地址。但这个ubuntu官网不在国内，而在国外，也就是要下载个包，还要从国外网站下，所以这样下载速度自然就慢了。还好国内也有自己的镜像源，比如清华镜像源，阿里镜像源，这些镜像源包含了咱们想要下的软件包。这样下载软件就不必在国外网下，而直接从国内网站下载，这样下载速度就快了很多。 寻找国内镜像源清华的镜像源 然后下滑找到ubuntu，点击旁边的问号.然后选择相应的ubuntu版本，我用的是18.04版本的，并复制下面字段。 修改首先拷贝一份/etc/apt/sources.list文件 12sudo cp -v /etc/apt/sources.list /etc/apt/sources.list.backupsudo gedit /etc/apt/sources.list 然后全选并删除里面的内容，添加镜像源。 123456789101112131415161718192021222324# 清华源# 默认注释了源码镜像以提高 apt update 速度，如有需要可自行取消注释deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-security main restricted universe multiverse# 预发布软件源，不建议启用# deb https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse# deb-src https://mirrors.tuna.tsinghua.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse 123456789101112# 阿里源deb http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-security main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-updates main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-proposed main restricted universe multiversedeb-src http://mirrors.aliyun.com/ubuntu/ bionic-backports main restricted universe multiverse 123456789101112# 中科大源deb https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-updates main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-backports main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-security main restricted universe multiversedeb https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiversedeb-src https://mirrors.ustc.edu.cn/ubuntu/ bionic-proposed main restricted universe multiverse 12sudo apt update sudo apt upgrade 遇到的问题sudo apt upgrade E: 无法获得锁 /var/lib/dpkg/lock-frontend - open (11: 资源暂时不可用) E: 无法获取 dpkg 前端锁 (/var/lib/dpkg/lock-frontend)，是否有其他进程正占用它？ 强制解锁 1sudo rm /var/lib/dpkg/lock-frontend 指定用户sudo免密码操作终端 1sudo nano /etc/sudoers 在%sudo ALL=(ALL:ALL) ALL 的下一行添加你的用户名 ALL=(ALL:ALL) NOPASSWD:ALL然后按 ctrl+x 保存离开 截图软件-flameshotUbuntu 18.04默认带的版本是0.51 无法标注文字,要0.6+才可以添加文字标注 1234sudo add-apt-repository ppa:rikmills/bionicsudo apt-get updatesudo apt install flameshotflameshot -v #显示为0.6.0 设置&gt;设备&gt;键盘，设置一个自定义快捷键（拉到最下面）命令填写：flameshot gui 截完图后保存Ctrl+S，复制到剪贴板 Ctrl+C deepin-wine环境https://mirrors.tuna.tsinghua.edu.cn/deepin/pool/non-free/d/ http://mirrors.aliyun.com/deepin/pool/non-free/d/ https://github.com/zq1997/deepin-wine 1234wget -O- https://deepin-wine.i-m.dev/setup.sh | sh#超简单安装微信QQ等Wine软件#例如安装微信，替换对应的包名就行sudo apt-get install deepin.com.wechat 安装搜狗输入法1234567sudo apt install fcitx-bin #安装fcitx-binsudo apt update --fix-missing #修复fcitx-bin安装失败的情况sudo apt install fcitx-bin #重新安装fcitx-binsudo apt install fcitx-table #安装fcitx-table 然后去搜狗官网下载好给予linux的搜狗输入法deb安装包 123456cd 下载 #若在根目录sudo dpkg -i sogoupinyin*.deb #安装搜狗拼音sudo apt install -f #修复搜狗拼音安装的错误sudo dpkg -i sogoupinyin*.deb #重新安装搜狗拼音 输入法安装成功后要重新进入系统生效，重新登录或重启。 问题在ubuntu系统下，安装好sogou拼音之后，用了一段时间之后，输入拼音之后，老是出现繁体字，很烦。 解决办法 按住shift 不放，同时，按下ctrl 和F键，会出现“繁简转换”-已启用简体中文 即可 安装Typora（不推荐）方法一：直接在应用商店里下载并安装 创建图标 Ubuntu中的DashBoard中软件启动器均存贮在/usr/share/applications这个目录， 12cd /usr/share/applicationssudo gedit typora.desktop 打开需要编辑的文本内容为： 1234567891011121314151617[Desktop Entry]Encoding=UTF-8Name=Typora#Exec=/home/innovation/typora/Typora#Icon=/home/innovation/typora/resources/app/asserts/icon/icon_256x256@2x.pngExec=/snap/typora-alanzanattadev/2/usr/share/typora/TyporaIcon=/snap/typora-alanzanattadev/2/usr/share/typora/resources/app/asserts/icon/icon_256x256@2x.pngCategories=Application;Development;Java;IDEType=Application#Terminal=1 最后右上角保存退出就OK （推荐）方法二如下： 12345678910111213sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAEwget -qO - https://typora.io/linux/public-key.asc | sudo apt-key add -#add Typora's repositorysudo add-apt-repository 'deb https://typora.io/linux ./'sudo apt-get update#install typorasudo apt-get install typora 通过搜索行搜索typora，然后点击 typora 图标 问题Typora 在 Ubuntu18.04 上面不显示 Markdown 加粗语法 在 Typora’s github.css 里面，将 body 修改为如下内容 12345body &#123; font-family: "Open Sans Regular","Open Sans","Clear Sans","Helvetica Neue",Helvetica,Arial,sans-serif; color: rgb(51, 51, 51); line-height: 1.6;&#125; github.css 是这个主题的css文件 打开方式：文件-&gt;偏好设置-&gt;外观-&gt;打开主题文件夹 参考文章Typora 在 Ubuntu18.04 上面不显示 Markdown 加粗语法 ubuntu18.04配置镜像源 解决：efi usb device has been blocked by the current security policy UEFI 模式下win10安装ubuntu16.04双系统教程 UEFI+GPT双硬盘安装Win10+Ubuntu16.04双系统 windows系统安装ubuntu双系统及分区方案 win10下Ubuntu 双系统安装（解决关机卡死问题和WiFi问题） win10安装ubuntu18.04 LTS双系统 Win10+Ubuntu双系统删除Ubuntu方法解释 Win10+Ubuntu双系统删除Ubuntu方法 – UEFI Win10环境下安装Ubuntu 18.04 LTS超详教程]]></content>
      <categories>
        <category>window</category>
      </categories>
      <tags>
        <tag>tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重装win10并改变分区类型]]></title>
    <url>%2Farticle%2F62a72a03%2F</url>
    <content type="text"><![CDATA[重装win10并改变类型为（uefi+gpt） U盘启动盘制作网上一大堆，搜下 我已经做好了。 重启插上，调节Bios为U盘优先（具体可以看参考文章） 安装ing 到系统版本选择上图界面后，按下Shift+F10调出命令提示符，如下图。 输入diskpart命令后按回车键，进入DISKPART工具，如下图所示 输入list disk命令后按回车键，查看电脑当前可用硬盘，编号为0、1、2……如果你只有一块硬盘，则只有0号磁盘；有两 块硬盘，则还会显示1号磁盘，以此类推，如下图所示 输入select disk x（x为上述硬盘编号），选择你要进行分区操作的硬盘，如果只有一块硬盘，输入select disk 0后按回车键 即可，如下图所示。 执行clean命令清除该硬盘上的所有分区（新硬盘无需此步骤），此时会清除所有硬盘数据，如下图所示。 执行convert gpt命令将该硬盘转换成GPT分区表，如下图所示。 创建EFI分区，执行create partition efi size=512（分区大小为200MB），如下图所示。 创建MSR分区，执行create partition msr size=512（微软系统保留分区），如下图所示。 创建主分区，执行create partition primary size=xxx（具体大小根据你的要求而定，作为系统分区来说，如果有足够空间，可以留出大于100GB即102400MB的空间，命令为create partition primary size=102400，方便系统有足够周转空间），如图下所示。 这是专业版Win的序列号，可以在网上找序列号，或者后面在利用网上的软件生成。 选择自定义安装，因为我们还要将系统准确安装在SSD上 结果~~ 完成 问题找不到任何设备驱动程序请确保安装媒体包 更换U盘接口 参考文章U盘安装WIN10时显示 windows无法安装到这个磁盘 选中的磁盘采用GPT分区形式 （Legacy+MBR）转（UEFI+GPT） win10系统安装]]></content>
      <categories>
        <category>window</category>
      </categories>
      <tags>
        <tag>tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[制作多系统U盘启动盘]]></title>
    <url>%2Farticle%2F48db235e%2F</url>
    <content type="text"><![CDATA[预先准备（1）16GB U盘 后来用8GB（实际上只有7.43G）的U盘同时安装window10与Ubuntu18.04刚好够，还剩1GB多。 （2）YUMI （本机选择UEFI版本0.0.1.9） 版本选择根据window的引导模式决定（命令行msinfo32查看） https://www.pendrivelinux.com/yumi-multiboot-usb-creator/ （3）Win 10 IOS(镜像) 下载 WIN10 系统镜像教程 WIN10 系统镜像网站 下载后并校验：下载完成后务必进行SHA1校验（推荐使用iHasher），与上面网站所给值核对一致后再使用。 （4）Ubuntu16.04 LST(镜像) ubuntu-16.04.6-desktop-amd64.iso Ubuntu官网 http://releases.ubuntu.com/xenial/ 制作过程除了制作多合一的系统引导盘外，YUMI 还能让你创建保留的存储空间，不至于在做成启动盘后白白浪费了 U 盘剩余的容量。 大致流程： Step1：选择 U 盘或移动硬盘的盘符 Step2：选择你要制作的 Windows 系统或者 Linux 发行版 Step3：选择对应的 ISO 镜像文件 Step4：设置要保留的存储空间 (可选) 最后，按下 「Create」即开始制作 值得注意的是，YUMI 每次只能加入一个 Windows 或 Linux 系统的引导。如果你要制作多个系统，那么就要重复执行多次该软件和上述的步骤来增加操作系统。 具体操作： （1）备份好你的U盘数据后，再格式化U盘，注意文件系统格式为FAT32 安装Window（2）启动YUMI 第一步，选择你要写入的U盘盘符，下图中是F，如果是第一次可以勾选格式，如果U盘有数据，要先备份保存（因为我们之前自己格式化了，所以这里就不需要勾选“Format F:Drive”）；第二步选择镜像的发行版，这里我想安装的是Window；第三步是选择镜像文件，然后点击“Create”按钮，等待完毕就可以了。 等待写入ing… 安装Uubuntu第4步的预设空间是创建保留的存储空间（具体看官网说明） 所以我计算了下空间（window 5G，ubuntu5G，还剩余4G刚好够），所以拉到最大~~ 接下来操作都一样~ 确认后，不需要其他操作了，直到显示下图为止。单击finish，U盘就做好了。 让我们瞅瞅引导界面 链接两篇博文重装win10并改变分区类型 win10【uefi+gpt】安装or删除双系统Ubuntu16.04 参考文章YUMI-简单实用的多系统引导U盘制作工具 如何利用YUMI制作多引导多系统USB启动盘 YUMI-一款强大的多系统引导U盘刻录软件 关于Linux的U盘安装与启动 YUMI - 简单制作 Windows 与 Linux 多系统启动盘 (免费多合一U盘制作工具)]]></content>
      <categories>
        <category>window</category>
      </categories>
      <tags>
        <tag>tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[刷题记录]]></title>
    <url>%2Farticle%2Ffe644ae0%2F</url>
    <content type="text"><![CDATA[CFF真题【 201909-2 】小明种苹果（续） 思路１．发生苹果掉落和疏果是两种不同的操作 发生苹果掉落（5 3） 疏果（5 -3） ２．一棵树可能出现多次苹果掉落的情况 比如：3 5 2 1(对于一棵树来说 有３个操作，原来有５个苹果，第一次掉落后还剩２个，第二次掉落后还剩１个) ３．当发生苹果掉落的苹果树的棵树大于等于３时才可能形成连续的三个苹果树 注意 a可能是10^6 所以采用long long 源代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273#include &lt;iostream&gt;#include &lt;string.h&gt;using namespace std;typedef long long ll;int main()&#123; //定义长整型 ll n; cin&gt;&gt;n; ll *appleDrop = new ll [n]; ll lostNum = 0,lostCon = 0,sum = 0; //初始化掉落苹果数组 memset(appleDrop,0,n*sizeof(appleDrop)); //输出数组初始化情况 // for(int i=0;i&lt;n;i++) cout&lt;&lt;appleDrop[i]; // cout&lt;&lt;endl; for(ll i=0;i&lt;n;i++)&#123; // 第 i 棵苹果的操作 ll number; cin &gt;&gt; number; //这颗苹果树有多少苹果 ll apple; cin &gt;&gt; apple; number--; while(number--)&#123; //疏果或重新统计 ll lost; cin &gt;&gt; lost; if(lost &lt;= 0)&#123; //疏果 apple += lost; &#125;else&#123; //重新统计 if(apple &gt; lost)&#123; apple = lost; appleDrop[i] = 1; &#125; &#125; &#125;//end of while //总数 sum += apple; //掉落数 if(appleDrop[i]) lostNum++; &#125;//end of for// 掉落苹果数组情况// for(int i=0;i&lt;n;i++) cout&lt;&lt;appleDrop[i];// cout&lt;&lt;endl; //连续三颗苹果树掉落 for(int i=1;i&lt;n-1;i++)&#123; if(appleDrop[i-1]&amp;&amp;appleDrop[i]&amp;&amp;appleDrop[i+1])&#123; lostCon++; &#125; &#125; //首尾特殊情况 if(appleDrop[n-2]&amp;&amp;appleDrop[n-1]&amp;&amp;appleDrop[0]) lostCon++; if(appleDrop[n-1]&amp;&amp;appleDrop[1]&amp;&amp;appleDrop[0]) lostCon++; cout&lt;&lt;sum&lt;&lt;" "&lt;&lt;lostNum&lt;&lt;" "&lt;&lt;lostCon; delete []appleDrop; return 0;&#125; 测试用例 44 74 -7 -12 -55 73 -8 -6 59 -45 76 -5 -10 60 -25 80 -6 -15 59 0 54 10 0 9 04 10 -2 7 02 10 04 10 -3 5 04 10 -1 8 0 33 10 2 12 10 -53 10 3 1 【201909-4】 推荐系统 思路C++常用数据结构–STL ，照搬大佬的 使用map&lt;int,set&lt;pair&lt;int,int&gt; &gt; &gt; F保存所有数据，格式为map&lt;Score,set&lt;pair&lt;Type,Commodity&gt; &gt; &gt;这样在容器内部首先会按Score排序，同一Score的先按Type排序，再按Commodity排序。使用unordered_map&lt;int,int&gt;[50]保存编号到成绩的映射，格式为unordered_map&lt;Commodity,Score&gt; G[Type]这样指定Type和Commodity可以在F中迅速定位，从F中增删商品的时间度为O(log(Score的种数) * log(该分数的物品数))。 基础知识map&lt;class T1,class T2&gt;特性：存储T1-&gt;T2映射的键值对,map先按照T1升序排序，再按T2升序排序，其中T1,T2可以是任意类（如：int、string、char或自定义类）,T1值唯一，其插入删除的时间复杂度为O(log2) unordered_map&lt;class T1,class T2&gt;特性：仅存储T1-&gt;T2映射的键值对，T1值唯一，其插入和删除的时间复杂度为O(1) 源代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129#include &lt;iostream&gt;#include &lt;string.h&gt;#include &lt;set&gt;#include &lt;map&gt;#include &lt;vector&gt;#include &lt;tr1/unordered_map&gt;using namespace std::tr1;using namespace std;typedef long long ll;int main()&#123; //总商品种类 n ll n; //商品类别 m int m; //操作数 ll op; //查询次数 int opAsk; //存储编号到分数的映射 ，map&lt;commodity,score&gt; g[Type] unordered_map&lt;ll,ll&gt; g[51]; unordered_map&lt;ll,ll&gt;::iterator itG; //存放商品信息 ，map&lt;score,set&lt;pair&lt;Type,Commodity&gt;&gt;&gt; map&lt;ll,set&lt;pair&lt;int,ll&gt; &gt; &gt; pro; map&lt; ll,set&lt; pair&lt;int,ll&gt; &gt; &gt;::iterator itP; //分数，编号 ll score,commodity; //种类 int type; cin&gt;&gt;m&gt;&gt;n; for(int i = 0; i &lt; n; i++)&#123; cin&gt;&gt;commodity&gt;&gt;score; for(int j = 0; j &lt; m; j++)&#123; pro[score].insert(pair&lt;int,ll&gt;(j,commodity)); g[j][commodity] = score; &#125; &#125; cin&gt;&gt;op; opAsk = 0; while(op--)&#123; int opType; cin&gt;&gt;opType; switch(opType)&#123; //添加 case 1:&#123; cin&gt;&gt;type&gt;&gt;commodity&gt;&gt;score; g[type][commodity] = score; pro[score].insert(pair&lt;int,ll&gt;(type,commodity)); break; &#125; //删除 case 2:&#123; cin&gt;&gt;type&gt;&gt;commodity; //若商品存在 则删除 if((itG = g[type].find(commodity)) != g[type].end())&#123; //找到该分数的容器 itP = pro.find(itG-&gt;second); //删除该容器内指定商品 itP-&gt;second.erase(pair&lt;int,ll&gt;(type,commodity)); //若该分数下无商品，则删除该分数的容器 if(itP-&gt;second.empty()) pro.erase(itP); //删除对应映射 g[type].erase(itG); &#125; break; &#125; //查询 case 3:&#123; opAsk++; //总数 k int k; cin&gt;&gt;k; //每类最大选出个数 ll Cout[m]; for(int i = 0; i &lt; m; i++)&#123; cin&gt;&gt;Cout[i]; &#125; //查询K类 ，不能使用set 因为set将自动排序，该题有bug //set&lt;ll&gt; queryP[51]; //每类选择的编号 vector&lt;ll&gt; queryP[51]; // 按分数由大到小选择前k个物品 for(auto it = pro.rbegin(); it != pro.rend() &amp;&amp; k&gt;0; ++it)&#123; set&lt;pair&lt;int,ll&gt; &gt;&amp;s = it -&gt; second; for(auto itS = s.begin(); itS != s.end()&amp;&amp;k&gt;0; ++itS)&#123; const pair&lt;int,ll&gt;&amp;p = *itS; if(Cout[p.first] &gt; 0)&#123; //queryP[p.first].insert(p.second),--queryCout[p.first],--k; queryP[p.first].push_back(p.second),--Cout[p.first],--k; &#125; &#125; &#125; //输出选择的所有物品 for(int j=0;j&lt;m;++j)&#123;//对所有类输出选择的物品 if(queryP[j].empty())//该类没有选中任何物品 cout&lt;&lt;-1&lt;&lt;endl; else&#123; k=0;//仅仅用于判断是否输出空格 for(auto it=queryP[j].begin();it!=queryP[j].end();++it,++k) cout&lt;&lt;(k==0?"":" ")&lt;&lt;*it; cout&lt;&lt;endl; &#125; &#125; break; &#125; &#125;// end-switch &#125;// end-while return 0;&#125; 测试用例123456789101112132 31 32 23 183 100 1 11 0 4 31 0 5 13 10 2 23 10 1 12 0 13 2 1 13 1 1 1 【 201903-1 】 小中大 注意点：涉及到四舍五入，以及小数点位数 源代码12345678910111213141516171819202122232425262728293031323334353637383940414243#include&lt;iostream&gt;#include&lt;iomanip&gt;using namespace std;typedef long long ll;int main()&#123; ll max,min; ll mid; int n; cin&gt;&gt;n; ll *arr = new ll[n]; for(int i = 0; i &lt; n; i++)&#123; cin&gt;&gt;arr[i]; &#125; max = arr[0]; min = arr[n-1]; if(min &gt; max)&#123; ll t; t = max; max= min; min = t; &#125; if(n%2 == 0)&#123; mid = arr[n/2-1] + arr[n/2]; if(mid%2 == 0)&#123; cout&lt;&lt;max&lt;&lt;" " &lt;&lt;mid/2&lt;&lt;" "&lt;&lt;min; &#125;else&#123; cout&lt;&lt;max&lt;&lt;" " &lt;&lt;fixed&lt;&lt; setprecision(1)&lt;&lt;mid*0.5&lt;&lt;" "&lt;&lt;min; &#125; &#125;else&#123; mid = arr[n/2]; cout&lt;&lt;max&lt;&lt;" " &lt;&lt;mid&lt;&lt;" "&lt;&lt;min; &#125; return 0;&#125; 测试用例123456784-2 -1 3 44-1 4 5 63-1 2 4 【 201903-2 】 二十四点 思路方法一： 中缀表达式先构建表达树，后序遍历得到后缀表达式，然后计算结果 方法二：利用一个栈和一个字符串 得到后缀表达式，然后计算结果 参考 https://blog.csdn.net/fireflylane/article/details/83017889 将中缀表达式转换为后缀表达式step1：初始化一个栈和一个后缀表达式字符串step2：从左到右依次对中缀表达式中的每个字符进行以下处理，直到表达式结束 如果字符是‘)’，将其入栈 如果字符是数字，添加到s2中 如果字符是运算符，先将栈顶优先级不低于该运算符的运算符出栈，添加到s2中，再将该运算符入栈。当‘）’在栈中是，优先级最低如果字符是‘（’，将栈顶元素出栈，添加到s2中，直到出栈的是‘）’ step3：如果表达式结束，但栈中还有元素，将所有元素出栈，添加s2中 step4：将栈s2中元素依次出栈，即得到前缀表达式 后缀表达式的计算后缀表达式没有括号，运算符的顺序即为实际运算顺序，在求值过程中，当遇到运算符时，只要取得前两个操作数就可以立即进行计算。当操作数出现时，不能立即求值，需要先保存等待运算符。对于等待中的操作数而言，后出现的先运算，所以需要一个栈辅助操作。后缀表达式的运算过程如下：step1：设置一个栈step2：从左到右对后缀表达式中的字符进行以下处理： 如果字符是数字，现将其转化为数字，然后入栈 如果字符是运算符，出栈两个值进行计算。计算结果入栈 重复以上步骤，直到后缀表达式扫描结束，栈中最后一个元素就是表达式的结果。 方法三（比较巧妙，只适合本题没有括号的情况）：提前考虑优先级 ，来源 https://blog.csdn.net/Dqmail/article/details/89318208 首先遍历表达式数组，当遇到x / 时 计算结果，例如 +AxB 算为 +0+（AxB）即： 原来的符号变成上一个符号,A位置变成0，B位置变成 A x B ，如果操作符在a[1],计算完后也需要改变为‘+’ ,最后再遍历一回数组依次相加减即可。 注意一点：首先是拿字符数组接收表达式，但如果直接拿字符相加减容易出问题，所以额外用整形数组保存数字。 源代码方法一： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;string str;const int maxn = 1000;int lch[maxn], rch[maxn]; char op[maxn];int nc = 0; //结点数int build_tree(char* s, int x, int y)&#123; int i, c1 = -1, c2 = -1, p = 0; int u; if(y-x == 1) //仅一个字符，建立单独结点 &#123; u = ++nc; lch[u] = rch[u] = 0; op[u] = s[x]; return u; &#125; for(i = x; i &lt; y; i++) &#123; switch(s[i]) &#123; case '(': p++; break; case ')': p--; break; case '+': case '-': if(!p) c1 = i; break; case '*': case '/': if(!p) c2 = i; break; &#125; &#125; if(c1 &lt; 0) c1 = c2; //找不到括号外的加减号，就用乘除号 if(c1 &lt; 0) return build_tree(s, x+1, y-1); //整个表达式被一对括号括起来 u = ++nc; lch[u] = build_tree(s, x, c1); rch[u] = build_tree(s, c1+1, y); op[u] = s[c1]; return u;&#125;int main()&#123; cin&gt;&gt;str; build_tree((char*)str.data(),0,str.length()); for(int i = 0; i &lt; nc; i++ ) cout&lt;&lt;lch[i]; return 0;&#125; 方法二： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107#include&lt;iostream&gt;#include &lt;string&gt;#include &lt;stack&gt;using namespace std;//定义运算符的优先级int symPriority(char a)&#123; if(a == 'x' || a == '/') return 2; if(a == '+' || a == '-') return 1; return 0;&#125;//比较运算符的优先级bool comparePriority(char a, char b)&#123; int aValue = symPriority(a), bValue = symPriority(b); if(aValue &gt;= bValue) return true; return false;&#125;int main()&#123; int n ; cin&gt;&gt;n; while(n--)&#123; //需要计算的中缀表达式 string expression; cin&gt;&gt;expression; //运算符栈，用于保存运算符 stack&lt;char&gt; opStack; //后缀表达式 string symbol; int el = expression.length(); for(int i = 0; i &lt; el; i++)&#123; //如果 （ 压入栈 if(expression[i] == '(')&#123; opStack.push(expression[i]); &#125; //数字 放入 后缀表达式 else if(expression[i] &gt;= '0' &amp;&amp; expression[i] &lt;= '9')&#123; symbol.push_back(expression[i]); &#125; // ) 弹出栈顶元素，直至出栈是的 （ else if(expression[i] == ')')&#123; while(!opStack.empty() &amp;&amp;opStack.top() != '(')&#123; symbol.push_back(opStack.top()); opStack.pop(); &#125; if(opStack.top() == '(') opStack.pop(); &#125; // 若是其他运算符 else&#123; //先将栈顶优先级不低于该运算符的运算符出栈， while(!opStack.empty() &amp;&amp; comparePriority(opStack.top(),expression[i]))&#123; symbol.push_back(opStack.top()); opStack.pop(); &#125; //再压入栈 opStack.push(expression[i]); &#125; &#125; // end of for while(!opStack.empty())&#123; symbol.push_back(opStack.top()); opStack.pop(); &#125; //cout&lt;&lt;symbol&lt;&lt;endl; /* 表达式的计算 */ int sl = symbol.length(); stack&lt;int&gt; op; for(int i = 0; i &lt; sl; i++)&#123; if( symbol[i]&gt;= '0' &amp;&amp; symbol[i] &lt;= '9')&#123; op.push( symbol[i]-'0'); &#125;else&#123; int a = op.top(); op.pop(); int b = op.top(); op.pop(); switch(symbol[i])&#123; case '+': b+=a;op.push(b);break; case '-': b-=a;op.push(b);break; case 'x': b*=a;op.push(b);break; case '/': b/=a;op.push(b);break; &#125; &#125; &#125; // cout&lt;&lt;op.top()&lt;&lt;endl; //判断是否等于24点 if(op.top() == 24)&#123; cout&lt;&lt;"Yes"&lt;&lt;endl; &#125;else&#123; cout&lt;&lt;"No"&lt;&lt;endl; &#125; &#125; return 0;&#125; 方法三： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include&lt;iostream&gt;using namespace std;int main()&#123; int n ; cin&gt;&gt;n; while(n--)&#123; //输入表达式 char a[8]; int b[7]; cin&gt;&gt;a; for(int i = 0; i &lt; 8; i+=2) b[i] = a[i]-'0'; for(int i = 1; i &lt; 7; i+=2)&#123; //遍历一遍 x / if(a[i] == 'x')&#123; b[i+1] *= b[i-1]; b[i-1] = 0; if(i == 3 || i == 5 ) a[i] = a[i-2]; if(i == 1) a[i] = '+'; &#125; if(a[i] == '/')&#123; b[i+1] = b[i-1] / b[i+1]; b[i-1] = 0; if(i == 3 || i == 5 ) a[i] = a[i-2]; if(i == 1) a[i] = '+'; &#125; &#125; int sum = b[0]; //此时表达式只有+-，依次计算 for(int i = 1; i &lt; 7; i+=2)&#123; if(a[i] == '+')&#123; sum += b[i+1] ; &#125; if(a[i] == '-')&#123; sum -= b[i+1] ; &#125; &#125; //判断是否等于24点 if(sum == 24)&#123; cout&lt;&lt;"Yes"&lt;&lt;endl; &#125;else&#123; cout&lt;&lt;"No"&lt;&lt;endl; &#125; &#125;// end of n return 0;&#125; 测试用例1234567891011109+3+4x35+4x5x57-9-9+85x6/5x43+5+7+91x1+9-91x9-5/98/5+6x96x7-3x66x4+4/5 【 201812-2 】 小明放学题目背景 汉东省政法大学附属中学所在的光明区最近实施了名为“智慧光明”的智慧城市项目。具体到交通领域，通过“智慧光明”终端，可以看到光明区所有红绿灯此时此刻的状态。小明的学校也安装了“智慧光明”终端，小明想利用这个终端给出的信息，估算自己放学回到家的时间。 问题描述 一次放学的时候，小明已经规划好了自己回家的路线，并且能够预测经过各个路段的时间。同时，小明通过学校里安装的“智慧光明”终端，看到了出发时刻路上经过的所有红绿灯的指示状态。请帮忙计算小明此次回家所需要的时间。 输入格式 输入的第一行包含空格分隔的三个正整数 r、y、g，表示红绿灯的设置。这三个数均不超过 106。 输入的第二行包含一个正整数 n，表示小明总共经过的道路段数和路过的红绿灯数目。 接下来的 n 行，每行包含空格分隔的两个整数 k、t。k=0 表示经过了一段道路，将会耗时 t 秒，此处 t 不超过 106；k=1、2、3 时，分别表示出发时刻，此处的红绿灯状态是红灯、黄灯、绿灯，且倒计时显示牌上显示的数字是 t，此处 t 分别不会超过 r、y、g。 输出格式 输出一个数字，表示此次小明放学回家所用的时间。 样例输入 30 3 30 8 0 10 1 5 0 11 2 2 0 6 0 3 3 10 0 3 样例输出 46 样例说明 小明先经过第一段路，用时 10 秒。第一盏红绿灯出发时是红灯，还剩 5 秒；小明到达路口时，这个红绿灯已经变为绿灯，不用等待直接通过。接下来经过第二段路，用时 11 秒。第二盏红绿灯出发时是黄灯，还剩两秒；小明到达路口时，这个红绿灯已经变为红灯，还剩 11 秒。接下来经过第三、第四段路，用时 9 秒。第三盏红绿灯出发时是绿灯，还剩 10 秒；小明到达路口时，这个红绿灯已经变为红灯，还剩两秒。接下来经过最后一段路，用时 3 秒。共计 10+11+11+9+2+3 = 46 秒。 评测用例规模与约定 有些测试点具有特殊的性质： 前 2 个测试点中不存在任何信号灯。 测试点的输入数据规模： 前 6 个测试点保证 n ≤ 103。 * 所有测试点保证 n ≤ 105。 数据小于10^6次方，可以使用int，但是累加的话会出现数据溢出，所以还是使用long long 比较好 源代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104#include &lt;iostream&gt;using namespace std;typedef long long ll;int main()&#123; ll r,y,g; int n; ll sum = 0; cin&gt;&gt;r&gt;&gt;y&gt;&gt;g; cin&gt;&gt;n; while(n--)&#123; ll k,t,k2,t2; cin&gt;&gt;k&gt;&gt;t; switch(k)&#123; case 0: k2 = k; t2 = t; break; //红灯 case 1: if(sum &lt; t)&#123; k2 = 1; t2 = t - sum; &#125;else if(sum&gt;t &amp;&amp; sum&lt;t+g)&#123; k2 = 3; &#125;else&#123; int x = (sum-t-g) % (r+y+g); if(x&lt;y)&#123; k2 = 2; t2 = y-x; &#125;else if(x&gt;y &amp;&amp; x&lt; (y+r))&#123; k2 = 1; t2 = y+r-x; &#125;else&#123; k2 = 3; &#125; &#125; break; //黄灯 case 2: if(sum &lt; (t+r))&#123; k2 = 1; t2 = (t+r) - sum; &#125;else if(sum &gt; (t+r) &amp;&amp; sum &lt; (t+r+g))&#123; k2 = 3; &#125;else&#123; int x = (sum-t-r-g) % (r+y+g); if(x&lt;y)&#123; k2 = 2; t2 = y-x; &#125;else if(x&gt;y &amp;&amp; x&lt; (y+r))&#123; k2 = 1; t2 = y+r-x; &#125;else&#123; k2 = 3; &#125; &#125; break; //绿灯 case 3: if(sum &lt; t || sum == t)&#123; k2 = 3; &#125;else&#123; int x = (sum-t) % (r+y+g); if(x&lt;y)&#123; k2 = 2; t2 = y-x; &#125;else if(x&gt;y &amp;&amp; x&lt; (y+r))&#123; k2 = 1; t2 = y+r-x; &#125;else&#123; k2 = 3; &#125; &#125; break; &#125; switch(k2)&#123; case 0: sum += t2; break; case 1: sum += t2; break; case 2: sum += (t2+r); break; case 3: break; &#125; &#125; cout&lt;&lt;sum; return 0;&#125; 【201803-2】碰撞的小球问题描述 数轴上有一条长度为L（L为偶数)的线段，左端点在原点，右端点在坐标L处。有n个不计体积的小球在线段上，开始时所有的小球都处在偶数坐标上，速度方向向右，速度大小为1单位长度每秒。 当小球到达线段的端点（左端点或右端点）的时候，会立即向相反的方向移动，速度大小仍然为原来大小。 当两个小球撞到一起的时候，两个小球会分别向与自己原来移动的方向相反的方向，以原来的速度大小继续移动。 现在，告诉你线段的长度L，小球数量n，以及n个小球的初始位置，请你计算t秒之后，各个小球的位置。 提示 因为所有小球的初始位置都为偶数，而且线段的长度为偶数，可以证明，不会有三个小球同时相撞，小球到达线段端点以及小球之间的碰撞时刻均为整数。 同时也可以证明两个小球发生碰撞的位置一定是整数（但不一定是偶数）。 输入格式 输入的第一行包含三个整数n, L, t，用空格分隔，分别表示小球的个数、线段长度和你需要计算t秒之后小球的位置。 第二行包含n个整数a1, a2, …, an，用空格分隔，表示初始时刻n个小球的位置。 输出格式 输出一行包含n个整数，用空格分隔，第i个整数代表初始时刻位于ai的小球，在t秒之后的位置。 样例输入 3 10 5 4 6 8 样例输出 7 9 9 注意点碰撞之后转向，还要动。 两边都是墙壁。 源代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &lt;iostream&gt;using namespace std;typedef struct node&#123; int number; bool ori = 1; bool noCrush = 1;&#125;node;int main()&#123; int n,l,t; cin&gt;&gt;n&gt;&gt;l&gt;&gt;t; node *a = new node[n]; for(int i = 0; i &lt; n; i++)&#123; cin&gt;&gt;a[i].number; &#125; while(t--)&#123; for(int i = 0; i &lt; n; i++)&#123; //撞到墙壁 if(a[i].number == l)&#123; a[i].ori = 0; &#125;else if(a[i].number == 0)&#123; a[i].ori = 1; &#125;else&#123; //撞到其他小球 if(a[i].noCrush)&#123; for(int j = 0; j &lt; n; j++)&#123; if(i != j &amp;&amp; a[i].number == a[j].number)&#123; a[i].noCrush = 0; a[j].noCrush = 0; break; &#125; &#125;// end of j &#125; &#125; if(!a[i].noCrush)&#123; a[i].ori = !a[i].ori; a[i].noCrush = 1; &#125; if(a[i].ori) a[i].number++; else a[i].number--; &#125;// end of i &#125; for(int i = 0; i &lt; n; i++) cout&lt;&lt;a[i].number&lt;&lt;" "; delete []a; return 0;&#125; 【201803-3】 URL映射问题描述 URL 映射是诸如 Django、Ruby on Rails 等网页框架 (web frameworks) 的一个重要组件。对于从浏览器发来的 HTTP 请求，URL 映射模块会解析请求中的 URL 地址，并将其分派给相应的处理代码。现在，请你来实现一个简单的 URL 映射功能。 本题中 URL 映射功能的配置由若干条 URL 映射规则组成。当一个请求到达时，URL 映射功能会将请求中的 URL 地址按照配置的先后顺序逐一与这些规则进行匹配。当遇到第一条完全匹配的规则时，匹配成功，得到匹配的规则以及匹配的参数。若不能匹配任何一条规则，则匹配失败。 本题输入的 URL 地址是以斜杠 / 作为分隔符的路径，保证以斜杠开头。其他合法字符还包括大小写英文字母、阿拉伯数字、减号 -、下划线 _ 和小数点 .。例如，/person/123/ 是一个合法的 URL 地址，而 /person/123? 则不合法（存在不合法的字符问号 ?）。另外，英文字母区分大小写，因此 /case/ 和 /CAse/ 是不同的 URL 地址。 对于 URL 映射规则，同样是以斜杠开始。除了可以是正常的 URL 地址外，还可以包含参数，有以下 3 种： 字符串 ：用于匹配一段字符串，注意字符串里不能包含斜杠。例如，abcde0123。 整数 ：用于匹配一个不带符号的整数，全部由阿拉伯数字组成。例如，01234。 路径 ：用于匹配一段字符串，字符串可以包含斜杠。例如，abcd/0123/。 以上 3 种参数都必须匹配非空的字符串。简便起见，题目规定规则中 和 前面一定是斜杠，后面要么是斜杠，要么是规则的结束（也就是该参数是规则的最后一部分）。而 的前面一定是斜杠，后面一定是规则的结束。无论是 URL 地址还是规则，都不会出现连续的斜杠。 输入格式 输入第一行是两个正整数 n 和 m，分别表示 URL 映射的规则条数和待处理的 URL 地址个数，中间用一个空格字符分隔。 第 2 行至第 n+1 行按匹配的先后顺序描述 URL 映射规则的配置信息。第 i+1 行包含两个字符串 pi 和 ri，其中 pi 表示 URL 匹配的规则，ri 表示这条 URL 匹配的名字。两个字符串都非空，且不包含空格字符，两者中间用一个空格字符分隔。 第 n+2 行至第 n+m+1 行描述待处理的 URL 地址。第 n+1+i 行包含一个字符串 qi，表示待处理的 URL 地址，字符串中不包含空格字符。 输出格式 输入共 m 行，第 i 行表示 qi 的匹配结果。如果匹配成功，设匹配了规则 pj ，则输出对应的 rj。同时，如果规则中有参数，则在同一行内依次输出匹配后的参数。注意整数参数输出时要把前导零去掉。相邻两项之间用一个空格字符分隔。如果匹配失败，则输出 404。 样例输入 5 4 /articles/2003/ special_case_2003 /articles// year_archive /articles/// month_archive /articles//// article_detail /static/ static_serve /articles/2004/ /articles/1985/09/aloha/ /articles/hello/ /static/js/jquery.js 样例输出 year_archive 2004 article_detail 1985 9 aloha 404 static_serve js/jquery.js 样例说明 对于第 1 个地址 /articles/2004/，无法匹配第 1 条规则，可以匹配第 2 条规则，参数为 2004。 对于第 2 个地址 /articles/1985/09/aloha/，只能匹配第 4 条规则，参数依次为 1985、9（已经去掉前导零）和 aloha。 对于第 3 个地址 /articles/hello/，无法匹配任何一条规则。 对于第 4 个地址 /static/js/jquery.js，可以匹配最后一条规则，参数为 js/jquery.js。 数据规模和约定 1 ≤ n ≤ 100，1 ≤ m ≤ 100。 所有输入行的长度不超过 100 个字符（不包含换行符）。 保证输入的规则都是合法的。 思路一组string的vector存放数据，用“/”分割每条语句。 依次拿规则来匹配url,别弄反了不然思路会有点乱，尤其是&lt; path &gt;规则。 注意事项有：规则的字符串分组必须与url的一样，即在&lt; path &gt;规则中 url后所有字符串为一个整体。还有前导零的去除 源代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;string&gt;#include &lt;vector&gt;#include &lt;ctype.h&gt;using namespace std;typedef struct rule&#123; vector&lt;string&gt; p; string r; bool endflag;&#125;rule;void split(vector&lt;string&gt; &amp;v, char *s)&#123; char *sp = strtok(s,"/"); while(sp)&#123; v.push_back(sp); //strtok()是非线程安全的 sp = strtok(NULL,"/"); &#125;&#125;//字符串全是数字bool isint(string s)&#123; for(int i = 0; i &lt; s.length(); i++)&#123; if(!isdigit(s[i])) &#123; return false; &#125; &#125; return true;&#125;//合法字符串bool isLegal(string s)&#123; for(int i = 0; i &lt; s.length(); i++)&#123; if(isdigit(s[i]) || isalpha(s[i]) || s[i]=='/'|| s[i]=='-'|| s[i]=='.'|| s[i]=='_')&#123; continue; &#125;else &#123; return false; &#125; &#125; return true;&#125;int main()&#123; int n,m; cin&gt;&gt;n&gt;&gt;m; rule *a = new rule[n]; for(int i = 0; i &lt; n; i++)&#123; string str; //读取换行符，并舍弃 getline(cin,str,' '); str = str.substr(1); a[i].endflag = (str[str.length()-1] =='/')?0:1; //cout&lt;&lt;"ru last: "&lt;&lt;str[str.length()-1]&lt;&lt;endl; split(a[i].p,(char *)str.data()); cin&gt;&gt;a[i].r; &#125; for(int i = 0; i &lt; m; i++)&#123; string str; vector&lt;string&gt; url; vector&lt;string&gt; ans; //结尾中是否有/ bool flag; //是否存在path //bool pathflag = 0; //分离字符串 cin&gt;&gt;str; flag = (str[str.length()-1] =='/')?0:1; //cout&lt;&lt;"url last: "&lt;&lt;str[str.length()-1]&lt;&lt;endl; split(url,(char *)str.data()); //寻找匹配规则 int j; for(j = 0; j &lt; n; j++)&#123; int ui = 0,pi = 0; int ul = url.size(); int pl = a[j].p.size(); bool match = 1; ans.clear(); while(match &amp;&amp; ui &lt; ul &amp;&amp; pi &lt; pl )&#123; if(!isLegal(url[ui]))&#123; match = 0; ans.clear(); break; &#125; if(a[j].p[pi] == "&lt;int&gt;")&#123; if(isint(url[ui]))&#123; int w; //去除前导零 for(w=0;w&lt;url[ui].size()-1&amp;&amp;url[ui][w]=='0';w++); ans.push_back(url[ui].substr(w)); ui++;pi++; &#125;else&#123; match = 0; &#125; &#125;else if(a[j].p[pi] == "&lt;path&gt;")&#123; if(flag&amp;&amp;a[j].endflag)&#123; //将后面的字符串看作一个整体 string ts; while(ui&lt;ul) ts = ts+"/"+url[ui++]; ans.push_back(ts.substr(1)); pi++; /*pathflag = 1; while(pathflag)&#123; ans.push_back(url[ui]); ui++;pi++; if(ui &lt; ul)&#123; ans.push_back("/"); &#125; else break; &#125;*/ break; &#125;else&#123; match = 0; &#125; &#125;else if(a[j].p[pi] == "&lt;str&gt;" )&#123; ans.push_back(url[ui]); ui++;pi++; &#125; else if(a[j].p[pi] == url[ui])&#123; ui++;pi++; &#125; else&#123; match = 0; &#125; &#125;//// end of match //cout&lt;&lt;flag&lt;&lt;" "&lt;&lt;a[i].endflag; if(match &amp;&amp; (ui == ul) &amp;&amp; (pi == pl)&amp;&amp; (flag==a[j].endflag))&#123; break; break; &#125;else&#123; ans.clear(); &#125; &#125;// end of j if(j != n)&#123; cout&lt;&lt;a[j].r; for(int ansi = 0; ansi &lt; ans.size();ansi++)&#123; cout&lt;&lt;" "&lt;&lt;ans[ansi]; /*if(!pathflag)&#123; if(ansi &lt; ans.size()-1)&#123; cout&lt;&lt;" "; &#125; &#125;*/ &#125; &#125;else&#123; cout&lt;&lt;"404"; &#125; if(i != m-1) cout&lt;&lt;endl; &#125;// end of i delete [] a; return 0;&#125; 测试用例12345678910115 10/articles/2003/ special_case_2003/articles/&lt;int&gt;/&lt;int&gt;/&lt;str&gt;/ article_detail/articles/&lt;str&gt;/&lt;int&gt;/&lt;str&gt;/ le_detail/static/&lt;path&gt; static_serve/&lt;str&gt;/&lt;path&gt; serve/articles/2004/articles/2004//static/js/jquery.js/static/js/articles/1985/09/aloha/ 过程太艰难了。 他人满分代码出处 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118#include&lt;iostream&gt;#include&lt;cstring&gt;#include&lt;string&gt;#include&lt;vector&gt; using namespace std; const int N=105; struct Rule&#123; vector&lt;string&gt;p;//URL匹配的规则 string r;//URL 匹配的名字 bool flag;//标记规则最后是否有"/" &#125;a[N]; //字符串分割函数，将字符串s用"/"分割，并存放于向量v中 void split(vector&lt;string&gt; &amp;v,char *s)&#123; char *sp=strtok(s,"/"); while(sp) &#123; v.push_back(sp); sp=strtok(NULL,"/"); &#125;&#125; //判断字符串s是否都是数字 bool isNum(string s)&#123; for(int i=0;i&lt;s.length();i++) if(!isdigit(s[i])) return false; return true;&#125; int n,m;//规则和查询的条数 //处理URL地址，flag标记此URL地址最后是否有"/" void solve(vector&lt;string&gt;URL,bool flag)&#123; int i; vector&lt;string&gt;ans;//存放参数 for(i=0;i&lt;n;i++)//顺序遍历n条规则 &#123; ans.clear();//先清空 int j=0,k=0; vector&lt;string&gt;t(a[i].p); while(j&lt;t.size()&amp;&amp;k&lt;URL.size())//查看URL是否和此规则匹配 &#123; if(t[j]=="&lt;int&gt;")//情况一：&lt;int&gt; &#123; if(isNum(URL[k]))//如果都是数字 &#123; int w; for(w=0;w&lt;URL[k].size()-1&amp;&amp;URL[k][w]=='0';w++);//去前导零 ans.push_back(URL[k].substr(w)); j++,k++;//匹配下一部分 continue; &#125; &#125; else if(t[j]=="&lt;str&gt;")//情况二：&lt;str&gt; &#123; ans.push_back(URL[k]);//直接记录即可 j++,k++;//匹配下一部分 continue; &#125; else if(t[j]=="&lt;path&gt;")//情况三: &lt;path&gt; &#123; string s; while(k&lt;URL.size()) s=s+"/"+URL[k++]; ans.push_back(s.substr(1));//要去除第一个"/"符号 j++; continue; &#125; else if(t[j]==URL[k]) &#123; j++,k++;//匹配下一部分 continue; &#125; break; &#125; if(j==t.size()&amp;&amp;k==URL.size()&amp;&amp;flag==a[i].flag) break;//如果匹配就跳出 &#125; if(i==n) cout&lt;&lt;"404"&lt;&lt;endl;//如果n条规则都匹配失败，则输出"404" else &#123; cout&lt;&lt;a[i].r;//输出匹配的规则的名字 for(int w=0;w&lt;ans.size();w++)//输出各个参数 cout&lt;&lt;" "&lt;&lt;ans[w]; cout&lt;&lt;endl; &#125;&#125; int main()&#123; cin&gt;&gt;n&gt;&gt;m; string rule; for(int i=0;i&lt;n;i++) &#123; char temp[120]; scanf("%s",temp); cin&gt;&gt;a[i].r; a[i].flag=(temp[strlen(temp)-1]=='/')?true:false;//记录规则的最后是否有"/" split(a[i].p,temp);//分割 &#125; for(int i=0;i&lt;m;i++) &#123; vector&lt;string&gt;URL; char temp[120]; scanf("%s",temp); bool flag=(temp[strlen(temp)-1]=='/')?true:false;//记录规则的最后是否有"/" split(URL,temp);//分割 solve(URL,flag);//判断处理 &#125; return 0;&#125; 杭电OJ大数相加源代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#include &lt;iostream&gt;#include &lt;string.h&gt;using namespace std;void sum(char *a, char *b)&#123; int c[1000] = &#123;0&#125;, t[1000] = &#123;0&#125;; int aL = strlen(a); int bL = strlen(b); if( aL &gt;= bL )&#123; //游标 int i; //两数位数差 int x = aL - bL; for(i = bL-1; i &gt; -1 ; i--)&#123; //判断是否进位 c[i+x] = a[i+x]-'0' + b[i]-'0' + t[i+x+1]; if(c[i+x] &gt; 9)&#123; //标志前一位 需要加1（进位） t[i+x] = 1; c[i+x] -= 10; &#125; &#125; i = x-1; //a剩余位数直接赋值 for(; i &gt; -1 ; i--)&#123; c[i] = a[i]-'0' + t[i+1]; if(c[i] &gt; 9)&#123; //标志前一位 需要加1（进位） t[i] = 1; c[i] -= 10; &#125; &#125; if(t[0]==1) c[0] += 10; for(i = 0; i &lt; aL; i++) cout&lt;&lt;c[i]; // a&lt;b &#125;else&#123; sum(b,a); &#125;&#125;int main()&#123; int n,Case=0; cin&gt;&gt;n; while(n--)&#123; char a[1000]; char b[1000]; cin&gt;&gt;a&gt;&gt;b; Case++; cout&lt;&lt;"Case "&lt;&lt;Case&lt;&lt;":"&lt;&lt;endl&lt;&lt;a&lt;&lt;" + "&lt;&lt;b&lt;&lt;" = "; sum(a,b); //最后一行不需要换两行。。 if(n == 0) cout&lt;&lt;endl; else cout&lt;&lt;endl&lt;&lt;endl; &#125; return 0;&#125; 测试数据6组测试数据 699 1Case 1:99 + 1 = 100 888 744Case 2:888 + 744 = 1632 987654321987654321 987654321987654321Case 3:987654321987654321 + 987654321987654321 = 1975308643975308642 9999999999999999999 9Case 4:9999999999999999999 + 9 = 10000000000000000008 44 55Case 5:44 + 55 = 99 Case 6: 454546 4545499999999999999999999999999999 454546 + 4545499999999999999999999999999999 = 4545500000000000000000000000454545 递归转迭代（fi）源代码12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;string.h&gt;using namespace std;int main()&#123; int a,b,n; while(cin&gt;&gt;a&gt;&gt;b&gt;&gt;n)&#123; if(a == 0 || b == 0 || n == 0) break; if(n == 1) cout&lt;&lt;1&lt;&lt;endl; else if(n == 2) cout&lt;&lt;1&lt;&lt;endl; else&#123; int sum1 = 1 , sum2 = 1; int sum = 0; for(int i = 3; i &lt;= n; i++)&#123; sum = (a*sum2 + b*sum1) % 7; sum1 = sum2; sum2 = sum; &#125; cout&lt;&lt;sum&lt;&lt;endl; &#125; &#125; return 0;&#125; 字符串统计12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576#include &lt;iostream&gt;#include &lt;string.h&gt;using namespace std;int main()&#123; int n; cin&gt;&gt;n; //至0结束 while(n)&#123; //记录各种颜色出现多少次 //int *Count = new int[n]; int Count[1000] = &#123;0&#125;; int index = 0; //char (*p)[20] = new char[n][20]; char p[1000][20] = &#123;&#125;; bool flag = true; for(int i=0; i&lt;n; i++)&#123; char str[20]; cin&gt;&gt;str; //计数 if(i == 0)&#123; strcpy(p[index],str); Count[index]++; index++; &#125;else&#123; int j; for(j=0; j&lt;index; j++)&#123; if(strcmp(str,p[j]) == 0)&#123; Count[j]++; flag = false; break; &#125; &#125;//end of for //没有就添加进去 if(flag)&#123; strcpy(p[index],str); Count[index]++; index++; flag = true; &#125; &#125;//end of else &#125; //找到最大值 int Cmax = Count[0],t=0; for(int i = 1; i &lt; index; i++ )&#123; if(Cmax &lt; Count[i])&#123; Cmax = Count[i]; t = i; &#125; &#125; cout&lt;&lt;p[t]&lt;&lt;endl; //释放动态空间 // for(int i = 0; i &lt; n; i++)&#123; // delete []p[i]; // &#125; // delete []p; cin&gt;&gt;n; &#125; return 0;&#125;/*// template 模板template&lt;typename T&gt;//计算数组长度int calculateLength(T &amp;arr)&#123; cout&lt;&lt;"size arr:"&lt;&lt;sizeof(arr)&lt;&lt;endl; cout&lt;&lt;"size arr[0]:"&lt;&lt;sizeof(arr[0])&lt;&lt;endl; return sizeof(arr) / sizeof(arr[0]);&#125;*/ 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253/* 2072 */#include &lt;iostream&gt;#include &lt;string&gt;using namespace std;int main()&#123; string ju; string p[10001]; getline(cin,ju,'#'); int len = ju.length(); int index = 0; string letter; letter.clear(); for(int i = 0; i &lt; len; i++)&#123; if(ju[i]&gt;='a' &amp;&amp; ju[i]&lt;='z')&#123; letter.push_back(ju[i]); continue; &#125; if(!letter.empty())&#123; if(index == 0)&#123; p[index] = letter; index++; &#125;else&#123; int j; bool flag = true; for(j = 0; j &lt; index; j++)&#123; if(letter == p[j])&#123; flag = false; &#125; &#125;//end of for if(flag)&#123; p[index] = letter; index++; &#125; &#125;//end of inside if letter.clear(); &#125; &#125;// end of for cout&lt;&lt;index; return 0;&#125; 【1003】Max SumProblem Description Given a sequence a[1],a[2],a[3]……a[n], your job is to calculate the max sum of a sub-sequence. For example, given (6,-1,5,4,-7), the max sum in this sequence is 6 + (-1) + 5 + 4 = 14. Input The first line of the input contains an integer T(1&lt;=T&lt;=20) which means the number of test cases. Then T lines follow, each line starts with a number N(1&lt;=N&lt;=100000), then N integers followed(all the integers are between -1000 and 1000). Output For each test case, you should output two lines. The first line is “Case #:”, # means the number of the test case. The second line contains three integers, the Max Sum in the sequence, the start position of the sub-sequence, the end position of the sub-sequence. If there are more than one result, output the first one. Output a blank line between two cases. Sample Input 12325 6 -1 5 4 -77 0 6 -1 1 -6 7 -5 Sample Output 12345Case 1:14 1 4Case 2:7 1 6 题意给定T组序列，找到各个序列中和最大值，以及始点终点 思路采用动态规划，简化版-》当sum加上a[i]比a[i]小，则结束记录；重新开始下一段和的计算。同时更新最大值以及始点终点。 源代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#include &lt;iostream&gt;using namespace std;typedef long long ll;int all[1000000] = &#123;0&#125;;int main()&#123; int t; cin&gt;&gt;t; for(int index = 1; index &lt;= t; index++)&#123; ll n; cin&gt;&gt;n; int* a = new int[n] ; for(int i = 0; i &lt; n; i++) cin&gt;&gt;a[i]; int allMax,sum,startT,endT,f; allMax = sum = -0xfffffff; for(int i = 0; i &lt; n; i++)&#123; if((sum +a[i] &gt; a[i]) ||(sum +a[i] == a[i])) &#123; sum += a[i]; &#125;else&#123; sum = a[i]; f = i; &#125; if(sum &gt; allMax)&#123; allMax = sum; startT = f; endT = i; &#125; &#125; cout&lt;&lt;"Case " &lt;&lt;index&lt;&lt;":"&lt;&lt;endl; cout&lt;&lt;allMax&lt;&lt;" "&lt;&lt;startT+1&lt;&lt;" "&lt;&lt;endT+1&lt;&lt;endl; if(index != t)&#123; cout&lt;&lt;endl; &#125; &#125; return 0;&#125; 【1024】Max Sum Plus PlusProblem Description Now I think you have got an AC in Ignatius.L’s “Max Sum” problem. To be a brave ACMer, we always challenge ourselves to more difficult problems. Now you are faced with a more difficult problem. Given a consecutive number sequence S1, S2, S3, S4 … Sx, … Sn (1 ≤ x ≤ n ≤ 1,000,000, -32768 ≤ Sx ≤ 32767). We define a function sum(i, j) = Si + … + Sj (1 ≤ i ≤ j ≤ n). Now given an integer m (m &gt; 0), your task is to find m pairs of i and j which make sum(i1, j1) + sum(i2, j2) + sum(i3, j3) + … + sum(im, jm) maximal (ix ≤ iy ≤ jx or ix ≤ jy ≤ jx is not allowed). But I`m lazy, I don’t want to write a special-judge module, so you don’t have to output m pairs of i and j, just output the maximal summation of sum(ix, jx)(1 ≤ x ≤ m) instead. ^_^ Input Each test case will begin with two integers m and n, followed by n integers S1, S2, S3 … Sn.Process to the end of file. Output Output the maximal summation described above in one line. Sample Input 121 3 1 2 32 6 -1 4 -2 3 -2 3 Sample Output 1268 题意n个序列分m组，求出最大m个字段的序列元素和 思路源代码12]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>C++</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[代码精炼]]></title>
    <url>%2Farticle%2F2fdcd6a9%2F</url>
    <content type="text"><![CDATA[前言主要以 C++字体为主。 输入输出文件尾类型C 版 1234567#include &lt;stdio.h&gt;int main()&#123; int a,b; while (scanf("%d %d",&amp;a, &amp;b) != EOF) printf("%d\n",a+b);&#125; C++ 版 123456#include&lt;iostream&gt;using namespace std;int main()&#123; int a,b; while(cin&gt;&gt;a&gt;&gt;b)&#123;cout&lt;&lt;a+b&lt;&lt;endl;&#125;&#125; 在C++里，cin本身就是一个对象，因此可以直接运用到while( )检测是否有输入，若无输入则结束运行。 例如： 123 while(cin&gt;&gt;a)&#123; ...&#125; 格式化输出 1printf("%d%c",bmax," \n"[i==n]); 当后面的执行条件i!=n的时候，%c就相当于空格当后面的执行条件i==n的时候，%c就相当于\n 【来源】 字符串类型(1)cin&gt;&gt; ​ 可接受字符串，但遇到“空格”、“TAB”、”回车”（下一次缓冲区读取时 会被cin自动丢弃）均结束从缓冲区读取 (2)getline() ​ 默认”回车”（下一次缓冲区读取时 会被自动丢弃）结束 123456string line;getline（cin，line）;//按回车键结束输入#指定分隔符istream&amp; getline(istream&amp; is,string&amp; str,char delimiter='\n') //输入一串字符（不管多少个回车键），只要是在‘#’号之 前的字符都会读取并保存getline（cin,line,'#'); (3)cin.get() ​ 默认”回车”（下一次缓冲区读取时 仍会被读取）结束 ​ 两种用法： ​ a. cin.get(字符变量名)可以用来接收一个字符 12char ch;ch = cin.get(); or cin.get(ch); ​ b.cin.get(字符数组名,接收字符数目)用来接收一行字符串,可以接收空格 12char a[20];cin.get(a,20); ​ 注意： cin.get()会将回车键存储在缓存中，若后面还有cin.get()函数，则该函数会将缓存中的 回车 取出并赋予后面的输入变量中，故，使用了cin.get()函数就一定后面要加getchar()，将回车键读取并丢弃弃！！！​ (4)cin.getline()​ 接受一个字符串可以接受空格并输出 1234char m[20];cin.getline(m,5);//输入：jkjkjkjkkjkjkjkj//输出：jklj 第5个字符默认添加'\0' (5)gets()【c++11已弃用！！！】​ 接受一个字符，可以接受空格并输出，需包含头文件#include&lt;string&gt; 12char ch;gets(ch); ​ (6)getchar() 需包含头文件`#include&lt;stdio.h&gt;` getchar()是stdio.h中的库函数，它的作用是从stdin流中读入一个字符，也就是说，如果stdin有数据的话不用输入它就可以直接读取了，第一次getchar()时，确实需要人工的输入，但是如果你输了多个字符，以后的getchar()再执行时就会直接从缓冲区中读取了。​ 故一般用getchar()来清除缓存中的字符；​​ 4）输入强行退出​ Ctrl + Z 或输入EOF再按回车键 大佬解析:https://www.cnblogs.com/zzw1024/p/10502011.html 动态创建数组1234567891011//一维数组#include &lt;iostream&gt;#include &lt;string.h&gt;using namespace std;typedef long long ll;ll *aim = new ll[n];ll *maxj = new ll[n];memset(aim,0,sizeof(aim)*n);memset(maxj,0,sizeof(maxj)*n); 为何动态创建二维数组必先预先知道二维数组每一列的长度 大佬解析过 123456789101112131415161718192021222324252627282930/* 动态创建二维数组 *///半自动化char (*color)[20] = new char[n][20]; // √char *color[20] = new char[n][20]; // × /*[] 的优先级高于 * *color[20] 为 指针数组（array of pointers），即20个 char 类型的指针(*color)[20] 为 数组指针（a pointer to an array），即 指向数组（首地址，++按列大小）的指针*/ //释放 delete [] color;//真正意义上的创建二维数组int **array;array=new int *[row];for(int i = 0; i &lt; row; i++)&#123; array[i]=new int [col]; //以0初始化，#include &lt;string.h&gt; memset(array[i],0,col*sizeof(int));&#125;//释放for (int i = 0; i &lt; row; i ++) &#123; delete[] array[i]; array[i] = NULL; //不要忘记，释放空间后p[i]不会自动指向NULL值，还将守在原处，只是释放内存而已，仅此而已。&#125;delete [] array；array=NULL; 计算数组长度1234567template&lt;typename T&gt;//计算数组长度int calculateLength(T &amp;arr)&#123; cout&lt;&lt;"size arr:"&lt;&lt;sizeof(arr)&lt;&lt;endl; cout&lt;&lt;"size arr[0]:"&lt;&lt;sizeof(arr[0])&lt;&lt;endl; return sizeof(arr) / sizeof(arr[0]);&#125; STL标准库首先使得dev 支持 C++11 使devc++拥有c++11的功能(如何让devc++支持c++11) tools-&gt;compiler options-&gt;add the following commands when calling the compiler(勾选) ，然后添加如下语句（最好release\debug都添上） 1-std=c++11 头文件 1234567//set、map属于std, unordered_map属于std::tr1;#include &lt;set&gt;#include &lt;map&gt;#include &lt;tr1/unordered_map&gt;using namespace std::tr1;using namespace std; 调试 dev 首先设置”生成调试信息“ (1)Tools -&gt; setting -&gt;linker-&gt; generate debugging information 那栏改为”Yes“ (2) tool-&gt;environment options-&gt;watch variable under mouse(勾选) 设置断点后 编译 F5 调试（勾勾） F7运行下一行 四舍五入123456789101112131415161718192021222324252627#include&lt;iomanip&gt;#include&lt;iostream&gt;using namespace std;int main()&#123;double f = 123456789;// 输出1.23457*（10,6）（采用科学记数法变成包含整数和小数，共6位，且最后一位四舍五入）cout&lt;&lt;f&lt;&lt;endl; //输出120000000 1.2*pow（10,6）采用科学记数法,包含整数和小数，共两位，且最后一位四舍五入）cout&lt;&lt;setprecision(2)&lt;&lt;f&lt;&lt;endl; cout&lt;&lt;fixed&lt;&lt;f&lt;&lt;endl; //输出12345689.000000（小数6位补0）cout&lt;&lt;setprecision(2)&lt;&lt;fixed&lt;&lt;f&lt;&lt;endl; //输出12345689.00cout&lt;&lt;fixed&lt;&lt;setprecision(2)&lt;&lt;f&lt;&lt;endl; // 效果同上double f = 3.123456789;// 输出3.12346 （包含整数和小数，且四舍五入）cout&lt;&lt;f&lt;&lt;endl; //输出3.1（包含整数和小数，共两位，且最后一位四舍五入），这条会作用到下一条去cout&lt;&lt;setprecision(2)&lt;&lt;f&lt;&lt;endl; //输出3.123457 （仅包含小数，且四舍五入），没有上一条，则输出六位小数3.123457cout&lt;&lt;fixed&lt;&lt;f&lt;&lt;endl; cout&lt;&lt;setprecision(2)&lt;&lt;fixed&lt;&lt;f&lt;&lt;endl; //输出3.12 （小数2位，四舍五入）cout&lt;&lt;fixed&lt;&lt;setprecision(2)&lt;&lt;f&lt;&lt;endl; // 效果同上 system("pause"); return 0;&#125; setprecision(n)是流格式控制符之一，在iomanip头文件中。 c++默认的流输出数值有效位是6，包括整数和小数，若数值超出6位，则第七位四舍五入到6位数 fixed ：浮点值显示为定点十进制。 默认是小数6位数，不包含整数，若小数位超出6位，则四舍五入到6位数 1.setprecision(n) 指定一个浮点数的精度默认设置输出的数字的总位数为n，包含整数和小数部分；其中setprecision（0）效果是跟c++默认的流输出数值一样，有效位是6位，包括整数和小数 2.fixed ：必须与setprecision(n)配合使用，用来控制小数位数，不够补0，只要写一次fixed，后面的setprecision（n）就都是指小数了。 fixed与setprecision谁先谁后没有关系，但通常是fixed在前先固定6位小数（若此时小数已经超出6位，则先四舍五入到6位）再precision(n)取n位小数（n&lt;6） 3.如果与setiosnags(ios::scientific)合用， 可以控制指数表示法的小数位数。setiosflags(ios::scientific)是用指数方式表示实数。 4.resetiosflags(ios::fixed) 取消精度的设置。 1.）超出的位数会被四舍五入进去！！！ 2）与setw()不同 ，setprecision（n）一直作用到下一个setprecisin（n）之前，所以，只需要写一个setprecision（n）就可以。setw()要每次都写 floor(), ceil()函数都包含在头文件“Math.h”中 Floor() 不大于自变量的最大整数 Ceil() 不小于自变量的最大整数 数组开辟空间来源：C/C++数组的大小最大能有多大？ 局部变量：函数内申请的变量，数组，是在栈（stack）中申请的一段连续的空间。栈的默认大小为1M到2M，如果定义a[1024*1024];运行时就会报”段错误“； 全局变量：全局数组，静态数组（static）则是开在全局区（静态区）（static）。大小为2G，所以能够开的很大； 动态分配：malloc、new出的空间，则是开在堆（heap）的一段不连续的空间。理论上则是硬盘大小； 同一个程序共用栈和静态区，因此要申请一个超大数组或许定义为全局变量，而多个超大数组时最好是动态申请。 bugTime Limit Exceeded（1）可能是某种情况没有考虑到或者逻辑bug（很大概率是这个（；´д｀）ゞ） （2）算法不优 think数据范围，最好定义成long long 型123typedef long long ll;//定义长整型 ll n; 慎用递归，易导致超时动态规划大概率适合求最优解问题经典中的经典算法:动态规划(详细解释,从入门到实践,逐步讲解) 这一篇博文讲得比较透彻 逗号运算符逗号运算符是将一系列运算按顺序执行。 整个逗号表达式的值为系列中最后一个表达式的值。 1var = (count=19, incr=10, count+1); 在这里，首先把 count 赋值为 19，把 incr 赋值为 10，然后把 count 加 1，最后，把最右边表达式 count+1 的计算结果 20 赋给 var。上面表达式中的括号是必需的，因为逗号运算符的优先级低于赋值操作符 12a = 1*2,2*2,3*3;//a = 2;因，优先级低于 = 后面表达式只运行，不保留结果 https://www.runoob.com/cplusplus/cpp-comma-operator.html 计算机存储数据大佬解析: https://blog.csdn.net/daiyutage/article/details/8575248 乘法容易溢出 123456789101112131415161718192021222324252627#include &lt;iostream&gt;using namespace std;double sum(int n)&#123; if(n%2 != 0) return (n+1)/2*n; else return n/2*(n+1);&#125;int addSum(int n)&#123; int i,sum=0; for(i=1; i&lt;=n; i++)&#123; sum+=i; &#125; return sum;&#125;int main()&#123; int n; while(cin&gt;&gt;n)&#123; //cout&lt;&lt;sum(n)&lt;&lt;endl&lt;&lt;endl; cout&lt;&lt;addSum(n)&lt;&lt;endl&lt;&lt;endl; &#125; return 0;&#125; 初始化数据的重要性 在刷题过程中发现了如上bug ,以下面这段代码为例，目的是输出出现次数最多的字符串。 Count[1000] 、p[1000]若没有进行初始化，在多组数据输入后，虽然两者均为循环内部局部变量，但是未初始化将保留原来的数据，导致影响后续结果。 所以以后编程不要依赖于编译器缺省初始化，定义与初始化尽量同步~(ง •_•)ง 大佬解析：循环体内的局部变量内存分配和释放 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465#include &lt;iostream&gt;#include &lt;string.h&gt;using namespace std;int main()&#123; int n; cin&gt;&gt;n; //至0结束 while(n)&#123; //记录各种颜色出现多少次 //int *Count = new int[n]; int Count[1000] = &#123;0&#125;; int index = 0; //char (*p)[20] = new char[n][20]; char p[1000][20] = &#123;&#125;; bool flag = true; for(int i=0; i&lt;n; i++)&#123; char str[20]; cin&gt;&gt;str; //计数 if(i == 0)&#123; strcpy(p[index],str); Count[index]++; index++; &#125;else&#123; int j; for(j=0; j&lt;index; j++)&#123; if(strcmp(str,p[j]) == 0)&#123; Count[j]++; flag = false; break; &#125; &#125;//end of for //没有就添加进去 if(flag)&#123; strcpy(p[index],str); Count[index]++; index++; flag = true; &#125; &#125;//end of else &#125; //找到最大值 int Cmax = Count[0],t=0; for(int i = 1; i &lt; index; i++ )&#123; if(Cmax &lt;= Count[i])&#123; Cmax = Count[i]; t = i; &#125; &#125; cout&lt;&lt;p[t]&lt;&lt;endl; //释放动态空间 // for(int i = 0; i &lt; n; i++)&#123; // delete []p[i]; // &#125; // delete []p; cin&gt;&gt;n; &#125; return 0;&#125; C++的String对象（字符串类）String详解]]></content>
      <categories>
        <category>Programming</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>C++</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博文常用样式语句]]></title>
    <url>%2Farticle%2Fc4704731%2F</url>
    <content type="text"><![CDATA[文章标识123456789#同级标签tags: - [code] - [C++] - [algorithm]#父子分类categories: - [Programming] - [C++] 正文GitHub支持高亮貌似只能使用html形式 1&lt;label style="color:blue"&gt; XXX &lt;/label&gt; 1&lt;!--more--&gt; #表示只在首页展示以上内容]]></content>
      <categories>
        <category>tiny knowledge</category>
      </categories>
      <tags>
        <tag>code</tag>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[paperMemo - 3D Reconstruction]]></title>
    <url>%2Farticle%2Fc09ea54%2F</url>
    <content type="text"><![CDATA[三维重建定义根据单视图或者多视图的图像重建三维信息的过程 研究意义 港科大教授权龙认为：真正意义上的计算机视觉要超越识别，感知三维环境。 三维重建将作为计算机视觉的核心。将识别与重建融为一体，实现实时、精确的场景模型重构以及环境感知。作为环境感知的关键技术之一，可用于自动驾驶、虚拟现实、运动目标监测、行为分析、安防监控和重点人群监护等。 实现手段根据 采集设备是否主动发射测量信号 ，分为两类：基于主动视觉理论和基于被动视觉的三维重建方法。 主动视觉三维重建方法：主要包括结构光法和激光扫描法。 被动视觉三维重建方法：被动视觉只使用摄像机采集三维场景得到其投影的二维图像，根据图像的纹理分布等信息恢复深度信息，进而实现三维重建。 根据 模型重构类型 ，分为四种：深度图（depth）、点云（point cloud）、体素（voxel）、网格（mesh） 深度图（depth）: 图中每个像素值代表的是物体到相机xy平面的距离，单位为 mm。 点云（point cloud）：某个坐标系下的点的数据集。点包含了丰富的信息，包括三维坐标X，Y，Z、颜色、分类值、强度值、时间等等。在我看来点云可以将现实世界原子化，通过高精度的点云数据可以还原现实世界。万物皆点云，获取方式可通过三维激光扫描等。 体素（voxel）：三维空间中的一个有大小的点，一个小方块，相当于是三维空间种的像素。 网格（mesh）：由三角形组成的多边形网格。多边形和三角网格在图形学和建模中广泛使用，用来模拟复杂物体的表面，如建筑、车辆、人体，当然还有茶壶等。任意多边形网格都能转换成三角网格。 缺点： 从单张图片恢复出三维物体形状这一研究课题在许多应用中扮演着重要的角色，例如增加现实，图像编辑。但是由于物体的拓扑结构复杂多变，这一课题也颇具挑战性。目前，基于体素表达的方法受限于三维卷积网络计算和内存的限制而难以得到高分辨率的输出。基于点云表达的方法又很难生成平滑而又干净的表面。 三角网格表达对物体形状提供了一种更有效，更自然的离散化逼近方式，但是计算复杂。 （ volume受到分辨率和表达能力的限制，会缺乏很多细节；point cloud 的点之间没有连接关系，会缺乏物体的表面信息。相比较而言mesh的表示方法具有轻量、形状细节丰富的特点。 ） 这些方法本质上是在对一个给定拓扑连接关系的初始网格变形，比较有代表性的初始网格有单位平面，球。尽管它们有一定的效果，但是仍然难以恢复具有复杂拓扑结构的物体表面， 未来趋势将三者方式结合起来，从单张RGB图片中识别多个物体，并重建三维模型。 文献2017 A Point Set Generation Network for3D Object Reconstruction from a Single Image作者：Haoqiang Fan（Tsinghua University）, ​ Hao Su, Leonidas Guibas（Stanford University）来源：CVPR 2017 (oral)文章链接：https://arxiv.org/abs/1612.00603源码链接：https://github.com/fanhqme/PointSetGeneration 方法： 网络分为3个版本，输入都为二维图像I和一个用来使系统产生分布式输出的随机向量r，输出为N * 3的矩阵M。 vanilla versionvanilla version分为Encoder（编码器）和Predictor（预测器），Encoder由卷积层和ReLU层组成，随机向量r干扰了图像I的预测，Predictor由全连接网络生成N个点的坐标。 two prediction branch version该网络由vanilla改进而来，在vanilla的基础上在预测器中加入了deconv（反卷积），由于全连接网络对复杂结构表现出良好的性能，但对于简单光滑的结构便显得有些繁重，故引入deconv结构来优化网络，使得参数变少，并且这种结构对光滑表面效果很好。tips：其中添加了多个编码器和预测器之间的链接，以促进编码和预测之间的信息流。 hourglass version沙漏版本在前一个版本的基础上增加了递归循环，能更好的进行编解码操作，具有较强的表示能力，能较好的融合全局和局部信息。 损失函数： Chamfer distance （倒角距离 ） 在另一个集合中找到最近的点，并将其距离的平方求和。（并不是距离公式，因为它不满足三角形不等式）其特点是能更好的保存物体的详细形状 Earth Mover`s distance （EMD距离） 测量两个分布之间的距离，能产生相较于CD算法更紧凑的结果，但有时会过度收缩局部结构 结果： 成功的重建 失败的重建 贡献： （1）开创了单个2D视角用点云重构3D物体的先例（单图像3D重建） （2）系统地探讨了体系结构中的问题点生成网络的损失函数设计 （3）提出了一种基于单图像任务的三维重建的原理及公式和解决方案 （4）将输入改为RGBD图像，能够更加弥补原先模型缺失的部分 缺点： （1） 对于一部分与训练数据存在一定偏差的输入，该网络会用类似的东西来解释输入。 （2） 对于多个对象的组合，因没有实现检测或注意机制，导致输出失真。 2019 （重点看）A Skeleton-bridged Deep Learning Approach for Generating Meshes of Complex Topologies from Single RGB Images作者： Jiapeng Tang, Xiaoguang Han , Junyi Pan, Kui Jia y, Xin Tong School of Electronic and Information Engineering, South China University of Technology Shenzhen Research Institute of Big Data, the Chinese University of Hong Kong (Shenzhen) Microsoft Research Asia 来源：CVPR 2019(oral)文章链接：https://arxiv.org/abs/1903.04704?context=cs.CV源码链接：暂无 方法： 第一阶段是从输入图像中学习生成骨架点云。为此他们设计了平行的双分支网络架构，被命名为 CurSkeNet 和 SurSkeNet，分别用于曲线状和曲面状骨架点云的合成。为了 CurSkeNet 和 SurSkeNet 的训练，他们针对 ShapeNet 的物体模型处理了骨架数据集来当做 ground truth 用于训练。 他们采用编码器-解码器的网络结构从输入图片 I 学习出对应的骨架 K，它本质上是一个更简洁紧凑的点云表达。 在第二个阶段，他们通过将合成的骨架点云体素化，然后用三维卷积网络对粗糙的骨架体素进行修复提取出一个初始网格。此处为了减小高清体素输出时的网络复杂度，采取了用全局结构引导分块体素修复的方式，得到一个更精细化的体素 V。 最后一个阶段先从体素 M 中提取出一个粗糙的初始网格 Mb，然后再用图神经网络对网格的顶点位置进一步优化，得到最后的输出网格 M。 每个阶段都有一个图像编码器-解码器来提取所各自需要的信息结果： 贡献： 提出了阶段性学习的方法，利用了点云，体积和网格各自的优点，而且对有孔洞的物体又能够很好地重建 骨架桥接的方法 腐蚀学习方法来验证算法功效 缺点： 在多次训练后，仍然会在连接处、角落等缺失，或许添加深度图信息会有所改善 模型过于复杂 所展示的模型只是简单场景中的单个物体，而且最终轮廓比较圆润，没有线条感 Mesh R-CNN作者： Georgia Gkioxari、 Jitendra Malik、 Justin Johnson Facebook AI Research (FAIR) 来源：CVPR 2019(oral)文章链接：https://www.researchgate.net/publication/333650113_Mesh_R-CNN源码链接：暂无 方法： 基于Mask R-CNN（论文链接、源码链接， 在进行目标检测的同时进行实例分割 ）改进而来，增加了网格预测分支来输出高分辨的目标三角网格。 通过先预测转化为物体的粗体素分布并转化为三角形网格表示，然后通过一系列的图卷积神经网络改进网格的边角输出具有不同拓扑结构的网格。 结果： 贡献： 开发了从2D感知到3D形状预测的方法（ 图卷积神经网络 ） 由粗到细调整的思想 目标3D三角网格预测，有效地预测带有孔洞的物体 ，同时对于复杂环境中的三维物体也有良好的预测效果。 缺点： 细节缺失较为严重，如上图的凳腿，椅背 重构的模型看起来比较粗糙，没有线条感（作者解释说缺乏大量的真实场景数据-监督） Robust Point Cloud Based Reconstruction of Large-Scale Outdoor Scenes参考文章港科大教授权龙：计算机视觉下一步将走向三维重建 | CCF-GAIR 2018 基于深度学习的视觉三维重建研究总结 三维重建初探 CVPR 2019 | 基于骨架表达的单张图片三维物体重建方法]]></content>
      <categories>
        <category>Scientific Research</category>
      </categories>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文小知识]]></title>
    <url>%2Farticle%2Feeee6404%2F</url>
    <content type="text"><![CDATA[a.k.a.（又被称为）also known as的缩写，引出另一种对该物的表述。 例句1:The proposed DUQ is based on sequence-to-sequence (seq2seq, a.k.a. Encoder-Decoder). In part because you have the convenience to select specific keywords, write specific ads and direct the click-through to a specific web page (a.k.a. landing page). 最好把a.k.a连同它的解释一块放在括号中. e.g.(用于举例)exampli gratia（”for example; for instance;such as”: 举例如）的缩写，其目的用若干例子来让前面说法更具体，更易感知。 例句1: Buy some vegetables, e.g., carrots. 使用中，不仅e后的“.”常常被漏掉（如写成eg.） 为了方便记忆，你可把”e.g.” 与 “example given” 联想起来。 最好把e.g.连同它的例子放在括号中，如 例句2：I like quiet activities (e.g., reading) etc.（“等等”）et cetera(“and so forth; and the others; and other things; and the rest; and so on”:等等)的缩写。它放在列表的最后，表示前面的例子还没列举完，最后加个词“等等”。 例句3: I need to go to the store and buy some pie, milk, cheese, etc. etc.前面要有逗号。 不要在e.g.的列表最后用etc( 在including后的列表后也不宜使用etc)，这是因为 e.g. 表示泛泛的举几个例子，并没有囊括所有的实例，其中就已经有“等等”的含义了，如果再加一个 etc. 就多余了，例如这是错的： Writing instructors focus on a number of complex skills that require extensive practice (e.g., organization, clear expression, logical thinking, etc.) et al.（“等人”） 这是et alia（”and others; and co-workers”:等它人）的缩写。它几乎都是在列文献作者时使用，即把主要作者列出后，其它作者全放在et al. 里面。 例句4: These results agree with the ones published by Pelon et al. (2002). 人的场合用et al，而无生命的场合用etc.(et cetera)。 et后不要加“.”,因为et不是缩写。另外，与etc.不同，et al.的前面不要逗号。 i.e.（“换句话说”）这是id est（“that is” , “in other words”:也就是）的缩写。目的是用来进一步解释前面所说的观点（不是像e.g.那样引入实例来形象化），意思是“那就是说，换句话说”。你可把”i.e.” 与 “in essence” 联想起来。 例句5：In 2005, American had the lowest personal saving rate since 1933. In fact it was outright negavetive—i.e., consumers spent more money that they made. 例句6：There are three meals in the day (i.e., breakfast, lunch, and dinner) 使用中，i.e.的第一个”.”也常常被错误地漏掉了。它后面紧跟着一个逗号，再跟一个解释。 如同e.g., i.e.也最好放入括号中，如同例句6那样。 比较下面两个例句。 例句7：I like to eat boardwalk food, i.e., funnel cake and french fries. 例句8：I like to eat boardwalk food, e.g., funnel cake and french fries. 例句7表示只有 funnel cake and french fries这两样boardwalk食物，我喜欢。例句8表示我喜欢boardwalk食物，比如 funnel cake and french fries；其实snow cones and corn dogs等其他类型，我也喜欢。 viz.（”即“ 列项） 这是videlicet（ “namely”, “towit”, “precisely”, “that is to say”：即）的缩写，与e.g.不同，viz位于同位列表之前，要把它前面单词所包含的项目全部列出。 例句9：“Each symbol represents one of the four elements, viz. earth, air, fire, and water.”(每个符号代表如下四个元素之一，即： 地球，空气，火焰和水)。 例句10: The noble gases, viz. helium, neon, argon, xenon, krypton and radon, show a non-expected behaviour when exposed to this new element. 注意 viz.后面无逗号。 信息来源http://blog.sciencenet.cn/blog-510768-1112873.html]]></content>
      <categories>
        <category>tiny knowledge</category>
      </categories>
      <tags>
        <tag>tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机图形学-实验]]></title>
    <url>%2Farticle%2F9200fd3%2F</url>
    <content type="text"><![CDATA[实验 GLFW GLAD 123456789101112131415//渲染while (!glfwWindowShouldClose(window))&#123; //glClearColor(0.2f, 0.3f, 0.3f, 1.0f); glClearColor(0.0f, 0.34f, 0.57f, 1.0f); glClear(GL_COLOR_BUFFER_BIT); //交换颜色缓存，开始绘制 glfwSwapBuffers(window); //检查触发事件 glfwPollEvents();&#125; VAO 顶点数组对象 VBO 顶点缓冲对象 实验一：绘制三角形123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185#include &lt;glad/glad.h&gt;#include &lt;GLFW/glfw3.h&gt;#include &lt;iostream&gt;using namespace std;const unsigned int SCR_WIDTH = 800;const unsigned int SCR_HEIGHT = 600;int main()&#123; //cout &lt;&lt; SCR_HEIGHT &lt;&lt; endl; /* 初始化OpenGL 1)初始化GLFW 2)创建窗口 3)初始化GLAD 4)创建视口 */ // 1）初始化GLFW glfwInit(); //设置主版本号、次版本号为3.3 glfwWindowHint(GLFW_CONTEXT_VERSION_MAJOR, 3); glfwWindowHint(GLFW_CONTEXT_VERSION_MINOR, 3); //使用核心模式，即只能使用openGL功能的一个子集（没有向后兼容特性） glfwWindowHint(GLFW_OPENGL_PROFILE, GLFW_OPENGL_CORE_PROFILE); //Mac OS需要加上这句 glfwWindowHint(GLFW_OPENGL_FORWARD_COMPAT, GL_TRUE); //不可改变窗口大小 glfwWindowHint(GLFW_RESIZABLE, GL_FALSE); // 2)设置GLFW窗口对象 GLFWwindow* window = glfwCreateWindow(SCR_WIDTH, SCR_HEIGHT, "LearnOpenGL", NULL, NULL); if (window == NULL) &#123; //若未创建成功，终止程序 std::cout &lt;&lt; "Failed to create GLFW window" &lt;&lt; std::endl; glfwTerminate(); return -1; &#125; //设置GLFW上下文(状态)为当前线程的上下文 glfwMakeContextCurrent(window); //GLAD管理OpenGL函数指针 if (!gladLoadGLLoader((GLADloadproc)glfwGetProcAddress)) &#123; std::cout &lt;&lt; "Failed to initialize GLAD" &lt;&lt; std::endl; return -1; &#125; //指定当前视口尺寸（左下角坐标，视口宽高） glViewport(0, 0, SCR_WIDTH, SCR_HEIGHT); /* 数据处理 */ //1）三角形三个顶点输入（标准化，0-1之间） const float triangle[] = &#123; -0.5f,-0.5f,0.0f, //左下 0.5f,-0.5f,0.0f, //右下 0.0f,0.5f,0.0f //正上 &#125;; //2）VAO（顶点数组对象） VBO（顶点缓冲对象） GLuint vertex_array_object; glGenVertexArrays(1, &amp;vertex_array_object); glBindVertexArray(vertex_array_object); GLuint vertex_buffer_object; glGenBuffers(1, &amp;vertex_buffer_object); glBindBuffer(GL_ARRAY_BUFFER, vertex_buffer_object); glBufferData(GL_ARRAY_BUFFER, sizeof(triangle), triangle, GL_STATIC_DRAW); //设置顶点属性指针 //顶点着色器的位置，顶点属性为3分量向量，是否标准化，连续顶点属性之间的间隔，数据偏移量 glVertexAttribPointer(0, 3, GL_FLOAT, GL_FALSE, 3 * sizeof(float), (void*)0); glEnableVertexAttribArray(0); // 3) 生成并编译着色器 const char *vertex_shader_source = "#version 330 core\n" "layout(location = 0 ) in vec3 aPos;\n" "void main()\n" "&#123;\n" " gl_Position = vec4(aPos, 1.0);\n" "&#125;\n\0"; const char *fragment_shader_source = "#version 330 core\n" "out vec4 FragColor;\n" "void main()\n" "&#123;\n" " FragColor = vec4(1.0f, 0.5f, 0.2f, 1.0f);\n" "&#125;\n\0" ; //状态标识量 int success; char info_log[512]; //顶点着色器 int vertex_shader = glCreateShader(GL_VERTEX_SHADER); glShaderSource(vertex_shader, 1, &amp;vertex_shader_source, NULL); glCompileShader(vertex_shader); //检查编译是否成功 glGetShaderiv(vertex_shader, GL_COMPILE_STATUS, &amp;success); if (!success) &#123; glGetShaderInfoLog(vertex_shader, 512, NULL, info_log); cout&lt;&lt;"error 1:"&lt;&lt; info_log &lt;&lt; endl; &#125; //片段着色器 int fragment_shader = glCreateShader(GL_FRAGMENT_SHADER); glShaderSource(fragment_shader, 1, &amp;fragment_shader_source, NULL); glCompileShader(fragment_shader); //检查编译是否成功 glGetShaderiv(fragment_shader, GL_COMPILE_STATUS, &amp;success); if (!success) &#123; glGetShaderInfoLog(fragment_shader, 512, NULL, info_log); cout &lt;&lt; "error 2:" &lt;&lt; info_log &lt;&lt; endl; &#125; //链接顶点和片段着色器至一个着色器程序 int shader_program = glCreateProgram(); glAttachShader(shader_program, vertex_shader); glAttachShader(shader_program, fragment_shader); glLinkProgram(shader_program); //检查链接是否成功 glGetProgramiv(shader_program, GL_LINK_STATUS, &amp;success); if (!success) &#123; glGetProgramInfoLog(fragment_shader, 512, NULL, info_log); cout &lt;&lt; "error 3:" &lt;&lt; info_log &lt;&lt; endl; &#125; //渲染 while (!glfwWindowShouldClose(window)) &#123; //渲染指令 glClearColor(0.2f, 0.3f, 0.3f, 1.0f); //清空颜色缓冲后，填充蓝色 //glClearColor(0.0f, 0.34f, 0.57f, 1.0f); //清除颜色缓冲 glClear(GL_COLOR_BUFFER_BIT); //使用着色器程序 glUseProgram(shader_program); //绘制三角形 glBindVertexArray(vertex_array_object); glDrawArrays(GL_TRIANGLES, 0, 3); //解除绑定 glBindVertexArray(0); //交换颜色缓存，开始绘制 glfwSwapBuffers(window); //检查触发事件 glfwPollEvents(); &#125; //删除VAO VBO glDeleteVertexArrays(1,&amp;vertex_array_object); glDeleteBuffers(1, &amp;vertex_buffer_object); //释放资源 glfwTerminate(); return 0;&#125;]]></content>
      <categories>
        <category>Computer Graphics</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机图形学-课程笔记]]></title>
    <url>%2Farticle%2F87477f70%2F</url>
    <content type="text"><![CDATA[VS2017 注释： 先CTRL+K，然后CTRL+C 取消注释： 先CTRL+K，然后CTRL+U 图形应用计算机辅助设计(CAD:computer-aided design) 虚拟现实环境(virtual-reality environment) 科学计算可视化（scientific visualization） 未来发展表情与动作的精细化 图形学与深度学习的碰撞 阴极射线管（CRT） 发光二极管显示器（LED：Liquid-emtting diode） GPU 擅长于计算 像素：由图像的小方格组成的,这些小方块都有一个明确的位置和被分配的色彩数值,小方格颜色和位置就决定该图像所呈现出来的样子 显示分辨率：水平像素数*垂直像素数 ，取决于光点与帧缓存（存放一帧的信息）的大小 帧缓存的容量 = 分辨率 * 颜色位面数 可编程渲染管线发展历程固定管线：程序员控制权减少，不灵活 可编程图形库：自由搭建，功能范围广 从固定到可编程：hooks()函数（钩函数）突破固定功能流水线的限制，使用可编程着色器修改 流水线中特定步骤的行为。 GPU渲染管线应用阶段 -》 几何阶段 -》光栅化 软光栅扫描转换也就是光栅化,点阵单元是像素点阵 图源经过算法计算（不借助硬件提供的api）转换成像素点 逼近的本质相当于 连续量向离散量的转换 直线扫描转换算法逐点比较法、正负法、数值微分算法、Bresenham算法 数值微分算法（DDA） DDA是增量算法，优点：简单直观易实现，缺点：有浮点数和浮点运算，效率不高 记得取整（光栅化过程中不可能绘制半个像素点） 源码实现： Bresenham算法（1）中点 算法步骤： 假定0&lt;k&lt;1,x是最大位移方向；为防止浮点数出现，d（误差项）放大2倍 （2）改进 圆的扫描转换算法 常规计算太复杂 Bresenham算法 椭圆的扫描转换算法在椭圆弧的扫描转换算法中，如果考虑的是中心在原点，第一象限的1/4段椭圆弧，则上下部分的分界点是椭圆弧上法向量x、y两个分量相等的点 多边形的扫描转换算法X-扫描线思想 由于像素点较小，因此中心偏移看不出来 算法效率问题 ​ Y向连贯性算法具体算法思想查看此处 边标志算法上闭下开 区域填充 边界填充算法 8连通边界算法不可以填充4连通边界表示区域 泛填充算法通常用于给区域重新着色 8连通泛填充算法可以填充4泛填充的内点表示区域 重复入栈问题 属性颜色，虚实，宽细，纹理贴图 根据斜率改变模板 走样与反走样 为提高图片质量，进行反走样 造型技术研究如何在计算机中建立恰当的模型表示不同图形对象的技术 对象：规则对象（几何模型）、不规则对象（不能用欧式几何加以描述的对象如山树云烟） 实体 样条 样条的描述方法 Bezier实例 特性： （1）曲线总是通过第一个和最后一个控制点 （2）曲线起始点处的切线落在头两个控制点的连线上，曲线终点处的切线落在后两个控制点的连线上 （3）曲线落在控制点的凸壳内 （4）封闭曲线的第一个点和最后一个点重合； 多个控制点位于同一位置需要更多加权 实体模型的三类表示多边形表示（三角形或四边形） 扫描表示（旋转扫描、广义扫描） 构造实体几何法：由两个实体间的并、交或差操作生成的新的实体（CSG树） 光线投射算法 空间位置枚举 （1）八叉树 （2）松散八叉树 （3）BSP树 与八叉树相比较：自适应分割，有效减少树的深度和搜索时间 ​ 有效识别前向面与后向面 轴对齐：① xyz轴的顺序进行分割 或者 ②最长边 多边形对齐 ： 常运用于深度测试、相交测试、碰撞测试 分形几何熵：体系混乱程度的度量 分形： 具有以非整数维形式充填空间的形态特征，fractal 粒子系统模拟火、烟、水流… 模拟多个粒子及其运动（每个粒子都有生命值） 渲染过程中粒子始终面对着摄像机方向（如NPC：Non-Player Character 的名字） 变换与观察图形的几何变换是指对图形的几何信息经过平移、比例、旋转（逆时针为正方向）、对称、错切（剪切、错位变换，用于产生弹性物体的变形处理）等变换后产生新的图形 相关公式： 齐次坐标用n+1维向量表示一个n维向量（但具有不唯一性） 运用齐次坐标可以同时表示多种变化 齐次坐标三维变换 关于坐标原点： 复合变化其中三维复合变化可通过每次变换矩阵相乘来得到 比较难以理解：https://www.icourse163.org/learn/HUST-1003636001?tid=1206895203#/learn/content?type=detail&amp;id=1211820060&amp;cid=1214742100 （1）对任意参考点： eg： （2）对任意轴： 观察变换 世界坐标到观察坐标系的变换 ：实际上是世界坐标系Q（x,y,z）在观察坐标系中的坐标值 ，最后不需要逆变换回去 模型变化与观察变化具有对偶性 投影变换观察变换中隐含一个观察平面，即投影平面 按照投影中心的位置 投影分类 平行投影 正投影分为 三视图（主侧俯）与正轴测图 等轴测：投影面与三个坐标面的夹角都相等 正二测：投影面与二个坐标面的夹角都相等 正三测：投影面与三个坐标面的夹角都不相等 斜投影 透视投影 规范化投影 观察窗口的大小、投影方式的不同都影响着观察空间 为后续操作方便，因此规范化 通过openGL函数调用来完成这一操作 裁剪与屏幕映射CS算法 按左下右上的顺序求出直线段求出直线段与窗口边界的交点 改进后的CS中点法 LB算法将线段看作“先进后出”的带方向的线，建立参数方程，求出与裁剪窗口的交点，6个点：起终点，与窗口边界及其延长线的交点 多边形的裁剪 该算法遇到凹边形时会出现问题：多余边V2、V3 另一种算法 窗口定义为pw，多边形定义为ps 线段按指定方向，在可见侧进入不可见侧时进行相应操作（以BC,CD为例 到V2时同时输出V1、V2，并返回D点） 三维空间的裁剪增加至6个点，与面的交点 增加至8个点，与面的交点]]></content>
      <categories>
        <category>Computer Graphics</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-pytorch框架]]></title>
    <url>%2Farticle%2F6dc244e8%2F</url>
    <content type="text"><![CDATA[前言理论知识对于 AI 算法工程师极其重要。敲代码只是思路的一个实现过程。这里的「算法」和计算机 CS 的「算法」还不太一样，AI 算法是偏数学推导的，所以数学底子还是需要点的，学的越深，要求越高。面试的时候，很少让手写代码，90% 都是在问模型抠算法细节。 Tensor张量是对矢量和矩阵向潜在的更高维度的泛化,表示为基本数据类型的 n 维数组。 张量中的每个元素都具有相同的数据类型，且该数据类型一定是已知的。形状，即张量的维数和每个维度的大小，可能只有部分已知。 具体特点 使用方法 Note（1）任何以“_”结尾的操作都会用结果替换原变量 ​ 例如: x.copy_(y), x.t_(), 都会改变 x. （2）Torch Tensor与NumPy数组共享底层内存地址，修改一个会导致另一个的变化。 （3）所有的 Tensor 类型默认都是基于CPU， CharTensor 类型不支持到 NumPy 的转换. （4）Tensor 和 Function互相连接并生成一个非循环图，它表示和存储了完整的计算历史。每个张量都有一个.grad_fn属性，这个属性引用了一个创建了Tensor的Function（除非这个张量是用户手动创建的，即，这个张量的grad_fn是None） 如果需要计算导数，你可以在Tensor上调用.backward()。如果Tensor是一个标量（即它包含一个元素数据）则不需要为backward()指定任何参数，但是如果它有更多的元素，你需要指定一个gradient 参数来匹配张量的形状。 （5）torch.nn 只支持小批量输入。整个 torch.nn包都只支持小批量样本，而不支持单个样本。例如，nn.Conv2d 接受一个4维的张量，每一维分别是sSamples nChannels Height Width（样本数通道数高宽）如果你有单个样本，只需使用 `input.unsqueeze(0) 来添加其它的维数 （6）output为网络的输出，target为实际值 （7）torch.view()与 np.reshape()的区别： 效果一样，reshape（）操作nparray，view（）操作tensor view（）只能操作contiguous的tensor，且view后的tensor和原tensor共享存储，reshape（）对于是否contiuous的tensor都可以操作。 （8） 12import matplotlib.pyplot as plt plt.imshow(np.transpose(npimg, (1, 2, 0))) img的格式为（channels,imagesize,imagesize），plt.imshow在现实的时候输入的是（imagesize,imagesize,channels） 因此由np.transpose函数进行转换 np.transpose函数理解 常用于图片翻转 （9）在张量创建时，通过设置 requires_grad 标识为Ture来告诉Pytorch需要对该张量进行自动求导，PyTorch会记录该张量的每一步操作历史并自动计算 机器学习知识点快速回顾 神经网络快速回顾 卷积神经网络快速回顾 其中GoogLeNet (Inception) 、ResNet（跳连接）仔细看看 （10）MNIST数据集手写数字识别 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimfrom torchvision import datasets, transformstorch.__version__BATCH_SIZE = 512EPOCHS = 20DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")train_loader = torch.utils.data.DataLoader( datasets.MNIST( 'data', train=True, download=True, transform=transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=BATCH_SIZE, shuffle=True)test_loader = torch.utils.data.DataLoader( datasets.MNIST( 'data', train=False, transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,)) ])), batch_size=BATCH_SIZE, shuffle=True)class ConvNet(nn.Module): def __init__(self): super().__init__() # 1,28x28 self.conv1=nn.Conv2d(1, 10, 5) # 10, 24x24 self.conv2=nn.Conv2d(10, 20, 3) # 128, 10x10 self.fc1 = nn.Linear(20*10*10, 500) self.fc2 = nn.Linear(500, 10) def forward(self, x): in_size = x.size(0) out = self.conv1(x) #24 out = F.relu(out) out = F.max_pool2d(out, 2, 2) #12 out = self.conv2(out) #10 out = F.relu(out) out = out.view(in_size, -1) out = self.fc1(out) out = F.relu(out) out = self.fc2(out) out = F.log_softmax(out, dim=1) return outmodel = ConvNet().to(DEVICE)optimizer = optim.Adam(model.parameters())def train(model, device, train_loader, optimizer, epoch): model.train() for batch_idx, (data, target) in enumerate(train_loader): data, target = data.to(device), target.to(device) optimizer.zero_grad() output = model(data) loss = F.nll_loss(output, target) loss.backward() optimizer.step() if(batch_idx+1)%30 == 0: print('Train Epoch: &#123;&#125; [&#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)]\tLoss: &#123;:.6f&#125;'.format( epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))def test(model, device, test_loader): model.eval() test_loss = 0 correct = 0 with torch.no_grad(): for data, target in test_loader: data, target = data.to(device), target.to(device) output = model(data) test_loss += F.nll_loss(output, target, reduction='sum').item() # 将一批的损失相加 pred = output.max(1, keepdim=True)[1] # 找到概率最大的下标 correct += pred.eq(target.view_as(pred)).sum().item() test_loss /= len(test_loader.dataset) print('\nTest set: Average loss: &#123;:.4f&#125;, Accuracy: &#123;&#125;/&#123;&#125; (&#123;:.0f&#125;%)\n'.format( test_loss, correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))for epoch in range(1, EPOCHS + 1): train(model, DEVICE, train_loader, optimizer, epoch) test(model, DEVICE, test_loader) 其中 PyTorch中的nn.Conv1d与nn.Conv2d tips: 在PyTorch中，池化操作默认的stride大小与卷积核的大小一致； 如果池化核的大小为一个方阵，则仅需要指明一个数，即kernel_size参数为常数n，表示池化核大小为n x n。 question window下 tensorboard不显示相应的网络结构以及数据 解决方案https://www.cnblogs.com/tengge/p/6376073.html tips:路径中不能存在中文路径]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-卷积神经网络]]></title>
    <url>%2Farticle%2Fc486835d%2F</url>
    <content type="text"><![CDATA[互相关运算和卷积运算实际上，卷积运算与互相关运算类似。为了得到卷积运算的输出，我们只需将核数组左右翻转并上下翻转，再与输入数组做互相关运算。可见，卷积运算和互相关运算虽然类似，但如果它们使用相同的核数组，对于同一个输入，输出往往并不相同。 那么，你也许会好奇卷积层为何能使用互相关运算替代卷积运算。其实，在深度学习中核数组都是学出来的：卷积层无论使用互相关运算或卷积运算都不影响模型预测时的输出。 二维卷积层的核心计算是二维互相关运算。在最简单的形式下，它对二维输入数据和卷积核做互相关运算然后加上偏差。 特征图和感受野二维卷积层输出的二维数组可以看作是输入在空间维度（宽和高）上某一级的表征，也叫特征图（feature map）。影响元素x的前向计算的所有可能输入区域（可能大于输入的实际尺寸）叫做x的感受野（receptive field）。 以图为例，输入中阴影部分的四个元素是输出中阴影部分元素的感受野。我们将图中形状为2×2的输出记为Y，并考虑一个更深的卷积神经网络：将Y与另一个形状为2×2的核数组做互相关运算，输出单个元素z。那么，z在Y上的感受野包括Y的全部四个元素，在输入上的感受野包括其中全部9个元素。可见，我们可以通过更深的卷积神经网络使特征图中单个元素的感受野变得更加广阔，从而捕捉输入上更大尺寸的特征。 我们常使用“元素”一词来描述数组或矩阵中的成员。在神经网络的术语中，这些元素也可称为“单元”。 一般来说，假设输入形状是nh×nw，卷积核窗口形状是kh×kw，那么输出形状将会是(nh−kh+1)×(nw−kw+1). 所以卷积层的输出形状由输入形状和卷积核窗口形状决定。本节我们将介绍卷积层的两个超参数，即填充和步幅。它们可以对给定形状的输入和卷积核改变输出形状。 padding为防止卷积将图片变小，损失信息，在此操作前先进行填充。 一般来说，如果在高的两侧一共填充ph行，在宽的两侧一共填充pw 列，那么输出形状将会是(nh−kh+ph+1)×(nw−kw+pw+1), 也就是说，输出的高和宽会分别增加ph和pw 在很多情况下，我们会设置ph=kh−1和pw=kw−1来使输入和输出具有相同的高和宽。这样会方便在构造网络时推测每个层的输出形状。假设这里kh是奇数，我们会在高的两侧分别填充ph/2行。如果kh是偶数，一种可能是在输入的顶端一侧填充⌈ph/2⌉行，而在底端一侧填充⌊ph/2⌋行。在宽的两侧填充同理。 卷积神经网络经常使用奇数高宽的卷积核，如1、3、5和7，所以两端上的填充个数相等。对任意的二维数组X，设它的第i行第j列的元素为X[i,j]。当两端上的填充个数相等，并使输入和输出具有相同的高和宽时，我们就知道输出Y[i,j]是由输入以X[i,j]为中心的窗口同卷积核进行互相关计算得到的 卷积步长一般来说，当高上步幅为sh，宽上步幅为sw时，输出形状为⌊(nh−kh+ph+sh)/sh⌋×⌊(nw−kw+pw+sw)/sw⌋. 如果设置ph=kh−1和pw=kw−1，那么输出形状将简化为⌊(nh+sh−1)/sh⌋×⌊(nw+sw−1)/sw⌋。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是(nh/sh)×(nw/sw)。 过滤器必须处于图像中或者填充之后得图像区域内，采用向下取整的方法。 在机器学习领域一般没有翻转操作，也叫卷积，但是实际上是互相关操作（cross-correlation）。 多通道输入 多通道输出 1*1卷积层1×1卷积层被当作保持高和宽维度形状不变的全连接层使用。于是，我们可以通过调整网络层之间的通道数来控制模型复杂度。 单层卷积网络 特征 “防止过拟合” 一般卷积神经网络的结构： 全连接层输出层中的神经元和输入层中各个输入完全连接（即：输入和输出全连接）。因此，这里的输出层又叫全连接层（fully-connected layer）或稠密层（dense layer）。梯度会指向各点处的函数值降低的方向。更严格的讲，梯度指示的方向是各点处的函数值减少最多的方向。(采用梯度下降法) 全连接层：上一层与该层（full connect）的每个单元相连接 池化层（pooling）它的提出是为了缓解卷积层对位置的过度敏感性。 举例：实际图像里，我们感兴趣的物体不会总出现在固定位置：即使我们连续拍摄同一个物体也极有可能出现像素位置上的偏移。这会导致同一个边缘对应的输出可能出现在卷积输出Y中的不同位置，进而对后面的模式识别造成不便。 池化层每次对输入数据的一个固定形状窗口（又称池化窗口）中的元素计算输出。不同于卷积层里计算输入和核的互相关性，池化层直接计算池化窗口内元素的最大值或者平均值。该运算也分别叫做最大池化或平均池化。 池化层的输出通道数跟输入通道数相同。 因为在处理多通道输入数据时，池化层对每个输入通道分别池化，而不是像卷积层那样将各通道的输入按通道相加。 作用：最大池化层，增强图片亮度；平均池化层，减少冲击失真，模糊，平滑。 最大池化层（常用） 平均池化层 小结 填充可以增加输出的高和宽。这常用来使输出与输入具有相同的高和宽。 步幅可以减小输出的高和宽，例如输出的高和宽仅为输入的高和宽的1/n（n为大于1的整数）。 最大池化和平均池化分别取池化窗口中输入元素的最大值和平均值作为输出。 池化层的一个主要作用是缓解卷积层对位置的过度敏感性。 可以指定池化层的填充和步幅。 池化层的输出通道数跟输入通道数相同。 CNN卷积神经网络就是含卷积层的网络 卷积层尝试解决这两个问题。一方面，卷积层保留输入形状，使图像的像素在高和宽两个方向上的相关性均可能被有效识别；另一方面，卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。 计算神经网络有多少层时，通常只统计具有权重和参数的层。有时将卷积层和池化层计为1层 卷积神经网络的优点：参数共享和稀疏连接来减少参数，用于训练更小的训练集以及防止过拟合。 CNN举例LeNet模型分为卷积层块和全连接层块两个部分 卷积层块里的基本单位是卷积层后接最大池化层：卷积层用来识别图像里的空间模式，如线条和物体局部，之后的最大池化层则用来降低卷积层对位置的敏感性。卷积层块由两个这样的基本单位重复堆叠构成。在卷积层块中，每个卷积层都使用5×5的窗口，并在输出上使用sigmoid激活函数。第一个卷积层输出通道数为6，第二个卷积层输出通道数则增加到16。这是因为第二个卷积层比第一个卷积层的输入的高和宽要小，所以增加输出通道使两个卷积层的参数尺寸类似。卷积层块的两个最大池化层的窗口形状均为2×2，且步幅为2。由于池化窗口与步幅形状相同，池化窗口在输入上每次滑动所覆盖的区域互不重叠。 卷积层块的输出形状为(批量大小, 通道, 高, 宽)。当卷积层块的输出传入全连接层块时，全连接层块会将小批量中每个样本变平（flatten）。也就是说，全连接层的输入形状将变成二维，其中第一维是小批量中的样本，第二维是每个样本变平后的向量表示，且向量长度为通道、高和宽的乘积。全连接层块含3个全连接层。它们的输出个数分别是120、84和10，其中10为输出的类别个数。 其他传统网络AlexNet VGG16 残差网络有助于解决梯度消失和梯度爆炸问题，也能保证深层网络良好的性能 跳远连接：a^[l]数据传送到更远的层 通过1*1卷积来压缩或保持输入层中信道数量 Inception网络不需要人为决定使用那个过滤器或是否需要池化，通过合理构建瓶颈层，可以缩小表示层规模，而又不降低网络性能。 但是直接这样做计算成本太高。 利用1*1卷积层做瓶颈层 参考文献二维卷积层 卷积运行和互相关运算]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Qt creator 封装带图片的可执行文件]]></title>
    <url>%2Farticle%2F1c3f6ab%2F</url>
    <content type="text"><![CDATA[前言经过大半天的折腾，终于将所需要的exe弄出来了。 期间查阅了大量资料，发现大多是简单的一个示例。。 只有一位博主简单说明了一下 带图片资源的操作方式 目的在其他未配置过环境的window电脑上直接运行我们所写程序 环境与软件Qt creator 5.11.1 mingw 32 bit openCV库 kienct 库 Dependency Walker (查询程序所用库的软件) 下载传送门 Enigma Virtual Box(打包整个exe的软件) 下载传送门 初步生成.exe（1）以release形式编译代码 （2）在你项目放置同级目录下会生成对应的buildXXXXX文件-》release文件夹-xxx.exe 自动拷贝所需dll（1）将该exe单独放置在一个文件夹 （2）window搜索框搜索并调用出QT的mingw命令行，使用windeployqt命令 （3）执行以下命令 1234# cd 你所放exe的文件路径cd D:\showEXE # 执行windeployqt xxx(名字).exewindeployqt KinectPeopleDection.exe 添加缺失的dll(动态链接库)此时你可以直接运行该exe文件，但是当你移植到其他电脑时可能会出现库缺失的现象。这可能时因为所使用的第三方库如openCV并没有同步添加进来。 （1）下载Dependency Walker （2）运行并打开xxx.exe （3）检查缺失dll 对比depens所罗列的dll文件与上一步自动拷贝的dll，找出所缺库。这里可以看到openCV kinect的dll根本没有 （4）添加缺失dll full path 可以看到exe所用dll的路径 根据所给路径，将所缺dll复制到所含exe文件夹中 小小坑：感觉应该全部找到了，但是在实际运行中Qt5Test.dll Qt5OpenGL.dll也缺失了，该软件也没有提示… 小小理解：有些系统dll文件引用过来全是64位（如：user32.dll），而mingw编译完是32位的，似乎该软件也提示不兼容。但是我觉得既然在我们的系统上可以运行，就说明没什么太大影响。于是这些dll我选择忽略 图片就位图片文件夹与exe放置在同一目录下 程序修改对应路径12345678910//在mainwindow.h中声明变量，最好是全局变量啦，QString applicationdDirPath; //在mainwindow.cpp//MainWindow::MainWindow(QWidget *parent) 函数里//获取当前程序所在位置 applicationdDirPath = QCoreApplication::applicationDirPath();//程序中的图片路径便是 applicationdDirPath +... QString frameBGRPath = applicationdDirPath + "/img/"+QString::number(collectNumber)+"/bgr"+QString::number(imgCount)+".jpg"; 在此插一个小知识点 QString 字符串拼接类型转换 QString转整型、字符串 eg: 12frameBGRPath.toStdString();frameBGRPath.toInt(); 整型转QString eg: 12int collectNumber = 1;QString bgr = QString::number(collectNumber)； 可选操作更换exe的图标QT程序图标和窗口图标 转icon线上工具 整体打包Enigma Virtual Box下载 QTcreator生成可在其他电脑上跑的exe文件封包过程 参考文献VS2015+QT5.8 程序打包发布详解（包含图片打包，附工具和源码）]]></content>
      <categories>
        <category>Qt creator</category>
      </categories>
      <tags>
        <tag>tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-深层神经网络]]></title>
    <url>%2Farticle%2Fb4ebaec5%2F</url>
    <content type="text"><![CDATA[深度学习的含义深度学习时所指的”深度”是层与层之间更深层次的协调以及随之产生的更加复杂的连接。最终的结果就是你的模型中，有百万级别甚至十亿级别数量的神经元。这就是为什么通过深度神经网络得到的结果。能够极大地优于，早期的手工构建并且手工调试的模型。 作者：大腿君 链接：https://zhuanlan.zhihu.com/p/32225723 来源：知乎 symbol层数大于3 （tips：不计输入层） n^[ l ] : 每层的单元数 a^[ l ] : 每层的激活函数 —- a^[0]为X ， a^[l]为y^ w^[ l ] : 在 a^[ l ]中计算z^[ l ]的权重 深层神经网络的前向和反向传播（ 深度神经网络版梯度下降法正反向传播 ） 前向传播似乎没有比“显示for循环”更好的办法 z^[i] = w^[i] * a^[i-1] + b^[i] 检查bug ：过一遍生成矩阵的维数 z^[i] 的维数 = [ n^[i] ,1 ] = b^[i] 的维数 w^[i] 的维数 = [ n^[i] , n^[i-1] ] 反向传播 中 dw^[i] 的维数等于 w^[i] ​ db^[i] 的维数等于 b^[i] 向量化实现 搭建神经网络块缓存 z 向量化实现 参数与超参数超参数 最终决定了 h(x)中参数的值 目前根据数据或者经验来不断调试超参数的值 方差 与 偏差偏差 bias 👆 训练集错误率 👆 欠拟合 方差 variance 👆 交叉验证集错误率 👆 过拟合 Dropout 正则化设置神经网络中每个节点消除或保留的概率，随机消除节点，从而得到节点更少，规模更小的网络 dropout 会压缩权重并完成一些预防过拟合的外层正则化，可能更适用于不同的输入范围。 keep-prob 代表每一层上保留单元的概率 梯度爆炸与梯度消失举例 ： 如果 激活函数是线性的，与神经网络层数有关的话，呈指数级关系，容易出现梯度爆炸或消失 通过初始化神经网络的权重来部分解决 采用双边误差的方法来检测梯度的数值逼近 梯度检测检查反向传播是否出错 注意事项： mini-batch梯度下降在面对大型的训练集时，将训练集分为几个 大小一致 的集合依次训练 对比随机梯度下降、batch梯度下降、mini-batch梯度下降batch梯度下降：代价函数从某处值开始向最小值收敛，步长大一些。但在训练集巨大时，单次迭代耗时巨大。 随机梯度下降：随机取点，只对一个样本进行梯度下降，大部分向着全局最小值靠近，小部分方向错误，最终在 最小值附近徘徊，但始终不会收敛。但丧失向量化速度优势 修改mini-batch的size，可以在随机梯度下降与batch梯度下降中转换 如何针对实际情况选择size如果训练集较小（m &lt; 2000）, 使用 batch梯度下降 如果训练集较大, 使用 随机梯度下降 一般的mini-batch size :64 ,128, 256 ,512 适应计算机存储方式 指数加权平均以伦敦每日气温为例， 预测曲线的点与前一天、今天的温度有关 偏差修正紫色初始端路线变为绿色曲线 梯度下降消除摆动算法动量梯度下降在某些情况下希望梯度下降在上下方向缓慢一些，在左右方向快速一些 RMSproproot mean square prop算法也用来加速梯度下降，消除梯度下降中的摆动。 Adamadaptive moment esimation 算法是以上两种算法的结合版 一般情况下只需调整alpha的值，其他缺省 学习率衰减加快学习网络效率，随着时间慢慢减少学习率 局部最优问题因为训练了一个大型的神经网络，所以不太可能陷入一个糟糕的局部最优中 平稳区会降低学习速率 超参数调试方法 （1）使用随机值，而非网格，来明确哪些超参数比较重要 （2）由粗到细，放大 效果较好的点集，更密集地随机取点 （3）选择合适的超参数取值范围 （4）β （指数加权平均参数）不适合线性轴均匀取值，再接近某个值时对结果影响巨大。 “熊猫模式” 与 “鱼子酱模式 ” 随着时间，根据数据的变化改变超参数的值； 同时并行多个模型，挑选适应性较好的模型； 正则化网络的激活函数归一化输入（输入层、隐藏层）来加快学习速率 Batch Norm 归一化 发生在 z与a之间，激活函数前 它减弱了前一层参数的作用与后一层参数的作用之间的联系，使得结构更稳定 在测试阶段的 batch norm 需要逐一处理样本 根据训练集估算 μ和σ^2 Softmax 回归将逻辑回归激活函数推广到了多类即：多个输入值而不是0、1了 深度学习框架 TensorFlow 数据集划分在机器学习领域大多是 训练集 交叉验证集 测试集 70% 30% 60% 20% 20% 而在深度学习领域 训练集 交叉验证集 测试集 98% 1% 1% 因为深度学习比较吃数据，而且数据集比机器学习所需要的大多了 误差分析在结果中找寻错误例子，观察错误标记的例子，对每种错误进行分类，看看真阳性或假阴性。统计不同种错误类型的占比，分析改善后其性能上限。 深度学习对随机误差有很强的鲁棒性 训练集和验证集、测试集要来自同一分布 可避免偏差 ：训练集分类错误率 与 人类分类错误率之间的差距 方差 ：训练集分类错误率 与 traning-dev（或测试集）分类错误率之间的差距 数据不匹配：有时可人工制造数据，但要注意过拟合问题 迁移学习试图优化数据相对较少的任务B，寻找一个相关但不同的任务A（数据量较大）训练得到低层次的特征，改善B的学习情况 何时迁移才起作用： 多任务学习单神经网络同时解决多个问题，（比如 对图片标注多个标签） 在数据量较大的情况下，多任务学习性能较好 代替方式：为每一个问题训练单神经网络 何时“多任务学习”才起作用： 端到端学习需要大数据集才能表现其优点]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-浅层神经网络]]></title>
    <url>%2Farticle%2F45fe3766%2F</url>
    <content type="text"><![CDATA[神经网络表示a:activation —— 代表着网络中不同层传给下一层的值 a^(i)_j 第 i 层的第 j 的节点。 计算层数时不计入“输入层”，从隐藏层开始数 以逻辑回归为例 计算过程解释 激活函数种类σ激活函数：除非用在二元分类的输出层，不然绝对不要用 tanh函数 在任何场合都具有优越性 ReLU函数 最常用的默认的激活函数 或者带泄露的ReLU函数 非线性激活函数为了计算出更复杂的函数，必须引入非线性激活函数 只用线性激活函数的情况 （1）machine learning on regression problem(回归问题) ​ （2）输出层 ​ （3）在隐藏层使用是与压缩有关的一些非常特殊的情况 导数 梯度下降前向与反向传播(用到导数) keepdims = true ,是为了防止python出现秩为1的矩阵（n, ） 还有种方式是 不使用keepdims参数，而是显式调用reshape,将np.sum的结果写成矩阵形式 随机初始化对于逻辑回归，可以将权重初始化为0 对于神经网络，各参数数组全部初始化为0 ，再使用梯度下降 将无效 因为隐藏层会计算完全一样的函数，出现对称性，权重一致无意义。 而我们需要不同的隐藏单元去计算不同的函数 解决方案： 随机初始化所有参数( 很小的值 ) 示例：]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习-概论]]></title>
    <url>%2Farticle%2F940b252a%2F</url>
    <content type="text"><![CDATA[神经网络在监督学习中发挥很大作用 规模一直推动深度学习的发展。 大型神经网络、算法创新 以及海量数据 深度学习的 “ 端到端模型（end-to-end learning）” 一个神经网络训练过程 X.shape = (NX,M) Y.shape = (1,M) M为训练集的个数 ，N 为X的特征量个数 逻辑回归w b称为拦截器 α 步长或者学习率 在深度学习过程中，能不显式使用for循环就不用。尽量向量化，并行处理。 tips: 并行是同时处理，并发是同一时间间隔内处理 向量化逻辑回归 python中的广播机制在python中使用numpy进行按位运算的时候，broadcasting,广播机制：如果你有一个m*n的矩阵，让它加减乘除一个1*n的矩阵，它会被复制m次，成为一个m*n的矩阵，然后再逐元素地进行加减乘除操作。同样地对m*1的矩阵成立 python编程注意事项1.不用秩为1的数组，而是用对应的行向量或列向量 2.使用assert()来检查矩阵维数 3.不要害怕使用reshape()]]></content>
      <categories>
        <category>deep learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-照片OCR应用]]></title>
    <url>%2Farticle%2F5e39b6d7%2F</url>
    <content type="text"><![CDATA[Photo Optical Character Recognition 机器学习流水线： 滑动窗口以特定的大小遍历整幅图片，传给分类器 在整幅图片中找到包含字符的矩形框 再次利用滑动窗口分离单个字符 获取大量数据以及人工生成 选取不同字体和背景进行人工合成 对现有数据进行字符拉伸、模糊等操作 1 用学习曲线做一个合理检验，查看更多的数据是否有用 2 需要花多少时间来获得当前10倍的数据量 天花板分析（上限分析）在某几个环节给予正确的结果，查看改善后系统准确度变化。 决定集中精力来优化某些模块]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[常见希腊字母及其读音]]></title>
    <url>%2Farticle%2F3fcc9a30%2F</url>
    <content type="text"><![CDATA[Upper Case Letter Lower Case Letter Greek Letter Name English Equivalent Letter Name Pronounce Α α Alpha a al-fa Β β Beta b be-ta Γ γ Gamma g ga-ma Δ δ Delta d del-ta Ε ε Epsilon e ep-si-lon Ζ ζ Zeta z ze-ta Η η Eta h eh-ta Θ θ Theta th te-ta Ι ι Iota i io-ta Κ κ Kappa k ka-pa Λ λ Lambda l lam-da Μ μ Mu m m-yoo Ν ν Nu n noo Ξ ξ Xi x x-ee Ο ο Omicron o o-mee-c-ron Π π Pi p pa-yee Ρ ρ Rho r row Σ σ Sigma s sig-ma Τ τ Tau t ta-oo Υ υ Upsilon u oo-psi-lon Φ φ Phi ph f-ee Χ χ Chi ch kh-ee Ψ ψ Psi ps p-see Ω ω Omega o o-me-ga 来源： https://blog.csdn.net/justisme/article/details/82874034]]></content>
      <categories>
        <category>tiny knowledge</category>
      </categories>
      <tags>
        <tag>tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-大规模机器学习]]></title>
    <url>%2Farticle%2Fa036031c%2F</url>
    <content type="text"><![CDATA[随机梯度下降打乱数据集顺序，为了收敛更快 比起批量梯度下降 ，随机梯度下降不需要对整个样本求和后来得到梯度项，可以对单个训练样本求梯度，并且在此过程中已经开始优化参数了。在某个区域连续朝着全局最小值的方向徘徊，而不是像批量梯度下降一样直接达到全局最小值 保证收敛以及学习效率α的选择 提前计算 J ，取1000 / 5000样本的均值 红线是α小的曲线 当曲线发散时，可以减小α 当曲线震荡时，可以加大 J 样本数量 假设拥有一个连续的数据不再使用一个固定的数据集，随机梯度下降可以运用于在线学习 Mini-Batch梯度下降（微型梯度下降）在向量化过程中，Mini-Batch梯度下降可能比随机梯度下降速度、效果更优 缺点是需要确定 b 的大小。 改变学习效率 减少映射与数据并行只要是表示对训练集的求和，便可以用MapReduce 适应于多台计算机和多核电脑]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-推荐系统]]></title>
    <url>%2Farticle%2Fe1151c0b%2F</url>
    <content type="text"><![CDATA[以推荐电影为例 symbol 基于内容的推荐算法根据电影内容找到用户特征θ 本质上利用线性回归 其中 Σi:r(i,j) = 1是指 将所有电影 i 的用户评分累加起来 协同过滤（collaborative filtering）及算法不再根据电影内容，而是直接根据用户评分得到电影特征 其中 Σj:r(i,j) = 1是指 将用户 j 评分过所有电影的分值累加起来 没有 X_0 ， θ_0 同时学习几乎所有电影的特征和所有用户参数 ，最终预测所有用户对未评价电影的评分 矢量化：低秩矩阵分解 电影相似度 实施细节均值规范化（防止全未评分用户预测全0） 重新对均值进行协同过滤 编程作业cofiCostFunc.m 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051function [J, grad] = cofiCostFunc(params, Y, R, num_users, num_movies, ... num_features, lambda)%COFICOSTFUNC Collaborative filtering cost function% [J, grad] = COFICOSTFUNC(params, Y, R, num_users, num_movies, ...% num_features, lambda) returns the cost and gradient for the% collaborative filtering problem.%% Unfold the U and W matrices from paramsX = reshape(params(1:num_movies*num_features), num_movies, num_features);Theta = reshape(params(num_movies*num_features+1:end), ... num_users, num_features); % You need to return the following values correctlyJ = 0;X_grad = zeros(size(X));Theta_grad = zeros(size(Theta));% ====================== YOUR CODE HERE ======================% Instructions: Compute the cost function and gradient for collaborative% filtering. Concretely, you should first implement the cost% function (without regularization) and make sure it is% matches our costs. After that, you should implement the % gradient and use the checkCostFunction routine to check% that the gradient is correct. Finally, you should implement% regularization.%% Notes: X - num_movies x num_features matrix of movie features% Theta - num_users x num_features matrix of user features% Y - num_movies x num_users matrix of user ratings of movies% R - num_movies x num_users matrix, where R(i, j) = 1 if the % i-th movie was rated by the j-th user%% You should set the following variables correctly:%% X_grad - num_movies x num_features matrix, containing the % partial derivatives w.r.t. to each element of X% Theta_grad - num_users x num_features matrix, containing the % partial derivatives w.r.t. to each element of Theta%J = sum(sum(((X*Theta&apos; - Y).^2) .* R)) / 2 + lambda/2*(sum(sum(Theta.^2)) + sum(sum(X.^2)));X_grad = ((X*Theta&apos; - Y).* R) * Theta + lambda * X;Theta_grad = ((X*Theta&apos; - Y).* R)&apos; * X+ lambda * Theta;% =============================================================grad = [X_grad(:); Theta_grad(:)];end]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-异常检测以及高斯分布]]></title>
    <url>%2Farticle%2F36321a51%2F</url>
    <content type="text"><![CDATA[异常检测给定样本，判断待测数据是否异常。 高斯分布也称 正态分布，面积积分=1 高斯分布的参数极大似然估计 训练集中各特征量可以不是相互独立的 Π：对一系列数的乘积 算法流程P(x)是对特征的建模 评估异常检测系统用60%带标签的数据计算p(x),20%交叉验证集 20%测试集输出结果 然后计算准确率与召回率 ，F1从而评估异常检测系统 有点类似监督学习 异常检测 vs 监督学习1 是异常的 0 是正常的 可能之后的特征跟现在不一致 使用（1）转换非高斯分布特征 （2）误差分析，或添加新特征 多高斯分布 Σ 协方差矩阵 可体现特征间的相关性 μ 均值 集中点（概率较大的位置） 运用单高斯与多高斯之间的联系 多高斯分布能够自然地捕捉特征之间的关系 而单高斯分布计算量小，适应大规模计算 所以在m&gt;n时用多高斯分布 tips: Σ 如果是奇异矩阵即不可逆，可能有两种情况： ​ （1）没有满足m&gt;n的条件 ​ （2）存在冗余特征（高度线性相关的特征、不包含额外信息） 编程作业estimateGaussian.m 12345678910111213141516171819202122232425262728293031function [mu sigma2] = estimateGaussian(X)%ESTIMATEGAUSSIAN This function estimates the parameters of a %Gaussian distribution using the data in X% [mu sigma2] = estimateGaussian(X), % The input X is the dataset with each n-dimensional data point in one row% The output is an n-dimensional vector mu, the mean of the data set% and the variances sigma^2, an n x 1 vector% % Useful variables[m, n] = size(X);% You should return these values correctlymu = zeros(n, 1);sigma2 = zeros(n, 1);% ====================== YOUR CODE HERE ======================% Instructions: Compute the mean of the data and the variances% In particular, mu(i) should contain the mean of% the data for the i-th feature and sigma2(i)% should contain variance of the i-th feature.%mu = sum(X) / m;sigma2 = sum((X-mu).^2) / m;% =============================================================end selectThreshold.m 123456789101112131415161718192021222324252627282930313233343536373839404142function [bestEpsilon bestF1] = selectThreshold(yval, pval)%SELECTTHRESHOLD Find the best threshold (epsilon) to use for selecting%outliers% [bestEpsilon bestF1] = SELECTTHRESHOLD(yval, pval) finds the best% threshold to use for selecting outliers based on the results from a% validation set (pval) and the ground truth (yval).%bestEpsilon = 0;bestF1 = 0;F1 = 0;stepsize = (max(pval) - min(pval)) / 1000;for epsilon = min(pval):stepsize:max(pval) % ====================== YOUR CODE HERE ====================== % Instructions: Compute the F1 score of choosing epsilon as the % threshold and place the value in F1. The code at the % end of the loop will compare the F1 score for this % choice of epsilon and set it to be the best epsilon if % it is better than the current choice of epsilon. % % Note: You can use predictions = (pval &lt; epsilon) to get a binary vector % of 0&apos;s and 1&apos;s of the outlier predictions predictions = (pval &lt; epsilon); tp = sum( (predictions == 1) &amp; (yval == 1) ); fp = sum( (predictions == 1) &amp; (yval == 0) ); fn = sum( (predictions == 0) &amp; (yval == 1) );; prec = tp / (tp+fp); rec = tp / (tp+fn); F1 = 2*prec*rec / (prec+rec); % ============================================================= if F1 &gt; bestF1 bestF1 = F1; bestEpsilon = epsilon; endendend]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-降维]]></title>
    <url>%2Farticle%2F8226faf3%2F</url>
    <content type="text"><![CDATA[降维运用于数据压缩,减少冗余 运用于可视化,抓住关键数据，绘制2D、3D图像 # 高维特征的问题：• 存在大量冗余的特征，降低了机器学习的性能• 数据可视化问题• 数据处理的维度灾难降维的目的：• 发掘高维数据的内在维度，得到更紧凑(低维)的数据表达 内在维度内在维度：表征数据变化的自由变量的个数 线性降维：关于内在维度的线性子空间的降维问题非线性降维：非线性子空间(流形) 线性降维将d维的原始数据线性投影到𝑑′维子空间• 通常 𝑑′ ≪ 𝑑𝑑′维子空间(投影矩阵)的选择取决于任务的要求 有监督的降维降维要求：• 降维后的不同类别数据之间的差别最大化LDA算法(Linear Discriminative Analysis)：• 最大化 类别间散度(scatter)与类别内散度的比值 无监督的降维特点：数据没有类别标签要求： 降维后保留尽可能多的原始数据的信息 PAC（主成分分析 ）principal components analysis 不是线性回归 PAC 是找到低维子空间（正交子空间 ）来对数据进行投影（对所有 x一 视同仁，没有 y）以便最小化投影误差的平方，也就是min(点与投影后的点之间的距离) 老师版教学过程最近重构性数据样本到投影点的距离最近 最大可分性数据样本的投影点之间尽可能分开 PCA的优化方法 算法过程 吴恩达版教学过程数据预处理（1）特征放缩 （2）均值标准化 计算过程Σ（大写的σ，不是求和符号） （奇异值分解）svd( )数值上更稳定 再进行协方差计算时 结果等于 eig( ) z是降维后的U ，整个过程就是最小的平方投影误差 （数学证明复杂） 主成分数量的选择 压缩重现得到压缩前数据的估计值 应用建议（1）给监督算法加速 训练集利用PCA 建立 X 到 z 的映射，分类精度不会受影响 （2）错误用法 ：防止过拟合 ​ 因为PCA丢掉了一些关键信息 （3）错误用法 ：设计机器学习系统时 不比较 使用PCA和不使用PCA的情况 线性降维的不足• 线性降维基于欧式距离• 欧式距离无法应用于非线性子空间(流形) 非线性降维“流形”是在局部与欧氏空间同胚（等价）的空间，形象的说法：“一块可弯曲的橡皮擦”。换言之，它在局部具有欧氏空间的性质，在局部才能用欧氏距离来进行距离计算。 “流形学习”是一类对分布在流形上的数据样本进行非线性降维的方法。 测地距离(Geodesic Distance)• 测地距离衡量了流形(弯曲表面)上两点之间的最短距离• 测地距离可以由两点之间的邻近点构成的最短路径来近似 等度量映射(ISOMAP)目标：降维后的样本保持原样本空间的测地距离方法：基于测地距离的“多维缩放” (MDS) 核心步骤： 计算任意两样本之间的测地距离(最短路径算法，例如： Djikstar算法) 以所有样本间的测地距离矩阵作为输入，调用MDS算法 多维缩放(MDS: Multi-dimensional Scaling)假定有m个样本,在原始空间中的距离矩阵为D ,其第i行j列的元素dist_i_j 为样本 X_i到 _j的距离。 目标： 在低维空间中保持原始数据样本之间的欧式距离 MDS的求解方法]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-Kmeans聚类算法]]></title>
    <url>%2Farticle%2F63be982%2F</url>
    <content type="text"><![CDATA[在无监督学习中，数据都是不带任何标签的 通过算法发现数据中隐藏的结构从而找到分类簇或者其他形式 k-means聚类算法 随机生成K个聚类中心 迭代以下操作，直至聚类中心不在移动，训练集的标签不再改变 簇分配 移动聚类中心 不易分离簇 优化目标J 也叫失真函数 ，畸变函数 随机初始化使得算法避免局部最优解 随机初始化状态不同，导致结果也不同，可能得到不好的局部最优。 使用多次随机初始化找到使得 J 最小的聚类中心 选取聚类数量（1）手动选择 ​ 可视化数据，观察数据分离情况 （2）“肘部法则” ​ 分别K = 1，2，3，4，5… 绘制 J 的曲线，选择拐点。但是有时很模糊~ （3）接下来数据的分类情况做一个评估 编程作业pca.m 1234567891011121314151617181920212223242526function [U, S] = pca(X)%PCA Run principal component analysis on the dataset X% [U, S, X] = pca(X) computes eigenvectors of the covariance matrix of X% Returns the eigenvectors U, the eigenvalues (on diagonal) in S%% Useful values[m, n] = size(X);% You need to return the following variables correctly.U = zeros(n);S = zeros(n);% ====================== YOUR CODE HERE ======================% Instructions: You should first compute the covariance matrix. Then, you% should use the &quot;svd&quot; function to compute the eigenvectors% and eigenvalues of the covariance matrix. %% Note: When computing the covariance matrix, remember to divide by m (the% number of examples).%sigma = X&apos; * X / m;[U,S,v] = svd(sigma);% =========================================================================end projectData.m 123456789101112131415161718192021222324252627function Z = projectData(X, U, K)%PROJECTDATA Computes the reduced data representation when projecting only %on to the top k eigenvectors% Z = projectData(X, U, K) computes the projection of % the normalized inputs X into the reduced dimensional space spanned by% the first K columns of U. It returns the projected examples in Z.%% You need to return the following variables correctly.Z = zeros(size(X, 1), K);% ====================== YOUR CODE HERE ======================% Instructions: Compute the projection of the data using only the top K % eigenvectors in U (first K columns). % For the i-th example X(i,:), the projection on to the k-th % eigenvector is given as follows:% x = X(i, :)&apos;;% projection_k = x&apos; * U(:, k);%U_redeuce = U(:,1:K);Z = X * U_redeuce;% =============================================================end recoverData.m 12345678910111213141516171819202122232425262728function X_rec = recoverData(Z, U, K)%RECOVERDATA Recovers an approximation of the original data when using the %projected data% X_rec = RECOVERDATA(Z, U, K) recovers an approximation the % original data that has been reduced to K dimensions. It returns the% approximate reconstruction in X_rec.%% You need to return the following variables correctly.X_rec = zeros(size(Z, 1), size(U, 1));% ====================== YOUR CODE HERE ======================% Instructions: Compute the approximation of the data by projecting back% onto the original space using the top K eigenvectors in U.%% For the i-th example Z(i,:), the (approximate)% recovered data for dimension j is given as follows:% v = Z(i, :)&apos;;% recovered_j = v&apos; * U(j, 1:K)&apos;;%% Notice that U(j, 1:K) is a row vector.% X_rec = Z * U(:,1:K)&apos;;% =============================================================end findClosestCentroids.m 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748function idx = findClosestCentroids(X, centroids)%FINDCLOSESTCENTROIDS computes the centroid memberships for every example% idx = FINDCLOSESTCENTROIDS (X, centroids) returns the closest centroids% in idx for a dataset X where each row is a single example. idx = m x 1 % vector of centroid assignments (i.e. each entry in range [1..K])%% Set KK = size(centroids, 1);% You need to return the following variables correctly.idx = zeros(size(X,1), 1);% ====================== YOUR CODE HERE ======================% Instructions: Go over every example, find its closest centroid, and store% the index inside idx at the appropriate location.% Concretely, idx(i) should contain the index of the centroid% closest to example i. Hence, it should be a value in the % range 1..K%% Note: You can use a for-loop over the examples to compute this.%for i = 1:size(X,1) min_dis = Inf; for j = 1:K a = X(i,:) - centroids(j,:); dis = sum(a.^2); if min_dis &gt; dis idx(i) = j; min_dis = dis; endif endforendfor%else methods%for i = 1 : size(X, 1)% for j = 1 : K% dis(j) = sum((centroids(j, :) - X(i, :)) .^ 2, 2);% end% [t, idx(i)] = min(dis); %t存储的最小值， idx存储的最小值的索引%end % =============================================================end computeCentroids.m 12345678910111213141516171819202122232425262728293031323334353637383940414243444546function centroids = computeCentroids(X, idx, K)%COMPUTECENTROIDS returns the new centroids by computing the means of the %data points assigned to each centroid.% centroids = COMPUTECENTROIDS(X, idx, K) returns the new centroids by % computing the means of the data points assigned to each centroid. It is% given a dataset X where each row is a single data point, a vector% idx of centroid assignments (i.e. each entry in range [1..K]) for each% example, and K, the number of centroids. You should return a matrix% centroids, where each row of centroids is the mean of the data points% assigned to it.%% Useful variables[m n] = size(X); %300,2% You need to return the following variables correctly.centroids = zeros(K, n); %3,2% ====================== YOUR CODE HERE ======================% Instructions: Go over every centroid and compute mean of all points that% belong to it. Concretely, the row vector centroids(i, :)% should contain the mean of the data points assigned to% centroid i.%% Note: You can use a for-loop over the centroids to compute this.%for i = 1:m for j= 1:n centroids(idx(i),j) = centroids(idx(i),j) + X(i,j); endforendforfor i = 1 : K id = (idx ==i); centroids(i,:) = centroids(i,:)./sum(id);endfor%else methods（vectorized）%for i = 1 : K% centroids(i, :) = (X&apos; * (idx == i)) / sum(idx == i); %(idx ==i)目的是将不是i值的X中对应数据变为0.% end% =============================================================end kMeansInitCentroids.m 1234567891011121314151617181920212223function centroids = kMeansInitCentroids(X, K)%KMEANSINITCENTROIDS This function initializes K centroids that are to be %used in K-Means on the dataset X% centroids = KMEANSINITCENTROIDS(X, K) returns K initial centroids to be% used with the K-Means on the dataset X%% You should return this values correctlycentroids = zeros(K, size(X, 2));% ====================== YOUR CODE HERE ======================% Instructions: You should set centroids to randomly chosen examples from% the dataset X%% Randomly reorder the indices of examplesrandidx = randperm(size(X, 1));% Take the first K examples as centroidscentroids = X(randidx(1:K), :);% =============================================================end]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hexoBlog迁移与GitHub源码备份]]></title>
    <url>%2Farticle%2F67d32740%2F</url>
    <content type="text"><![CDATA[新电脑环境配置安装git/node.js1.安装git ​ https://gitforwindows.org/ ​ 一路next,安装完后 git --version查看版本（可在 git bash里面输入） 2.安装node.js 与 npm ​ https://nodejs.org/en/ (说明：LTS为长期支持版，Current为当前最新版) ​ 一路next，不过在Custom Setup这一步记得选 Add to PATH ​ 输入 node -v 和npm -v 可以查看版本（可在 git bash里面输入） ps：若是出现这种情况 “安装完成node.js 在git bash中输入node -v查询版本时报错：bash: node: command notfound，但在cmd窗口中输入node -v 能查询出版本“，重启一下系统就好了。 切换镜像(加快下载速度)：npm config set registry http://registry.npm.taobao.org/原来的地址：npm config set registry https://registry.npmjs.org/ 安装typora很好用的makeDown软件 https://www.typora.io/ 方式一：备份原机上的博客源码只需备份如下文件即可。 _config.yml .package.json .gitignore(如果有的话) themes/ source/ scffolds/ 新建 blog 文件夹存放博客 （1）在该目录下 右键选择 git bash here （2）安装hexo ​ npm install hexo-cli -g （3）存放 ​ 新博客 hexo init ​ 老博客 直接复制过来之前备份的文件 方式二：拷贝GitHub上的博客源码12#拷贝对应博客仓库远程地址，在此操作前先链接好GitHubgit clone git@github.com:Pabebezz/Pabebezz.github.io.git hexo常用插件安装（见上文“hexo环境搭建”） 拷贝成功 链接GitHub指令版 1234567891011121314151617# a.连接GitHubgit config --global user.name "yourUsername"git config --global user.email "yourUser.com"#使用SSH，保证密码的安全性，省事# b.检查本机是否存在 SSHls -al ~/.ssh# c.删除 C:\Users\***\.ssh 下所有文件 # d.生成SSHssh-keygen -t rsa -C "zezuwang@qq.com"# e.添加SSH到github网页版个人设置那块# f.检测SSH是否配置成功ssh -T git@github.com 123本来记录git config --global user.name "Pabebezz"git config --global user.email "zezuwang@qq.com" 图形界面版 创建本地工作库 生成SSH 放置SSH 遇到的问题 1234567$ ssh -T git@github.comgit@github.com's password:Permission denied, please try again.git@github.com's password:Permission denied, please try again.git@github.com's password:git@github.com: Permission denied (publickey,gssapi-keyex,gssapi-with-mic,password). 解决： 12345678Pabebe@DESKTOP-SJKD659 MINGW64 /d/hexoBlog (master)$ eval "$(ssh-agent -s)"Agent pid 252Pabebe@DESKTOP-SJKD659 MINGW64 /d/hexoBlog (master)$ ssh-addEnter passphrase for /c/Users/Pabebe/.ssh/id_rsa:(输入当时图形化创建时输入的口令)Identity added: /c/Users/Pabebe/.ssh/id_rsa (Pabebe@DESKTOP-SJKD659) 如果仍然出现要输入密码的情况，重新生成ssh，再添加公钥。若这操作不行，可能是校园网屏蔽了GitHub，只能等待了。。 成功则出现 hexo环境搭建1234567891011121314151617181920212223#安装便于自动部署到Github上的插件npm install hexo-deployer-git --save#安装atom生成插件，便于感兴趣的小伙伴们订阅npm install hexo-generator-feed --save#安装Sitemap文件生成插件$ npm install hexo-generator-sitemap --save#安装百度Sitemap文件生成插件，因为普通的Sitemap格式不符合百度的要求npm install hexo-generator-baidu-sitemap --save!-- 下面选做 --#安装博客首页生成插件npm install hexo-generator-index --save#安装tag生成插件npm install hexo-generator-tag --save#安装category生成插件npm install hexo-generator-category --save#安装归档生成插件npm install hexo-generator-archive --save#置顶文章npm uninstall hexo-generator-index --save hexo 常用命令12345678#清除生成博客界面的文件hexo clean#生成博客界面hexo g#本地查看hexo s#发布至GitHubhexo d 博客源码备份备份 在GitHub博客仓库新建分支hexo（自定义）； 在该仓库中导航栏 setting-&gt;Branches-&gt;Default branch 设置hexo为默认分支； git clone git@github.com:Pabebezz/yourUsername.github.io.git 将博客仓库clone至本地，将之前备份的文件拷贝至yourUsername.github.io 这一文件夹中。 （可选操作：删除 yourUsername.github.io 内部的文件 因我之前发布了博客，所以包含了博客发布界面） 将themes/next/(我用的是NexT主题)中的.git/删除，否则无法将主题文件夹push； 在Username.github.io文件夹git bash执行(查看分支是不是显示为Hexo) npm install hexo hexo init（新博客写上，老博客忽略该条命令） npm install npm install hexo-deployer-git 中间可能会出现警告，更新对应插件就好了（例如我的） npm upgrade core-js@3 npm upgrade ejs@2.5.5 npm upgrade ajv@^5.0.0 为加快上传速度，可将部分文件不上传至github中。 可选操作：生成“.gitignore”文件 touch .gitignore 在”.gitignore” 文件里输入你要忽略的文件夹及其文件就可以了,例如： db.json debug.log node_modules/ public/ .deploy_git/ 可以直接把.git文件放到原来的hexo文件夹中，这样就变成原有文件夹作为本地仓而不是Username.github.io文件夹。 远程仓库master分支保存静态网页，hexo分支保存源文件。 执行以下命令来提交Hexo网站源文件； git add. #添加目录下所有文件 git commit -m “提交文件” #提交至缓存区，并备注更新说明 git push origin hexo #推送更新至github远程仓库 执行hexo g -d 生成静态网页部署到github上。 日常更新同步的话先执行 git pull #拷贝远程仓库 在本地对博客修改（包括修改主题样式、发布新文章等）后,依次执行下列命令： git add. git commit -m “博客更新” git push origin hexo hexo g -d git命令环境文件推送到hexo分支，然后再hexo发布网站并推送静态文件到master分支。 多终端同步另一台电脑同样搭建好相关环境，安装好插件 1.下拉远程仓库文件 git init #创建全新本地仓库或者将已存在项目加入版本管理git remote add origin &lt;server&gt;git fetch --allgit reset --hard origin/master 2.日常更新 批处理自动部署同步 12345678910@echo offset branchName = hexoset blogPath = F:\hexoBlog::跳转至博客根目录F:cd %blogPath%::下拉远程仓库git pull::停留dos窗口pause 更新 12345678910111213141516171819::不显示所有命令@echo off::set branchName = hexo::set blogPath = D:\hexoBlog ::跳转至博客根目录::D:::cd %blogPath%::输出语句::echo "start commit"::源码提交至GitHub_privategit add .git commit -m "upgrade blog"git push origin master -f::发布博客至GitHub_public::界面并停留dos窗口::hexo clean &amp;&amp; hexo g -d &amp;&amp; pausehexo clean &amp;&amp; hexo g -d 发布完关机 123456789101112131415161718::不显示所有命令@echo off::set branchName = hexo::set blogPath = D:\hexoBlog ::跳转至博客根目录::D:::cd %blogPath%::输出语句::echo "start commit"::源码提交至GitHub_privategit add .git commit -m "upgrade blog"git push origin master -f::发布博客至GitHub_public::运行完自动关机hexo clean &amp;&amp; hexo g -d &amp;&amp; shutdown -s -t 00 hexo环境安装 12345678910111213141516171819202122::不显示所有命令::@echo off::set blogPath = D:\hexoBlog ::跳转至博客根目录::D:::cd %blogPath%npm install hexo --savenpm install hexo-server --savenpm install hexo-deployer-git --savenpm install hexo-generator-feed --savenpm install hexo-generator-sitemap --savenpm install hexo-generator-baidu-sitemap --savenpm install hexo-generator-index --savenpm install hexo-generator-tag --savenpm install hexo-generator-category --savenpm install hexo-generator-archive --savenpm install hexo-abbrlink --savenpm uninstall hexo-generator-index --savenpm install hexo-generator-index-pin-top --savepause 博客源码同步到GitHub and Gitee 更改.git下的config文件的remote为下面的内容，有多少个远程仓库地址就加多少个url即可 1234567#查看远程地址git remote# 更新git push origin hexo# 出现无法push的情况，强制更新git push -f#如果上述解决方式不管用也可以输入：git pull --rebase origin master 之后再进行git push 即可。 注意：默认只能从config中的第一个url内的仓库pull代码。 博客更新到GitHub and Gitee更改博客根文件下的_config.yml文件的deploy下面repo内容，有多少个远程仓库地址就添加即可 tips:码云（gitee）上备份/做博客如何创建一个首页访问地址不带二级目录的 pages，如ipvb.gitee.io？ 答：如果你想你的 pages 首页访问地址不带二级目录，如ipvb.gitee.io，你需要建立一个与自己个性地址同名的仓库，如 https://gitee.com/ipvb 这个用户，想要创建一个自己的站点，但不想以子目录的方式访问，想以ipvb.gitee.io直接访问，那么他就可以创建一个名字为ipvb的仓库 https://gitee.com/ipvb/ipvb 部署完成后，就可以以 https://ipvb.gitee.io 进行访问了。 here —– 码云Pages问题 官方解决 参考文章使用Hexo搭建博客，备份至GitHub过程（基于网上资料的实践操作） hexo博客搭建及github备份全记录 hexo博客同步管理及迁移 码云 + Hexo 搭建个人博客 git 命令详解 使用bat脚本部署hexo到coding和github (这篇是同时拷贝coding 和 github) git技巧-项目同时推送至github和gitee]]></content>
      <categories>
        <category>gitHub</category>
      </categories>
      <tags>
        <tag>tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-支持向量机]]></title>
    <url>%2Farticle%2F36b4f21c%2F</url>
    <content type="text"><![CDATA[机器学习最具有理论计算的部分，有点难the Support Vector Machine (SVM)监督学习算法 也称大间距分类器，因其设置了一个安全区域 在某些情况下比逻辑回归更适合构造非线性复杂分类器 优化目标： 最小化假设函数，一样是找到最优的θ 决策边界以最大的间距与训练集分开 C过大 对异常点敏感 会导致训练集与边界线之间的间距变小。 向量内积的性质 其中 P 是 投影，有符号的实数 || u|| 向量u的欧几里长度 就是 我们所说的“向量长度” 运用于代价函数的计算 θ的方向与决策边界90°正交 θ_0代表决策边界与Y轴的截距 为什么支持向量机会找到最大间距呢？ 如图它会使得P最大，从而min||θ||，P就是训练集在θ上面的投影， 在这样的过程中便找到了最大间距~ 核函数利用核函数K根据特征 X 和标记点 l 定义新的特征量 f 本质上 f是X与l的相似度 高斯核函数 核函数与标记点共同定义复杂非线性决策边界 越大 特征量x从标记点l离开时变化速度越大 同样不需要正则化θ_0 θT M θ 更多的是为了计算效率，稍微改变了正则化的结果 参数的选择C过大 相当于逻辑回归中 λ过小，不正则化 ，易出现过拟合；反之易出现欠拟合。 σ^2 过大 f变化过于平滑，易出现欠拟合；反之f变化剧烈易出现过拟合。 实现步骤n表示特征量的数量，m表示训练集的数量 可以在n X多 ，m少的情况 不要核函数，得到一条线性决策边界 可以在m X多 ，n少的情况 选择高斯核函数，得到一条复杂非线性决策边界 在进行高斯核函数前不用特征归一化 多类别分类类似于 逻辑回归中的“一对多” ，选择概率最大的 z tips: 逻辑回归与不带核函数的支持向量机十分类似，效果略有不同。 神经网络适合大部分问题，但速度可能被限制 编程作业gaussianKernel.m 1234567891011121314151617181920212223function sim = gaussianKernel(x1, x2, sigma)%RBFKERNEL returns a radial basis function kernel between x1 and x2% sim = gaussianKernel(x1, x2) returns a gaussian kernel between x1 and x2% and returns the value in sim% Ensure that x1 and x2 are column vectorsx1 = x1(:); x2 = x2(:);% You need to return the following variables correctly.sim = 0;% ====================== YOUR CODE HERE ======================% Instructions: Fill in this function to return the similarity between x1% and x2 computed using a Gaussian kernel with bandwidth% sigma%%sim = exp(- sum((x1-x2).^2)/2/(sigma^2));% ============================================================= end dataset3Params.m 1234567891011121314151617181920212223242526272829303132333435363738394041424344function [C, sigma] = dataset3Params(X, y, Xval, yval)%DATASET3PARAMS returns your choice of C and sigma for Part 3 of the exercise%where you select the optimal (C, sigma) learning parameters to use for SVM%with RBF kernel% [C, sigma] = DATASET3PARAMS(X, y, Xval, yval) returns your choice of C and % sigma. You should complete this function to return the optimal C and % sigma based on a cross-validation set.%% You need to return the following variables correctly.C = 1;sigma = 0.3;% ====================== YOUR CODE HERE ======================% Instructions: Fill in this function to return the optimal C and sigma% learning parameters found using the cross validation set.% You can use svmPredict to predict the labels on the cross% validation set. For example, % predictions = svmPredict(model, Xval);% will return the predictions on the cross validation set.%% Note: You can compute the prediction error using % mean(double(predictions ~= yval))%C_multi = [0.01,0.03,0.1,0.3,1,3,10,30];sigma_multi = [0.01,0.03,0.1,0.3,1,3,10,30];min_err = Inf;for i = 1:length(C_multi) for j = 1:length(sigma_multi) model= svmTrain(X, y, C_multi(i), @(x1, x2) gaussianKernel(x1, x2, sigma_multi(j))); predictions = svmPredict(model, Xval); err = mean(double(predictions ~= yval)); if min_err &gt; err C = C_multi(i); sigma = sigma_multi(j); min_err = err; endif endforendfor% =========================================================================end processEmail.m 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122function word_indices = processEmail(email_contents)%PROCESSEMAIL preprocesses a the body of an email and%returns a list of word_indices % word_indices = PROCESSEMAIL(email_contents) preprocesses % the body of an email and returns a list of indices of the % words contained in the email. %% Load VocabularyvocabList = getVocabList();% Init return valueword_indices = [];% ========================== Preprocess Email ===========================% Find the Headers ( \n\n and remove )% Uncomment the following lines if you are working with raw emails with the% full headers% hdrstart = strfind(email_contents, ([char(10) char(10)]));% email_contents = email_contents(hdrstart(1):end);% Lower caseemail_contents = lower(email_contents);% Strip all HTML% Looks for any expression that starts with &lt; and ends with &gt; and replace% and does not have any &lt; or &gt; in the tag it with a spaceemail_contents = regexprep(email_contents, &apos;&lt;[^&lt;&gt;]+&gt;&apos;, &apos; &apos;);% Handle Numbers% Look for one or more characters between 0-9email_contents = regexprep(email_contents, &apos;[0-9]+&apos;, &apos;number&apos;);% Handle URLS% Look for strings starting with http:// or https://email_contents = regexprep(email_contents, ... &apos;(http|https)://[^\s]*&apos;, &apos;httpaddr&apos;);% Handle Email Addresses% Look for strings with @ in the middleemail_contents = regexprep(email_contents, &apos;[^\s]+@[^\s]+&apos;, &apos;emailaddr&apos;);% Handle $ signemail_contents = regexprep(email_contents, &apos;[$]+&apos;, &apos;dollar&apos;);% ========================== Tokenize Email ===========================% Output the email to screen as wellfprintf(&apos;\n==== Processed Email ====\n\n&apos;);% Process filel = 0;while ~isempty(email_contents) % Tokenize and also get rid of any punctuation [str, email_contents] = ... strtok(email_contents, ... [&apos; @$/#.-:&amp;*+=[]?!()&#123;&#125;,&apos;&apos;&quot;&gt;_&lt;;%&apos; char(10) char(13)]); % Remove any non alphanumeric characters str = regexprep(str, &apos;[^a-zA-Z0-9]&apos;, &apos;&apos;); % Stem the word % (the porterStemmer sometimes has issues, so we use a try catch block) try str = porterStemmer(strtrim(str)); catch str = &apos;&apos;; continue; end; % Skip the word if it is too short if length(str) &lt; 1 continue; end % Look up the word in the dictionary and add to word_indices if % found % ====================== YOUR CODE HERE ====================== % Instructions: Fill in this function to add the index of str to % word_indices if it is in the vocabulary. At this point % of the code, you have a stemmed word from the email in % the variable str. You should look up str in the % vocabulary list (vocabList). If a match exists, you % should add the index of the word to the word_indices % vector. Concretely, if str = &apos;action&apos;, then you should % look up the vocabulary list to find where in vocabList % &apos;action&apos; appears. For example, if vocabList&#123;18&#125; = % &apos;action&apos;, then, you should add 18 to the word_indices % vector (e.g., word_indices = [word_indices ; 18]; ). % % Note: vocabList&#123;idx&#125; returns a the word with index idx in the % vocabulary list. % % Note: You can use strcmp(str1, str2) to compare two strings (str1 and % str2). It will return 1 only if the two strings are equivalent. % for i = 1:length(vocabList) if strcmp(str,vocabList(i)) == 1 word_indices = [word_indices ; i]; endif endfor % ============================================================= % Print to screen, ensuring that the output lines are not too long if (l + length(str) + 1) &gt; 78 fprintf(&apos;\n&apos;); l = 0; end fprintf(&apos;%s &apos;, str); l = l + length(str) + 1;end% Print footerfprintf(&apos;\n\n=========================\n&apos;);end emailFeatures.m 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859function x = emailFeatures(word_indices)%EMAILFEATURES takes in a word_indices vector and produces a feature vector%from the word indices% x = EMAILFEATURES(word_indices) takes in a word_indices vector and % produces a feature vector from the word indices. % Total number of words in the dictionaryn = 1899;% You need to return the following variables correctly.x = zeros(n, 1);% ====================== YOUR CODE HERE ======================% Instructions: Fill in this function to return a feature vector for the% given email (word_indices). To help make it easier to % process the emails, we have have already pre-processed each% email and converted each word in the email into an index in% a fixed dictionary (of 1899 words). The variable% word_indices contains the list of indices of the words% which occur in one email.% % Concretely, if an email has the text:%% The quick brown fox jumped over the lazy dog.%% Then, the word_indices vector for this text might look % like:% % 60 100 33 44 10 53 60 58 5%% where, we have mapped each word onto a number, for example:%% the -- 60% quick -- 100% ...%% (note: the above numbers are just an example and are not the% actual mappings).%% Your task is take one such word_indices vector and construct% a binary feature vector that indicates whether a particular% word occurs in the email. That is, x(i) = 1 when word i% is present in the email. Concretely, if the word &apos;the&apos; (say,% index 60) appears in the email, then x(60) = 1. The feature% vector should look like:%% x = [ 0 0 0 0 1 0 0 0 ... 0 0 0 0 1 ... 0 0 0 1 0 ..];%%for i=1:length(word_indices) x(word_indices(i)) = 1; endfor% ========================================================================= end]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-评价方法]]></title>
    <url>%2Farticle%2Ff46160db%2F</url>
    <content type="text"><![CDATA[模型选择60% 训练集 20%验证集 20%测试集 训练集计算误差，利用交叉验证集选择维数，泛化能力 偏差与方差偏差（与训练集数据拟合程度），方差（与验证集数据拟合程度） 项数少 ：高偏差，高方差 项数多：低偏差，高方差 正则化：防止过拟合 lambda 小 ：低偏差，高方差 lambda大：高偏差，高方差 学习曲线当模型高偏差时，加大训练集无用。 当模型高方差时，加大训练集可能有用。 大型神经网络比小型神经网络性能要好 设计复杂学习系统 从简单的算法开始 绘制学习曲线 误差分析（验证集） 误差评估偏斜类 skewed class (其中一类占比巨大。不对称性分类) 查准率 = 真的 / 预测真的 召回率 = 真的 / （实际是真的，预测真或假） 平衡查准率与召回率F_1的值 综合： 机器学习数据大量数据 适合 大量参数的模型 编程作业linearRegCostFunction.m 1234567891011121314151617181920212223242526272829303132function [J, grad] = linearRegCostFunction(X, y, theta, lambda)%LINEARREGCOSTFUNCTION Compute cost and gradient for regularized linear %regression with multiple variables% [J, grad] = LINEARREGCOSTFUNCTION(X, y, theta, lambda) computes the % cost of using theta as the parameter for linear regression to fit the % data points in X and y. Returns the cost in J and the gradient in grad% Initialize some useful valuesm = length(y); % number of training examples% You need to return the following variables correctly J = 0;grad = zeros(size(theta));% ====================== YOUR CODE HERE ======================% Instructions: Compute the cost and gradient of regularized linear % regression for a particular choice of theta.%% You should set J to the cost and grad to the gradient.%h = X * theta;J = sum((h-y).^2)/2/m+ lambda/2/m*sum(theta(2:end).^2);grad = X&apos; * (h-y)/m + lambda/m*theta;grad(1) = grad(1) - lambda/m*theta(1);% =========================================================================grad = grad(:);end learningCurve.m 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566function [error_train, error_val] = ... learningCurve(X, y, Xval, yval, lambda)%LEARNINGCURVE Generates the train and cross validation set errors needed %to plot a learning curve% [error_train, error_val] = ...% LEARNINGCURVE(X, y, Xval, yval, lambda) returns the train and% cross validation set errors for a learning curve. In particular, % it returns two vectors of the same length - error_train and % error_val. Then, error_train(i) contains the training error for% i examples (and similarly for error_val(i)).%% In this function, you will compute the train and test errors for% dataset sizes from 1 up to m. In practice, when working with larger% datasets, you might want to do this in larger intervals.%% Number of training examplesm = size(X, 1);% You need to return these values correctlyerror_train = zeros(m, 1);error_val = zeros(m, 1);% ====================== YOUR CODE HERE ======================% Instructions: Fill in this function to return training errors in % error_train and the cross validation errors in error_val. % i.e., error_train(i) and % error_val(i) should give you the errors% obtained after training on i examples.%% Note: You should evaluate the training error on the first i training% examples (i.e., X(1:i, :) and y(1:i)).%% For the cross-validation error, you should instead evaluate on% the _entire_ cross validation set (Xval and yval).%% Note: If you are using your cost function (linearRegCostFunction)% to compute the training and cross validation error, you should % call the function with the lambda argument set to 0. % Do note that you will still need to use lambda when running% the training to obtain the theta parameters.%% Hint: You can loop over the examples with the following:%% for i = 1:m% % Compute train/cross validation errors using training examples % % X(1:i, :) and y(1:i), storing the result in % % error_train(i) and error_val(i)% ....% % end%% ---------------------- Sample Solution ----------------------for i = 1:m theta = trainLinearReg(X(1:i, :), y(1:i),lambda); error_train(i) = linearRegCostFunction(X(1:i, :), y(1:i),theta,0); error_val(i) = linearRegCostFunction(Xval, yval,theta,0); end% -------------------------------------------------------------% =========================================================================end polyFeatures.m 12345678910111213141516171819202122232425function [X_poly] = polyFeatures(X, p)%POLYFEATURES Maps X (1D vector) into the p-th power% [X_poly] = POLYFEATURES(X, p) takes a data matrix X (size m x 1) and% maps each example into its polynomial features where% X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ... X(i).^p];%% You need to return the following variables correctly.X_poly = zeros(numel(X), p);% ====================== YOUR CODE HERE ======================% Instructions: Given a vector X, return a matrix X_poly where the p-th % column of X contains the values of X to the p-th power.%% for i = 1:numel(X) for j = 1:p X_poly(i,j) = X(i).^j;endfor% =========================================================================end validationCurve.m 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152function [lambda_vec, error_train, error_val] = ... validationCurve(X, y, Xval, yval)%VALIDATIONCURVE Generate the train and validation errors needed to%plot a validation curve that we can use to select lambda% [lambda_vec, error_train, error_val] = ...% VALIDATIONCURVE(X, y, Xval, yval) returns the train% and validation errors (in error_train, error_val)% for different values of lambda. You are given the training set (X,% y) and validation set (Xval, yval).%% Selected values of lambda (you should not change this)lambda_vec = [0 0.001 0.003 0.01 0.03 0.1 0.3 1 3 10]&apos;;% You need to return these variables correctly.error_train = zeros(length(lambda_vec), 1);error_val = zeros(length(lambda_vec), 1);% ====================== YOUR CODE HERE ======================% Instructions: Fill in this function to return training errors in % error_train and the validation errors in error_val. The % vector lambda_vec contains the different lambda parameters % to use for each calculation of the errors, i.e, % error_train(i), and error_val(i) should give % you the errors obtained after training with % lambda = lambda_vec(i)%% Note: You can loop over lambda_vec with the following:%% for i = 1:length(lambda_vec)% lambda = lambda_vec(i);% % Compute train / val errors when training linear % % regression with regularization parameter lambda% % You should store the result in error_train(i)% % and error_val(i)% ....% % end%%for i = 1:length(lambda_vec) lambda = lambda_vec(i); theta = trainLinearReg(X, y,lambda); error_train(i) = linearRegCostFunction(X, y,theta,0); error_val(i) = linearRegCostFunction(Xval, yval,theta,0); endfor% =========================================================================end]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-神经网络（后向传播）]]></title>
    <url>%2Farticle%2F336e6e71%2F</url>
    <content type="text"><![CDATA[symbolsL ： 神经网络的层数 S_l ： 第 l 层神经网络的神经元数 K ：分类个数 代价函数从逻辑回归算法中衍生到神经网络 每个 logistic regression algorithm 的代价函数 然后 K次输出 最后求和。 不对偏置单元(bias)进行正则化操作，即含X_0的项 步骤定义：误差项 delta(l)_j 表示第 l 层的第 j 的激活值与y之间的误差。 不严谨地说 ：在忽略lambda(正则项)或lambda=0，误差项 = 相应的代价项的偏导 由此可以计算出所需参数。 综上： ▲是大写的delta 更好地理解 delta(l)_j 其实是代价函数关于Z(l)__j的偏导，其值只计算隐藏单元而不计算偏置单元。 参数矩阵与向量 公式中参数的形式均为向量，所以需要将矩阵转化为向量形式。 矩阵-&gt;向量： a = [ b1(:); b2(:); b3(:) ] 向量-&gt;矩阵： b1 = reshape(a(1:110),10,11); %向量前110个数组成10行11列的矩阵。 梯度检测（1）梯度估计 （2）J(θ)偏导数 （3）检查J(θ)偏导数 是否等于 反向传播的导数，近似相等则反向传播正确实现 综合检测步骤： 注意在正确之后，关闭梯度检测再进行训练分类器。 θ随机初始化防止 θ值相同，限制学习模型的特征输入 总结训练神经网络 （1）选择网络架构： 输入特征项数，输出分类项 ，隐藏层（1个或者 多个但每层隐藏单元相同） （2）随机初始化θ （3）完成前向传播算法得到h_θ(x) （4）完成代价函数 J(θ) 的计算 ​ (5) 完成反向传播算法得到 J关于θ的偏导数 （6）遍历训练集，运行前向传播算法和反向传播算法，得到每层激活项a(l)与delta(l) （7）梯度检查 （8）高级优化算法与反向传播算法结合计算min J. 编程作业nnCostFunction.m 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121function [J grad] = nnCostFunction(nn_params, ... input_layer_size, ... hidden_layer_size, ... num_labels, ... X, y, lambda)%NNCOSTFUNCTION Implements the neural network cost function for a two layer%neural network which performs classification% [J grad] = NNCOSTFUNCTON(nn_params, hidden_layer_size, num_labels, ...% X, y, lambda) computes the cost and gradient of the neural network. The% parameters for the neural network are &quot;unrolled&quot; into the vector% nn_params and need to be converted back into the weight matrices. % % The returned parameter grad should be a &quot;unrolled&quot; vector of the% partial derivatives of the neural network.%% Reshape nn_params back into the parameters Theta1 and Theta2, the weight matrices% for our 2 layer neural networkTheta1 = reshape(nn_params(1:hidden_layer_size * (input_layer_size + 1)), ... hidden_layer_size, (input_layer_size + 1));Theta2 = reshape(nn_params((1 + (hidden_layer_size * (input_layer_size + 1))):end), ... num_labels, (hidden_layer_size + 1));% Setup some useful variablesm = size(X, 1); % You need to return the following variables correctly J = 0;Theta1_grad = zeros(size(Theta1));Theta2_grad = zeros(size(Theta2));% ====================== YOUR CODE HERE ======================% Instructions: You should complete the code by working through the% following parts.%% Part 1: Feedforward the neural network and return the cost in the% variable J. After implementing Part 1, you can verify that your% cost function computation is correct by verifying the cost% computed in ex4.m%将y扩展为5000*10的0/1表示形式expend = eye(num_labels);%disp(expend);disp(&quot;\n&quot;);%disp(y(500:505,:));disp(&quot;\n&quot;);y = expend(y,:);%disp(y(500:505,:));one = ones(m,1);a1 = [one,X];z2 = Theta1 * a1&apos; ;a2 = sigmoid(z2);a2 = [one,a2&apos;];z3 = a2 * Theta2&apos;;h = sigmoid(z3);J = sum(sum((y-1).*log(1-h) - y.*log(h))) / m ;reg = lambda/2/m * (sum(sum(Theta1(:,2:end).^2)) + sum(sum(Theta2(:,2:end).^2)));%reg = lambda/2/m *(sum(sum(Theta1.^2)) + sum(sum(Theta2.^2))% - sum(sum(Theta1(:,1).^2)) + sum(sum(Theta2(:,1).^2)));J = J + reg;%% Part 2: Implement the backpropagation algorithm to compute the gradients% Theta1_grad and Theta2_grad. You should return the partial derivatives of% the cost function with respect to Theta1 and Theta2 in Theta1_grad and% Theta2_grad, respectively. After implementing Part 2, you can check% that your implementation is correct by running checkNNGradients%% Note: The vector y passed into the function is a vector of labels% containing values from 1..K. You need to map this vector into a % binary vector of 1&apos;s and 0&apos;s to be used with the neural network% cost function.%% Hint: We recommend implementing backpropagation using a for-loop% over the training examples if you are implementing it for the % first time.delta3 = h - y;delta2 = delta3 * Theta2 ;delta2 = delta2(:,2:end);delta2 = delta2 .* sigmoidGradient(z2)&apos;;D1 = zeros(size(Theta1));D2 = zeros(size(Theta2));D1 = D1 + delta2&apos; * a1;D2 = D2 + delta3&apos; * a2;%% Part 3: Implement regularization with the cost function and gradients.%% Hint: You can implement this around the code for% backpropagation. That is, you can compute the gradients for% the regularization separately and then add them to Theta1_grad% and Theta2_grad from Part 2.%Theta1_grad = ( D1 + lambda * Theta1 ) / m;Theta2_grad = ( D2 + lambda * Theta2 ) / m;Theta1_grad(:,1) = Theta1_grad(:,1) - lambda / m * Theta1(:,1);Theta2_grad(:,1) = Theta2_grad(:,1) - lambda / m * Theta2(:,1);%找到每个X最大的估计数字，h归一化% maxValue = max(h,[],2);% h = (h &gt;= maxValue);% -------------------------------------------------------------% =========================================================================% Unroll gradientsgrad = [Theta1_grad(:) ; Theta2_grad(:)];end sigmoidGradient.m 1234567891011121314151617181920function g = sigmoidGradient(z)%SIGMOIDGRADIENT returns the gradient of the sigmoid function%evaluated at z% g = SIGMOIDGRADIENT(z) computes the gradient of the sigmoid function% evaluated at z. This should work regardless if z is a matrix or a% vector. In particular, if z is a vector or matrix, you should return% the gradient for each element.g = zeros(size(z));% ====================== YOUR CODE HERE ======================% Instructions: Compute the gradient of the sigmoid function evaluated at% each value of z (z can be a matrix, vector or scalar).g = sigmoid(z) .* (1-sigmoid(z));% =============================================================end randInitializeWeights.m 1234567891011121314151617181920212223242526272829function W = randInitializeWeights(L_in, L_out)%RANDINITIALIZEWEIGHTS Randomly initialize the weights of a layer with L_in%incoming connections and L_out outgoing connections% W = RANDINITIALIZEWEIGHTS(L_in, L_out) randomly initializes the weights % of a layer with L_in incoming connections and L_out outgoing % connections. %% Note that W should be set to a matrix of size(L_out, 1 + L_in) as% the first column of W handles the &quot;bias&quot; terms%% You need to return the following variables correctly W = zeros(L_out, 1 + L_in);% ====================== YOUR CODE HERE ======================% Instructions: Initialize W randomly so that we break the symmetry while% training the neural network.%% Note: The first column of W corresponds to the parameters for the bias unit%epsilon_init = 0.12;W = rand(L_out, 1 + L_in)*2*epsilon_init - epsilon_init;% =========================================================================end]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-神经网络（前向传播）]]></title>
    <url>%2Farticle%2F4640c193%2F</url>
    <content type="text"><![CDATA[原因适应于非线性假设 sigmoid（logistic）activation function : 激活函数 指代 g(z) X_0 偏置单元，值永远是1 θ 代表 参数或权重 架构输入层-》隐藏层（不止一个）-》输出层 符号： a(i)_j表示 第i层，第j个神经元或单元的激活项(由具体的神经元计算并输出的值) θ(i) 表示 从第i层到第i+1层之间映射的权重矩阵 偏置单元省略没有写。 如果第 j 层中有 S_j 个单元，第 j+1 层中有 S_j+1 个单元，则θ(j)的维数： S_j+1*（ S_j + 1） 前向传播（向量化）依次计算激活项，从输入项到输出项的过程。 下面是它的向量化实现 最后一层隐藏层到输出层 类似于 逻辑回归 多类别分类（一对多分类的扩展） 编程作业displayData.m 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859function [h, display_array] = displayData(X, example_width)%DISPLAYDATA Display 2D data in a nice grid% [h, display_array] = DISPLAYDATA(X, example_width) displays 2D data% stored in X in a nice grid. It returns the figure handle h and the % displayed array if requested.% Set example_width automatically if not passed inif ~exist(&apos;example_width&apos;, &apos;var&apos;) || isempty(example_width) example_width = round(sqrt(size(X, 2)));end% Gray Imagecolormap(gray);% Compute rows, cols[m n] = size(X);example_height = (n / example_width);% Compute number of items to displaydisplay_rows = floor(sqrt(m));display_cols = ceil(m / display_rows);% Between images paddingpad = 1;% Setup blank displaydisplay_array = - ones(pad + display_rows * (example_height + pad), ... pad + display_cols * (example_width + pad));% Copy each example into a patch on the display arraycurr_ex = 1;for j = 1:display_rows for i = 1:display_cols if curr_ex &gt; m, break; end % Copy the patch % Get the max value of the patch max_val = max(abs(X(curr_ex, :))); display_array(pad + (j - 1) * (example_height + pad) + (1:example_height), ... pad + (i - 1) * (example_width + pad) + (1:example_width)) = ... reshape(X(curr_ex, :), example_height, example_width) / max_val; curr_ex = curr_ex + 1; end if curr_ex &gt; m, break; endend% Display Imageh = imagesc(display_array, [-1 1]);% Do not show axisaxis image offdrawnow;end lrCostFunction.m 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152function [J, grad] = lrCostFunction(theta, X, y, lambda)%LRCOSTFUNCTION Compute cost and gradient for logistic regression with %regularization% J = LRCOSTFUNCTION(theta, X, y, lambda) computes the cost of using% theta as the parameter for regularized logistic regression and the% gradient of the cost w.r.t. to the parameters. % Initialize some useful valuesm = length(y); % number of training examples% You need to return the following variables correctly J = 0;grad = zeros(size(theta));% ====================== YOUR CODE HERE ======================% Instructions: Compute the cost of a particular choice of theta.% You should set J to the cost.% Compute the partial derivatives and set grad to the partial% derivatives of the cost w.r.t. each parameter in theta%% Hint: The computation of the cost function and gradients can be% efficiently vectorized. For example, consider the computation%% sigmoid(X * theta)%% Each row of the resulting matrix will contain the value of the% prediction for that example. You can make use of this to vectorize% the cost function and gradient computations. %% Hint: When computing the gradient of the regularized cost function, % there&apos;re many possible vectorized solutions, but one solution% looks like:% grad = (unregularized gradient for logistic regression)% temp = theta; % temp(1) = 0; % because we don&apos;t add anything for j = 0 % grad = grad + YOUR_CODE_HERE (using the temp variable)%h = sigmoid(X * theta);J = sum((y-1)&apos; *log(1-h) - y&apos; *log(h))/m + lambda/2/m * (sum(theta.^2) - theta(1)^2);grad = (X&apos;*(h-y) + theta*lambda) / m;grad(1) = grad(1) - lambda/m*theta(1);% =============================================================grad = grad(:);end oneVsAll.m 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364function [all_theta] = oneVsAll(X, y, num_labels, lambda)%ONEVSALL trains multiple logistic regression classifiers and returns all%the classifiers in a matrix all_theta, where the i-th row of all_theta %corresponds to the classifier for label i% [all_theta] = ONEVSALL(X, y, num_labels, lambda) trains num_labels% logistic regression classifiers and returns each of these classifiers% in a matrix all_theta, where the i-th row of all_theta corresponds % to the classifier for label i% Some useful variablesm = size(X, 1);n = size(X, 2);% You need to return the following variables correctly all_theta = zeros(num_labels, n + 1);% Add ones to the X data matrixX = [ones(m, 1) X];% ====================== YOUR CODE HERE ======================% Instructions: You should complete the following code to train num_labels% logistic regression classifiers with regularization% parameter lambda. %% Hint: theta(:) will return a column vector.%% Hint: You can use y == c to obtain a vector of 1&apos;s and 0&apos;s that tell you% whether the ground truth is true/false for this class.%% Note: For this assignment, we recommend using fmincg to optimize the cost% function. It is okay to use a for-loop (for c = 1:num_labels) to% loop over the different classes.%% fmincg works similarly to fminunc, but is more efficient when we% are dealing with large number of parameters.%% Example Code for fmincg:%% % Set Initial theta% initial_theta = zeros(n + 1, 1);% % % Set options for fminunc% options = optimset(&apos;GradObj&apos;, &apos;on&apos;, &apos;MaxIter&apos;, 50);% % % Run fmincg to obtain the optimal theta% % This function will return theta and the cost % [theta] = ...% fmincg (@(t)(lrCostFunction(t, X, (y == c), lambda)), ...% initial_theta, options);%for c = 1:num_labels initial_theta = zeros(n+1 , 1); options = optimset(&apos;GradObj&apos;, &apos;on&apos;, &apos;MaxIter&apos;, 50); initial_theta = fmincg(@(t)(lrCostFunction(t, X, (y==c), lambda)),initial_theta,options); all_theta(c,:) = initial_theta&apos;;end% =========================================================================end predict.m 1234567891011121314151617181920212223242526272829303132333435363738394041function p = predict(Theta1, Theta2, X)%PREDICT Predict the label of an input given a trained neural network% p = PREDICT(Theta1, Theta2, X) outputs the predicted label of X given the% trained weights of a neural network (Theta1, Theta2)% Useful valuesm = size(X, 1);num_labels = size(Theta2, 1);% You need to return the following variables correctly p = zeros(size(X, 1), 1);% ====================== YOUR CODE HERE ======================% Instructions: Complete the following code to make predictions using% your learned neural network. You should set p to a % vector containing labels between 1 to num_labels.%% Hint: The max function might come in useful. In particular, the max% function can also return the index of the max element, for more% information see &apos;help max&apos;. If your examples are in rows, then, you% can use max(A, [], 2) to obtain the max for each row.%one = ones(size(X,1),1);addX = [one,X];z = addX * Theta1&apos;;a = sigmoid(z);addA = [one,a];h = addA * Theta2&apos;;maxRow = zeros(size(X,1) ,1);for i =1:m [maxRow(i), p(i)] = max(h(i,:));endfor% =========================================================================end predictOneVsAll.m 123456789101112131415161718192021222324252627282930313233343536373839404142434445function p = predictOneVsAll(all_theta, X)%PREDICT Predict the label for a trained one-vs-all classifier. The labels %are in the range 1..K, where K = size(all_theta, 1). % p = PREDICTONEVSALL(all_theta, X) will return a vector of predictions% for each example in the matrix X. Note that X contains the examples in% rows. all_theta is a matrix where the i-th row is a trained logistic% regression theta vector for the i-th class. You should set p to a vector% of values from 1..K (e.g., p = [1; 3; 1; 2] predicts classes 1, 3, 1, 2% for 4 examples) m = size(X, 1);num_labels = size(all_theta, 1);% You need to return the following variables correctly p = zeros(size(X, 1), 1);% Add ones to the X data matrixX = [ones(m, 1) X];% ====================== YOUR CODE HERE ======================% Instructions: Complete the following code to make predictions using% your learned logistic regression parameters (one-vs-all).% You should set p to a vector of predictions (from 1 to% num_labels).%% Hint: This code can be done all vectorized using the max function.% In particular, the max function can also return the index of the % max element, for more information see &apos;help max&apos;. If your examples % are in rows, then, you can use max(A, [], 2) to obtain the max % for each row.% % size(h) = 50000*10 h保存的是预测5000个数据所属label的概率情况 h = sigmoid(X * all_theta&apos;);rowMax = zeros(size(X,1),1);%这是取每行(最后一个参数：2)最大值 ps:取每列(最后一个参数：1)最大值%rowMax = max(h,[],2);%取每行最大值的索引值for i = 1:m [rowMax(i), p(i)] = max(h(i,:));end% ========================================================================= sigmoid.m 123456function g = sigmoid(z)%SIGMOID Compute sigmoid functoon% J = SIGMOID(z) computes the sigmoid of z.g = 1.0 ./ (1.0 + exp(-z));end]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-逻辑回归]]></title>
    <url>%2Farticle%2Fa0a1582a%2F</url>
    <content type="text"><![CDATA[sigmoid function also logistic function “分类问题”–逻辑回归算法 决策边界 不是训练集的属性，而是假设本身和其参数的属性 计算的是 属于1 的概率 cost function 适用于梯度下降的变形式： 综上： 来源于统计学的极大似然估计法—-凸函数 高级优化 使用库~直接调用，适用于大型的机器学习 多元分类：”一对多“分类采用多个分类器，针对其中一种情况进行训练 拟合泛化 ：一个假设模型应用到新样本的能力~ 欠拟合/过拟合 高方差 减少特征变量但是同时舍弃了一些有用信息 正则化简化假设模型，更少的倾向过拟合 线性回归正则化后一项是正则化项 1不惩罚 θ_0 （1）梯度下降： （2）正则化方程： 虽然octave在一般的正则化方程中 pinv() 可能会给 奇异矩阵 可逆结果，但是这不能泛化。但经过正则化 正则方程后，一定是可逆的。 逻辑回归正则化1不惩罚 θ_0 注意：虽然看起来和 线性回归一样，但是 h(x)不同 （3）高级算法： fminunc() :函数在无约束条件下的最小值 不需要写任何循环，也无需设置循环；只需要提供一个计算“代价”和“梯度”的函数，就返回正确的优化参数、代价值、θ tips: octave下标从1开始。。 编程作业costFunction.m 123456789101112131415161718192021222324252627282930function [J, grad] = costFunction(theta, X, y)%COSTFUNCTION Compute cost and gradient for logistic regression% J = COSTFUNCTION(theta, X, y) computes the cost of using theta as the% parameter for logistic regression and the gradient of the cost% w.r.t. to the parameters.% Initialize some useful valuesm = length(y); % number of training examples% You need to return the following variables correctly J = 0;grad = zeros(size(theta));% ====================== YOUR CODE HERE ======================% Instructions: Compute the cost of a particular choice of theta.% You should set J to the cost.% Compute the partial derivatives and set grad to the partial% derivatives of the cost w.r.t. each parameter in theta%% Note: grad should have the same dimensions as theta%h = sigmoid(X * theta);J = sum( log(1-h)&apos;*(y-1) - log(h)&apos;*y )/m;grad = X&apos; * (h - y)/m;% =============================================================end costFunctionReg.m 123456789101112131415161718192021222324252627282930313233343536function [J, grad] = costFunctionReg(theta, X, y, lambda)%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization% J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using% theta as the parameter for regularized logistic regression and the% gradient of the cost w.r.t. to the parameters. % Initialize some useful valuesm = length(y); % number of training examples% You need to return the following variables correctly J = 0;grad = zeros(size(theta));% ====================== YOUR CODE HERE ======================% Instructions: Compute the cost of a particular choice of theta.% You should set J to the cost.% Compute the partial derivatives and set grad to the partial% derivatives of the cost w.r.t. each parameter in thetah = sigmoid(X * theta);J = sum( log(1-h)&apos;*(y-1) - log(h)&apos;*y )/m + lambda/2/m * (sum(theta.^2)- theta(1)^2);for j = 1:length(theta) if j == 1 grad(j)= ((h - y)&apos; * X(:,j)) /m; else grad(j) = ( (h - y)&apos; * X(:,j) + theta(j)*lambda) / m; end%另一种方法：%grad = X&apos; * (h - y)/m + theta*lambda / m; %grad(1) = grad(1) -theta(1)*lambda / m;% =============================================================end plotData.m 123456789101112131415161718192021222324function plotData(X, y)%PLOTDATA Plots the data points X and y into a new figure % PLOTDATA(x,y) plots the data points with + for the positive examples% and o for the negative examples. X is assumed to be a Mx2 matrix.% Create New Figurefigure; hold on;% ====================== YOUR CODE HERE ======================% Instructions: Plot the positive and negative examples on a% 2D plot, using the option &apos;k+&apos; for the positive% examples and &apos;ko&apos; for the negative examples.%%pos = find(y == 1);neg = find(y == 0);%plot(X(pos,1), X(pos,2), &apos;k+&apos;, &apos;LineWidth&apos;, 2, &apos;MarkerSize&apos;, 7);plot(X(neg,1),X(neg,2), &apos;ko&apos;, &apos;markerFaceColor&apos;, &apos;y&apos;, &apos;MarkerSize&apos;, 7);% =========================================================================hold off;end predict.m 123456789101112131415161718192021222324252627282930313233343536function p = predict(theta, X)%PREDICT Predict whether the label is 0 or 1 using learned logistic %regression parameters theta% p = PREDICT(theta, X) computes the predictions for X using a % threshold at 0.5 (i.e., if sigmoid(theta&apos;*x) &gt;= 0.5, predict 1)m = size(X, 1); % Number of training examples% You need to return the following variables correctlyp = zeros(m, 1);% ====================== YOUR CODE HERE ======================% Instructions: Complete the following code to make predictions using% your learned logistic regression parameters. % You should set p to a vector of 0&apos;s and 1&apos;s%p = sigmoid(X * theta);rows = size(p,1);cols = size(p,2);for i = 1:rows for j = 1:cols if p(i,j) &lt; 0.5 p(i,j) = 0; else p(i,j) = 1; endend%p = sigmoid(X*theta)&gt;0.5;% =========================================================================end sigmoid.m 1234567891011121314151617181920function g = sigmoid(z)%SIGMOID Compute sigmoid function% g = SIGMOID(z) computes the sigmoid of z.% You need to return the following variables correctly g = zeros(size(z));% ====================== YOUR CODE HERE ======================% Instructions: Compute the sigmoid of each value of z (z can be a matrix,% vector or scalar).%rows = size(z)(1);%cols = size(z)(2);%if rows == 1 &amp;&amp; cols == 1 one = ones(size(z)); g = one ./ (1 + exp(-z));% =============================================================end]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-知识点]]></title>
    <url>%2Farticle%2Fafd3b944%2F</url>
    <content type="text"><![CDATA[分类监督学习 supervised learning“right answer” for each of our examples in the data regression(回归) ：predict a real-valued output classification(分类)：predict discrete-valued output (类似于0、1问题) 无监督学习 unsupervise learning数据未标记，类别未知。 计算机自己根据样本间的相似性对样本集进行分类 如何看懂一篇机器学习论文 symbolsm = number of training examples x’s = “input” variable / features y’s = “output”variable / “target” variable (x,y) - one training example (x^(i) , y^(i)) - the i training example ( i 表示索引，不是幂) hypothesis（假设，机器学习术语— 输出函数） univariate 单因素 编程环境Octave 最好是.exe 正常安装，不然其他还需要编译。 官网：http://www.gnu.org/software/octave/#install 镜像：https://mirrors.kernel.org/gnu/octave/ Octave语法疑点对 Octave中 A*B A.*B 这两种矩阵相乘做一个理解： 举例： a = [1 2 3;4 5 6] b = [1 1 1;1 1 1] c = [1 ; 1; 1] 分别做 a*b a.*b a*c a.*c ,结果如图 然后更换a、b的值，结果如下 可以得出： A.*B 是 矩阵A中的元素 × B中对应位置的元素 ​ A*B 是 线性代数中矩阵相乘的实现，即要满足 A的列数 = B的行数 ​ 补充：A‘ * B 只是A的转置矩阵再与B相乘。]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习-线性回归]]></title>
    <url>%2Farticle%2F19883263%2F</url>
    <content type="text"><![CDATA[单变量线性回归symbols m = number of training examples x’s = “input” variable / features y’s = “output”variable / “target” variable (x,y) - one training example (x^(i) , y^(i)) - the i training example ( i 表示索引，不是幂) hypothesis（假设，机器学习术语— 输出函数） univariate 单因素 代价函数 调整参数 最小化代价函数—– h(x)-y ，拟合数据 contour figure /plot (等高线图) 梯度下降（gradient descent）似乎只找到局部最优解 a:=b 赋值 a = b 真假判断 converge(收敛) diverge(发散) 线性回归的代价函数总是convex function(凸函数) bow - shape “Batch”:Each step of gradient descent uses all the training examples 多变量线性回归 其中X_0 为 1，这是为了适应计算机索引（从0开始）并不破坏原有顺序而添加的。 代价函数J对θ求偏导，其中x,y均为常量，h(x)是与θ有关的线性函数 多元梯度下降特征缩放是为了加快梯度下降速度，减少收敛所需次数。 不需要特别精确，只需使得特征在一相似范围内，一般约束到[-1,1]左右(接近)，区间范围不宜过大或过小 ​ Xi := Xi/range 均值归一化使得特征值具有为0的平均值，计算公式如下： 调试debug : making sure gradient descent is working correctly. （a）画出J(θ)与迭代次数的关系曲线图，直观得出结果 （b）自动判断if J(θ) &lt; ε 则收敛 若 J(θ)随迭代次数而上升，则降低学习率α，但α太小，则梯度下降缓慢。 每3（10）倍取一个α值，选择合适的learning rate —- α 特征与多项式回归特征的选择是自由的~ 取决于从什么角度来看问题，最终更拟合数据，建立更好的模型。 直线可能不太合适时，考虑多项式（注意特征缩放）。 正规方程直接求解θ，与梯度下降不同。不需要特征缩放 （a）令偏导数为0，求解θ1、2、3…的值，计算可能复杂 （b）看成向量，公式求解 对于特征量小于10000的线性回归模型，正规方程比梯度下降优 正规方程在矩阵不可逆的情况下的solution矩阵不可逆？？？ Octave中 ocpinv() inv() 两种方式优缺点对比]]></content>
      <categories>
        <category>machine learning</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[批量拷贝文件名]]></title>
    <url>%2Farticle%2F22ee1c26%2F</url>
    <content type="text"><![CDATA[操作新建 txt 文件 写入如下命令： 12345dir （目标文件夹）/b &gt; （生成的.txt/.docx/.excel输出路径）如：dir E:\program\opencv\msvc410\install\x64\vc15\lib\*.lib /b &gt; E:\program\opencv\msvc410\install\x64\vc15\lib\libname.txt将 E:\program\opencv\msvc410\install\x64\vc15\lib 文件夹下 所有.lib的文件名 打印到 同一目录下的libname.txt中 保存，将该txt文件后缀改为.bat ，运行，完成~]]></content>
      <categories>
        <category>window</category>
      </categories>
      <tags>
        <tag>skill</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[重编译openCV源码（4.1.0）with CUDA]]></title>
    <url>%2Farticle%2F3833e1c9%2F</url>
    <content type="text"><![CDATA[起因在利用openCV的GPU模块中 发现CUDA系列头文件找不到了openCV2 cudaXXXX.hpp file not found.md回想之前编译openCV源代码的时候，好像忘勾选CUDA了 解决方案重编译openCV。。。 相关环境openCV and contribute 4.1.0 -》后面尝试过 4.0.1-》还是确定为 4.1.0 cuda 10.1 cmake 3.14.3(默认安装并已添加环境变量) vs 2017(默认安装） TBB 4.4（可选） 题外话CUDA SDK不支持mingw编译，因此选用的是VS编译器 先备注起来，以后可能会用到 这是mingw编译器的操作 打开命令行 cd 到你编译后的文件夹中 其中-j8是指用8个线程进行编译，你可以根据自己的计算机选择线程数量cd E:\program\opencv\CUDA_Buildmingw32-make -j8 我大概编译了15分钟mingw32-make install 编译前准备工作1.安装CUDA toolkit安装之前电脑需装配VS2017社区版，并且关闭其应用。更重要的一点：关闭360等软件。。不然会导致安装失败。https://developer.nvidia.com/cuda-toolkit-archive选择对应版本 （本文 10.1）一路默认就OK了（可以自定义下安装目录） CUDA_BIN_PATH %CUDA_PATH%\bin CUDA_LIB_PATH %CUDA_PATH%\lib\x64 CUDA_SDK_BIN_PATH %CUDA_SDK_PATH%\bin\win64 CUDA_SDK_LIB_PATH %CUDA_SDK_PATH%\common\lib\x64 然后，在系统变量 PATH 添加： %CUDA_LIB_PATH%;%CUDA_BIN_PATH%;%CUDA_SDK_LIB_PATH%;%CUDA_SDK_BIN_PATH% 重启生效 检测是否安装成功 （以自带示例来测试） 命令行 cd C:\Program Files\NVIDIA GPU ComputingToolkit\CUDA\v10.1\extras\demo_suite 分别运行deviceQuery、bandwidthTest 如果两次都出现result pass，则成功啦~~~ 2.安装TBB（VS2015可安装，2017需要VC14才行）（Thread BuildingBlocks,线程构建模块，是Intel公司开发的并行编程开发的工具）https://github.com/01org/tbb/releases 选择对应系统 （本文 Win TBB 4.4）然后设置环境变量：在系统-&gt; 高级系统设置-&gt;环境变量-&gt;path 后边加入下边地址：..(tbb安装目录)\tbb44_20160413oss\bin\intel64\vc14 3.检查V2017的vc版本因为我安装的是最新版V2107，在最后用VS生成库文件的时候，出现了许多bug，具体可看最后的“遇到的问题” 查看vc工具集版本 打开 C:\Program Files (x86)\Microsoft Visual Studio\2017 \Community\VC\Tools\MSVC (默认安装目录） 我安装目录是：E:\program\VisualStudio\VC\Tools\MSVC 如果vc版本大于14.11，请更换VC版本 具体操作查看文章 “遇到的问题”-》“VS生成过程中的问题”-》“2” 4.下载openCV4.1.0以及contribute源码解压,最好放到同一目录下，注意路径中不含中文 https://github.com/opencv/opencv/releaseshttps://github.com/opencv/opencv_contrib/releases/ 其中build 是自己建立的 cmake重编译 点击右侧“Browse Source”按钮输入OpenCV源码所在路径 点击右侧“Browse Build”输入生成的OpenCV工程存放目录 点击左下角Configure按钮，配置编译器 cmake 新版本（3.14.3） cmake 老版本（3.9.0） 再一次点击Configure 注意输出栏会出现“CUDA detected 10.1” 修改红底色配置项 勾选 WITH_CUDA WITH_QT WITH_OPENGL OPENCV_ENABLE_NONFREE WITH_OPENCL_SVM WITH_TBB (VS2015可选) BUILD_EXAMPLES(可选，创建openCV实例) BUILD_opencv_world(可选，所有库变成一个) 不要勾选 Build_CUDA_STUBS ENABLE_PRECOMPILED_HEADERS 同时需要修改…(你源码解压路径)\opencv-4.1.0\modules\videoio\src下文件“cap_Dshow.cpp”，修改是在“#include “cap_dshow.hpp”（在第45行）前加宏定义“#define NO_DSHOW_STRSAFE”，如下图所示： 配置opencv_contrib的文件路径…(你源码解压路径)\opencv_contrib-4.0.1\modules 再一次点击Configure 配置Qt/TBB的路径QT配置如下： 如QT5Test_DIR E:/program/Qt5.11.2/5.10.0/msvc2017_64/lib/cmake/Qt5Concurrent TBB配置如下： 如：TBB_ENV_INCLUDE E:/program/tbb/tbb2019_20190320oss/include 再一次点击Configure。 配置CUDA架构版本网上查询显卡对应架构版本，==CUDA_GENERATION== 选择对应架构版本 再一次点击Configure，直至没有红底色栏出现。 再检查一下之前的勾选项，点generate生成MakeFile文件 点Open project打开项目文件 VS后续操作1.权限 以==管理员的身份==打开项目（你自定义的文件里），因为编译过程可能会出现需要系统授权才能访问的文件 2.测试 Cmake完成之后，==不要全部编译==。先选择==opencv_cudaarithm,opencv_cudabgsegm==这两个项目编译，因为这两个项目编译起来非常慢而且最容易出问题。在项目上 右键-&gt;生成，当这些文件都没有问题时，正式生成所有库文件。 3.正式开始生成 打开项目后， 点击“生成”-》“批生成”-》勾选install的“debug”和”release”，同时生成两个版本~ 时间较长。 如果全部编译完毕之后，如果有一两个项目没有编译成功，没必要再全部重新编译。只需要在VS侧边栏的“解决方案资源管理器”找到对应的项目重新生成就行了（各种库生成失败，可以看看文章后面“==vs生成过程中的问题==”）。 openCV环境配置以及测试生成成功后，将 E:\program\opencv\msvc410\install\x64\vc15\bin 添加入 path VS的网上有 ：https://blog.csdn.net/qq_41175905/article/details/80560429 QT mvsc版如下： 环境变量 path 添加：E:\program\opencv\msvc410\install\x64\vc15\bin （里面是dll动态链接库） 新建QT项目，套件为 mvsc 2017 64bit （因为openCV4.1.0只能用64位的） .pro文件内添加open’CV环境 1234#openCVLIBS += E:/program/opencv/msvc410/install/x64/vc15/lib/*.libINCLUDEPATH += E:/program/opencv/msvc410/install/include/ \ E:/program/opencv/msvc410/install/include/opencv2/ main.cpp 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071#include "mainwindow.h"#include &lt;QApplication&gt;#include &lt;opencv2/opencv.hpp&gt;#include &lt;opencv2/core/version.hpp&gt;#include "opencv2/core/cuda.hpp"#include "opencv2/imgproc.hpp"#include "opencv2/highgui.hpp"#include "opencv2/calib3d.hpp"#include "opencv2/video.hpp"#include "opencv2/cudalegacy.hpp"#include "opencv2/cudaimgproc.hpp"#include "opencv2/cudaarithm.hpp"#include "opencv2/cudawarping.hpp"#include "opencv2/cudafeatures2d.hpp"#include "opencv2/cudafilters.hpp"#include "opencv2/cudaoptflow.hpp"#include "opencv2/cudabgsegm.hpp"using namespace cv;using namespace std;int main(int argc, char *argv[])&#123; QApplication a(argc, argv); MainWindow w; //w.show();// Mat dst = imread("更换你的图片路径");// imshow("show",dst); namespace GPU = cv::cuda; // 首先要检查是否CUDA模块是否可用 if(GPU::getCudaEnabledDeviceCount()==0)&#123; cerr&lt;&lt;"此OpenCV编译的时候没有启用CUDA模块"&lt;&lt;endl; return -1; &#125;else&#123; cout&lt;&lt;"GPU enable :"&lt;&lt;GPU::getCudaEnabledDeviceCount()&lt;&lt;endl; &#125; const int rows = 16*50; const int cols = 16*60; const int type = CV_8UC3; // 初始化一个黑色的GpuMat GPU::GpuMat gpuMat(rows,cols,type,Scalar(0,0,0)); // 定义一个空Mat Mat dst; // 把gpuMat中数据下载到dst(从显存下载到内存) gpuMat.download(dst); // 显示 imshow("show1",dst); waitKey(1000); // 读取一张图片 Mat arr = imread("更换你的图片路径"); imshow("show2",arr); waitKey(1000); // 上传到gpuMat(若gpuMat不为空，会先释放原来的数据，再把新的数据上传上去) gpuMat.upload(arr); // 定义另外一个空的GpuMat GPU::GpuMat gray; // 把gpuMat转换为灰度图gray GPU::cvtColor(gpuMat,gray,0); // 下载到dst，如果dst不为空，旧数据会被覆盖 gray.download(dst); // 显示 imshow("show3",dst); waitKey(0); return a.exec();&#125; 遇到的问题1.CUDA版本与显卡驱动不适配运行示例时出现的错误提示 123cudaGetDeviceProperties returned 35-&gt; CUDA driver version is insufficient for CUDA runtime versionCUDA error at C:/dvs/p4/build/sw/rel/gpgpu/toolkit/r10.1/demo_suite/bandwidthTest/![img](file:///C:\Users\Pabebe\AppData\Local\Temp\%W@GJ$ACOF(TYDYECOKVDYB.png)bandwidthTest.cu:255 code=35(cudaErrorInsufficientDriver) "cudaSetDevice(currentDevice)" https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html从该网站可以得到对应版本要求，进行更换或升级显卡驱动 2.编译openCV-cuda模块出现如下问题CMake Error: The following variables are used in this project, but they are set to NOTFOUND.Please set them or make sure they are set and tested correctly in the CMake files:CUDA_cublas_LIBRARY (ADVANCED) CUDA_npps_LIBRARY (ADVANCED) 1我看了github上的解决方案， 就是编译器选择 vs 15 2017 win64 (cmake的版本为3.9.0，最新版没有该选项),就不会出现CUDA_cublas_LIBRARY (ADVANCED)CUDA_npps_LIBRARY (ADVANCED)了。 但是会出现CUDA_nppi_LIBRARY (ADVANCED)，这个网上有修改的方案。大佬说是CUDA9.0版本不支持2.0架构，我觉得CUDA10.1估计也是。 首先找到 FindCUDA.cmake 参考文章： https://blog.csdn.net/u014613745/article/details/78310916 （1）跟着上面大佬的步骤改就是了。 （2）最后一步添加头文件的文件路径变了 E:\program\openCV4.1.0\opencv-4.1.0\modules\core\include\opencv2\core\cuda ..(解压的地址)\opencv_contrib-4.1.0\modules\cudev \include\opencv2\cudev\common.hpp （3）文件OpenCVDetectCUDA.cmake中 set(_generations “Fermi” “kepler” “Maxwell” “Pascal”)中”Fermi”去掉 3.VS2017与VS2015的区别 重点是使用CMAKE编译时CUDA_HOST_COMPILER项的选择，正常更改的路径如下： C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Tools\MSVC\14.16.27023\bin\Hostx64\x64\cl.exe 当时安装VS2017时，没有把所有的东西都安装在C盘，所以我的路径是 E:/program/VisualStudio/VC/Tools/MSVC/14.16.27023/bin/Hostx64/x64/cl.exe 参考文章： https://blog.csdn.net/baidu_33310451/article/details/89456480 若还是出现该问题，更换openCV版本 -》 4.0.1（我就是..） 4.VS生成过程中的问题1.生成cuda_imgcodec.lib时 （1）error : dynamic initialization is not supported for a constant variable 解决方案：https://answers.opencv.org/question/205673/building-opencv-with-cuda-win10-vs-2017/ 对E:\program\openCV4.0.1\opencv-4.0.1（openCV源码路径）\modules\core\include\opencv2\core\cuda\detail\color_detail.hpp 文件中96-127行“const” 修改为“constexpr” （2）E0282 全局范围没有 “max”/“min” opencv_core E:\program\openCV4.0.1\opencv-4.0.1\modules\core\include\opencv2\core\cuda\detail\color_detail.hpp 1614 解决方案：双击该错误，跳转至错误行，在::前添加std ​ int ix = std::min(std::max(int(x), 0), n-1); 2.生成cuda_arithm.lib时 (1)template instantiation resulted in unexpected function type of “std::true_type (std::integral_constant&lt;__nv_bool, false&gt; )” (the meaning of a name may have changed since the template declaration – the type of the template is “std::true_type (std::is_same&lt;std::decay&lt;decltype(())&gt;::type, void&gt;::type )”) opencv_cudaarithm e:\program\opencv4.0.1\opencv-4.0.1\modules\core\include\opencv2\core\cvstd_wrapper.hpp 49 (2)name followed by “::” must be a class or namespace name opencv_cudaarithm e:\program\opencv4.0.1\opencv-4.0.1\modules\core\include\opencv2\core\cvstd_wrapper.hpp 52 (3)incomplete type is not allowed opencv_cudaarithm e:\program\opencv4.0.1\opencv-4.0.1\modules\core\include\opencv2\core\cvstd_wrapper.hpp 45 总算找到了解决方案：VS2017的VC编译器版本过高 （1）首先下载V14.11的工具集 启动VS intall ，点击对应2017版本“更多”下的“修改” ，选择“单个工具‘，安装V14.11的工具集。 （2）根据官网https://cmake.org/cmake/help/latest/variable/CMAKE_GENERATOR_TOOLSET.html#variable:CMAKE_GENERATOR_TOOLSET cmake-gui的生成器参数设置方法，设置VS的VCv14.11的工具集 参考文章： https://blog.csdn.net/hybtalented/article/details/80434075?tdsourcetag=s_pctim_aiomsg https://blog.csdn.net/ewqapple/article/details/81974210 12按他们所说的修改方式，我在cmake-gui中配置的时候出错了。下面还是按照我的做吧。 ——下面是按参考文章的做法—- 更改默认Visual Studio 2017 VC 工具链 打开 C:\Program Files (x86)\Microsoft Visual Studio\2017 \Community\VC\Auxiliary\Build（默认安装目录） 而我的在 E:\program\VisualStudio\VC\Auxiliary\Build 修改Microsoft.VCToolsVersion.default.props 、Microsoft.VCRedistVersion.default.txt、Microsoft.VCToolsVersion.default.txt的读写权限 右键 属性-&gt;安全 其中Microsoft.VCToolsVersion.default.props修改了也没用，那我们直接以管理员的身份运行该文件 桌面菜单栏右键”任务管理器”-》文件-》运行新任务 将以上三个文件内VS默认编译工具链修改为 14.11.25503 （一般后缀名为.props 的文件是VC工具集的配置文件） 备注一下： VS原始默认工具集版本 VCToolsVersion 14.16.27023 VCToolsRedistVersion 14.16.27012 现在默认 VCToolsVersion 14.11.25503 VCToolsRedistVersion 14.11.25325 E:\program\VisualStudio\VC\Auxiliary\Build\ 生成opencv_cvv.lib 在stringutils.cpp 出现“常量中有换行符” 等错误 解决方案：用==记事本==打开“stringutils.cpp” ，另存为 “UTF-8”格式覆盖原来的文件，再重新生成“opencv_cvv” 参考文献：http://www.cnblogs.com/cocos2d-x/archive/2012/02/26/2368873.html#commentform 5.QT使用cuda版open’CV出现的错误:error: C1083: 无法打开包括文件: “cuda_runtime.h”: No such file or directory E:\program\opencv\msvc410\install\include\opencv2\core\cuda\common.hpp:46 双击错误信息，引入cuda_runtime.h所在的绝对路径]]></content>
      <categories>
        <category>openCV</category>
      </categories>
      <tags>
        <tag>tutorial</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub更新项目步骤及常见问题]]></title>
    <url>%2Farticle%2Fd0a29510%2F</url>
    <content type="text"><![CDATA[gitHub上更新项目该文以本人博客网站文章上传为例，同时记录问题 配置SSH key为获取GitHub权限，但直接提交账号密码不安全，因此 利用公私钥来解决这一问题。博客目录下右键 Git GUI Here 创建新的本地仓库为你的项目所在地，本文为博客根目录点 Help-&gt;show SSH Key-&gt;Generate Key（有的话就不需要了）拷贝SSH key的内容，添加到GitHub中。GitHub网站上头像-&gt;setting-&gt;SSH and GPG keys-&gt;new SSH key -&gt;title里面可以写点提示性话语，把之前的Key粘贴过来-&gt;Add SSH Key验证：博客目录下右键 Git Bash Here ssh -T git@github.com出现“Hi Pabebezz! You’ve successfully authenticated, but GitHub does not provide shell access.” 则成功 修改站点配置文件(_根目录下的config.yml)deploy: type: git repo: git@github.com:username.github.io.git #自己gitHub项目地址 branch: master 将Hexo与GitHub pages联系起来（1）设置本地Git的user name 和 email 博客目录下右键 Git Bash Here git config –global user.name “Pabebezz” git config –global user.email “zezuwang@qq.com“ 新建GitHub远程仓库仓库是否存在初始化文件（1）不存在 此时,仓库为空 博客目录下右键 Git Bash Here 1hexo g 1hexo d //远程发布到gitHub上 1hexo s //本地查看 localhost:4000 （2）存在 看“问题栏” gitHub使用过程中遇到的问题简单测试$ hexo dERROR Deployer not found: git解决：$ npm install hexo-deployer-git –save 修改next主题相关颜色E:\program\Blog\themes\next\source\css_variables\base.styl YAMLException: can not read a block mapping entry; a multiline key may not be an implicit key at linhexo clean g d后出现错误 解决办法：检测文章开头的一系列格式，“源代码模式” 有空格，三条下划线不能少]]></content>
      <categories>
        <category>gitHub</category>
      </categories>
      <tags>
        <tag>tutorial</tag>
      </tags>
  </entry>
</search>
