<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="paper,">





  <link rel="alternate" href="/atom.xml" title="Pabebe's Blog" type="application/atom+xml">






<meta name="description" content="该论文理解与复现">
<meta name="keywords" content="paper">
<meta property="og:type" content="article">
<meta property="og:title" content="paperUnderstanding-ClearGrasp">
<meta property="og:url" content="https://pabebezz.github.io/article/2d17733a/index.html">
<meta property="og:site_name" content="Pabebe&#39;s Blog">
<meta property="og:description" content="该论文理解与复现">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://1.bp.blogspot.com/-VNB-xPguois/XkQ7sqF3VWI/AAAAAAAAFRc/ek6wUpjOGNw0u9vALnnW9zXCZ8VsWTjcACLcBGAsYHQ/s640/image3.png">
<meta property="og:image" content="https://1.bp.blogspot.com/-mcdDuutDw5Q/XkQ7ssTGUvI/AAAAAAAAFRg/OIyqAa7Qbgs1yGWB7vFKL2E90hCPXiuYACEwYBhgL/s640/image2.gif">
<meta property="og:image" content="https://1.bp.blogspot.com/-y-DPUgVcfSk/XkQ7seO_nII/AAAAAAAAFR4/W2yq1dWQ2YkglMA5IguxyG1vJRRPg235gCEwYBhgL/s640/image4.jpg">
<meta property="og:image" content="https://1.bp.blogspot.com/-r9_NUznjObE/XkQ7teoA4BI/AAAAAAAAFSA/xNDlyrAzI-0Ys7iT2iF40AkaeaSjvbuFQCEwYBhgL/s640/image5.gif">
<meta property="og:image" content="https://1.bp.blogspot.com/-oncutBVkLQU/XkQ7rQz4zLI/AAAAAAAAFSI/Y1dSYj38GkgDTtw-P-UaP1e7pRzmC6izgCEwYBhgL/s640/image11.png">
<meta property="og:image" content="https://1.bp.blogspot.com/-Ds5KPDwGazc/XkQ7t4atuLI/AAAAAAAAFSA/bnqbcZpPeO8BK8Ewr0if2oTOtJmM2WoiwCEwYBhgL/s640/image6.gif">
<meta property="og:image" content="https://1.bp.blogspot.com/-QcyTESfw4AM/XkQ7rJaGdVI/AAAAAAAAFR8/CUPvkzCJuygr6xw9S36vYIQ1r0wfi3rogCEwYBhgL/s400/image10.jpg">
<meta property="og:image" content="https://1.bp.blogspot.com/-xbiK9-km_BE/XkQ7rJmME0I/AAAAAAAAFSA/2kRSqd3DvlcOpv6ql4qILwn3HdmmryfBACEwYBhgL/s640/image1.png">
<meta property="og:image" content="https://1.bp.blogspot.com/-QBobUQ2ssow/XkQ7txFy4gI/AAAAAAAAFSI/wIR43QtxSd48ghg9bEOAyN617pqIfvsPwCEwYBhgL/s640/image7.png">
<meta property="og:image" content="https://1.bp.blogspot.com/-iaTKEeKdlTA/XkQ7u6Mv6gI/AAAAAAAAFSE/_59HIFZptyoP9tDllG_0WVy2HKD4edPcwCEwYBhgL/s640/image9.png">
<meta property="og:image" content="https://1.bp.blogspot.com/-fHSXEBQAp_8/XkQ7uzz9jzI/AAAAAAAAFSI/FKoaYnuntS8vA9IhQCUduBrRyrEnjtVhQCEwYBhgL/s640/image8.gif">
<meta property="og:updated_time" content="2020-09-09T12:57:09.656Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="paperUnderstanding-ClearGrasp">
<meta name="twitter:description" content="该论文理解与复现">
<meta name="twitter:image" content="https://1.bp.blogspot.com/-VNB-xPguois/XkQ7sqF3VWI/AAAAAAAAFRc/ek6wUpjOGNw0u9vALnnW9zXCZ8VsWTjcACLcBGAsYHQ/s640/image3.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"right","display":"post","offset":12,"b2t":false,"scrollpercent":true,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://pabebezz.github.io/article/2d17733a/">


<meta name="google-site-verification" content="skUdQN9cQrZuUJydKG0ivzB90q5kyLJMU5i5dt82GH4">
<meta name="google-site-verification" content="hv7_j7TFqTy83_mHkj5EDOJhzIl0_tusCnHJBxNOfAo">






  <title>paperUnderstanding-ClearGrasp | Pabebe's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-right page-post-detail">
    <div class="headband"> 
       
    </div>
 

      <!-- change "fork me on github" -->
     <!--  <a href="https://github.com/Pabebezz" target="_blank" ><img style="position: absolute; top: 0; right: 1200; border: 0;"  width="149" height="149" src="/images/forkme_left_green.png" class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a> -->
     <a href="https://github.com/Pabebezz" class="github-corner" target="_blank" aria-label="View source on GitHub"><svg width="80" height="80" viewbox="0 0 250 250" style="fill: #326755; color:#fff; position: absolute; top: 0; border: 0; left: 0; transform: scale(-1, 1);" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"/><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"/><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"/></svg></a><style>.github-corner:hover .octo-arm{animation:octocat-wave 560ms ease-in-out}@keyframes octocat-wave{0%,100%{transform:rotate(0)}20%,60%{transform:rotate(-25deg)}40%,80%{transform:rotate(10deg)}}@media (max-width:500px){.github-corner:hover .octo-arm{animation:none}.github-corner .octo-arm{animation:octocat-wave 560ms ease-in-out}}</style>
    
       
       <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Pabebe's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">业精于勤，荒于嬉；行成于思，毁于随。</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>





 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://pabebezz.github.io/article/2d17733a/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Pabebe">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Pabebe's Blog">
    </span>

    
      <header class="post-header">
        

        
        
          <h1 class="post-title" itemprop="name headline">paperUnderstanding-ClearGrasp</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2020-06-28T13:32:16+08:00">
                2020-06-28
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Scientific-Research/" itemprop="url" rel="index">
                    <span itemprop="name">Scientific Research</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          
           
           
            <span class="post-meta-divider">|</span>
            <span id="busuanzi_value_page_pv"></span>次阅读
            

          
            <div class="post-wordcount">
              
                
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">字数统计&#58;</span>
                
                <span title="字数统计">
                  1.8k 字
                </span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">阅读时长 &asymp;</span>
                
                <span title="阅读时长">
                  10 分钟
                </span>
              
            </div>
          

          
              <div class="post-description">
                  该论文理解与复现
              </div>
          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Learning-to-See-Transparent-Objects"><a href="#Learning-to-See-Transparent-Objects" class="headerlink" title="Learning to See Transparent Objects"></a><a href="http://ai.googleblog.com/2020/02/learning-to-see-transparent-objects.html" target="_blank" rel="noopener">Learning to See Transparent Objects</a></h1><p>Wednesday, February 12, 2020</p>
<p>Posted by Shreeyak Sajjan, Research Engineer, Synthesis AI and Andy Zeng, Research Scientist, Robotics at Google</p>
<p>Optical 3D range sensors, like <a href="https://rosindustrial.org/3d-camera-survey" target="_blank" rel="noopener">RGB-D cameras</a> and <a href="https://en.wikipedia.org/wiki/Lidar" target="_blank" rel="noopener">LIDAR</a>, have found widespread use in robotics to generate rich and accurate 3D maps of the environment, from self-driving cars to <a href="https://ai.googleblog.com/2019/03/unifying-physics-and-deep-learning-with.html" target="_blank" rel="noopener">autonomous manipulators</a>. However, despite the ubiquity of these complex robotic systems, transparent objects (like a glass container) can confound even a suite of expensive sensors that are commonly used. This is because optical 3D sensors are driven by algorithms that assume all surfaces are <a href="https://en.wikipedia.org/wiki/Lambertian_reflectance" target="_blank" rel="noopener">Lambertian</a>, i.e., they reflect light evenly in all directions, resulting in a uniform surface brightness from all viewing angles. However, transparent objects violate this assumption, since their surfaces both refract and reflect light. Hence, most of the depth data from transparent objects are invalid or contain unpredictable noise.</p>
<table>
<thead>
<tr>
<th><a href="https://1.bp.blogspot.com/-VNB-xPguois/XkQ7sqF3VWI/AAAAAAAAFRc/ek6wUpjOGNw0u9vALnnW9zXCZ8VsWTjcACLcBGAsYHQ/s1600/image3.png" target="_blank" rel="noopener"><img src="https://1.bp.blogspot.com/-VNB-xPguois/XkQ7sqF3VWI/AAAAAAAAFRc/ek6wUpjOGNw0u9vALnnW9zXCZ8VsWTjcACLcBGAsYHQ/s640/image3.png" alt="img"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://1.bp.blogspot.com/-mcdDuutDw5Q/XkQ7ssTGUvI/AAAAAAAAFRg/OIyqAa7Qbgs1yGWB7vFKL2E90hCPXiuYACEwYBhgL/s1600/image2.gif" target="_blank" rel="noopener"><img src="https://1.bp.blogspot.com/-mcdDuutDw5Q/XkQ7ssTGUvI/AAAAAAAAFRg/OIyqAa7Qbgs1yGWB7vFKL2E90hCPXiuYACEwYBhgL/s640/image2.gif" alt="img"></a></td>
</tr>
<tr>
<td>Transparent objects often fail to be detected by optical 3D sensors. <strong>Top, Right:</strong> For instance, glass bottles do not show up in the <a href="https://en.wikipedia.org/wiki/Range_imaging" target="_blank" rel="noopener">3D depth imagery</a> captured from an Intel® RealSense™ D415 RGB-D camera. <strong>Bottom:</strong> A 3D visualization via <a href="https://en.wikipedia.org/wiki/Point_cloud" target="_blank" rel="noopener">point clouds</a> constructed from the depth image.</td>
</tr>
</tbody>
</table>
<p>Enabling machines to better sense transparent surfaces would not only improve safety, but could also open up a range of new interactions in unstructured applications — from robots handling kitchenware or sorting plastics for recycling, to navigating indoor environments or generating AR visualizations on glass tabletops.</p>
<p>To address this problem, we teamed up with researchers from <a href="http://synthesis.ai/" target="_blank" rel="noopener">Synthesis AI</a> and <a href="https://shurans.github.io/group.html" target="_blank" rel="noopener">Columbia University</a> to develop <a href="https://sites.google.com/view/cleargrasp" target="_blank" rel="noopener">ClearGrasp</a>, a machine learning algorithm that is capable of estimating accurate 3D data of transparent objects from RGB-D images. This is made possible by a <a href="https://sites.google.com/view/transparent-objects" target="_blank" rel="noopener">large-scale synthetic dataset</a> that we are also releasing publicly today. ClearGrasp can work with inputs from any standard RGB-D camera, using deep learning to accurately reconstruct the depth of transparent objects and generalize to completely new objects unseen during training. This in contrast to <a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Qian_3D_Reconstruction_of_CVPR_2016_paper.pdf" target="_blank" rel="noopener">previous methods</a>, which required prior knowledge of the transparent objects (e.g., their 3D models), often combined with maps of background lighting and camera positions. In this work, we also demonstrate that ClearGrasp can benefit robotic manipulation by incorporating it into our <a href="https://ai.googleblog.com/2019/10/learning-to-assemble-and-to-generalize.html" target="_blank" rel="noopener">pick and place robot</a>’s control system, where we observe significant improvements in the grasping success rate of transparent plastic objects.</p>
<table>
<thead>
<tr>
<th><a href="https://1.bp.blogspot.com/-y-DPUgVcfSk/XkQ7seO_nII/AAAAAAAAFR4/W2yq1dWQ2YkglMA5IguxyG1vJRRPg235gCEwYBhgL/s1600/image4.jpg" target="_blank" rel="noopener"><img src="https://1.bp.blogspot.com/-y-DPUgVcfSk/XkQ7seO_nII/AAAAAAAAFR4/W2yq1dWQ2YkglMA5IguxyG1vJRRPg235gCEwYBhgL/s640/image4.jpg" alt="img"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td><a href="https://1.bp.blogspot.com/-r9_NUznjObE/XkQ7teoA4BI/AAAAAAAAFSA/xNDlyrAzI-0Ys7iT2iF40AkaeaSjvbuFQCEwYBhgL/s1600/image5.gif" target="_blank" rel="noopener"><img src="https://1.bp.blogspot.com/-r9_NUznjObE/XkQ7teoA4BI/AAAAAAAAFSA/xNDlyrAzI-0Ys7iT2iF40AkaeaSjvbuFQCEwYBhgL/s640/image5.gif" alt="img"></a></td>
</tr>
<tr>
<td>ClearGrasp uses deep learning to recover accurate 3D depth data of transparent surfaces.</td>
</tr>
</tbody>
</table>
<h2 id="A-Visual-Dataset-of-Transparent-Objects"><a href="#A-Visual-Dataset-of-Transparent-Objects" class="headerlink" title="A Visual Dataset of Transparent Objects"></a><strong>A Visual Dataset of Transparent Objects</strong></h2><p>Massive quantities of data are required to train any effective deep learning model (e.g., <a href="http://www.image-net.org/" target="_blank" rel="noopener">ImageNet</a> for vision or <a href="https://www.wikipedia.org/" target="_blank" rel="noopener">Wikipedia</a> for <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html" target="_blank" rel="noopener">BERT</a>), and ClearGrasp is no exception. Unfortunately, no datasets are available with 3D data of transparent objects. Existing 3D datasets like <a href="https://niessner.github.io/Matterport/" target="_blank" rel="noopener">Matterport3D</a> or <a href="http://www.scan-net.org/" target="_blank" rel="noopener">ScanNet</a> overlook transparent surfaces, because they require expensive and time-consuming labeling processes.</p>
<p>To overcome this issue, we created our own <a href="https://sites.google.com/view/transparent-objects" target="_blank" rel="noopener">large-scale dataset of transparent objects</a> that contains more than 50,000 photorealistic renders with corresponding <a href="https://en.wikipedia.org/wiki/Normal_(geometry" target="_blank" rel="noopener">surface normals</a>) (representing the surface curvature), segmentation masks, edges, and depth, useful for training a variety of 2D and 3D detection tasks. Each image contains up to five transparent objects, either on a flat ground plane or inside a tote, with various backgrounds and lighting.</p>
<table>
<thead>
<tr>
<th><a href="https://1.bp.blogspot.com/-oncutBVkLQU/XkQ7rQz4zLI/AAAAAAAAFSI/Y1dSYj38GkgDTtw-P-UaP1e7pRzmC6izgCEwYBhgL/s1600/image11.png" target="_blank" rel="noopener"><img src="https://1.bp.blogspot.com/-oncutBVkLQU/XkQ7rQz4zLI/AAAAAAAAFSI/Y1dSYj38GkgDTtw-P-UaP1e7pRzmC6izgCEwYBhgL/s640/image11.png" alt="img"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td>Some example data of transparent objects from the <a href="https://sites.google.com/view/cleargrasp/synthetic-dataset" target="_blank" rel="noopener">ClearGrasp synthetic dataset</a>.</td>
</tr>
</tbody>
</table>
<p>We also include a test set of 286 real-world images with corresponding ground truth depth. The real-world images were taken by a painstaking process of replacing each transparent object in the scene with a painted one in the same pose. The images are captured under a number of different indoor lighting conditions, using various cloth and veneer backgrounds and containing random opaque objects scattered around the scene. They contain both known objects, present in the synthetic training set, and novel objects.</p>
<table>
<thead>
<tr>
<th><a href="https://1.bp.blogspot.com/-Ds5KPDwGazc/XkQ7t4atuLI/AAAAAAAAFSA/bnqbcZpPeO8BK8Ewr0if2oTOtJmM2WoiwCEwYBhgL/s1600/image6.gif" target="_blank" rel="noopener"><img src="https://1.bp.blogspot.com/-Ds5KPDwGazc/XkQ7t4atuLI/AAAAAAAAFSA/bnqbcZpPeO8BK8Ewr0if2oTOtJmM2WoiwCEwYBhgL/s640/image6.gif" alt="img"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Left:</strong> The real-world image capturing setup, <strong>Middle:</strong> Custom user interface enables precisely replacing each transparent object with a spray-painted duplicate, <strong>Right:</strong> Example of captured data.</td>
</tr>
</tbody>
</table>
<h2 id="The-Challenge"><a href="#The-Challenge" class="headerlink" title="The Challenge"></a><strong>The Challenge</strong></h2><p>While the distorted view of the background seen through transparent objects confounds typical depth estimation approaches, there are clues that hint at the objects’ shape. Transparent surfaces exhibit specular reflections, which are mirror-like reflections that show up as bright spots in a well-lit environment. Since these visual cues are prominent in RGB images and are influenced primarily by the shape of the objects, convolutional neural networks can use these reflections to infer accurate surface normals, which then can be used for depth estimation.</p>
<table>
<thead>
<tr>
<th><a href="https://1.bp.blogspot.com/-QcyTESfw4AM/XkQ7rJaGdVI/AAAAAAAAFR8/CUPvkzCJuygr6xw9S36vYIQ1r0wfi3rogCEwYBhgL/s1600/image10.jpg" target="_blank" rel="noopener"><img src="https://1.bp.blogspot.com/-QcyTESfw4AM/XkQ7rJaGdVI/AAAAAAAAFR8/CUPvkzCJuygr6xw9S36vYIQ1r0wfi3rogCEwYBhgL/s400/image10.jpg" alt="img"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td>Specular reflections on transparent objects create distinct features that vary based on the object shape and provide strong visual cues for estimating surface normals.</td>
</tr>
</tbody>
</table>
<p>Most machine learning algorithms try to directly estimate depth from a monocular RGB image. However, monocular depth estimation is an ill-posed task, even for humans. We observed large errors in estimating the depth of flat background surfaces, which compounds the error in depth estimates for the transparent objects resting atop them. Therefore, rather than directly estimating the depth of all geometry, we conjectured that correcting the initial depth estimates from an <a href="https://rosindustrial.org/3d-camera-survey" target="_blank" rel="noopener">RGB-D 3D camera</a> is more practical — it would enable us to use the depth from the non-transparent surfaces to inform the depth of transparent surfaces.</p>
<h2 id="The-ClearGrasp-Algorithm"><a href="#The-ClearGrasp-Algorithm" class="headerlink" title="The ClearGrasp Algorithm"></a><strong>The ClearGrasp Algorithm</strong></h2><p>ClearGrasp uses 3 neural networks: a network to estimate surface normals, one for occlusion boundaries (depth discontinuities), and one that masks transparent objects. The mask is used to remove all pixels belonging to transparent objects, so that the correct depths can be filled in. We then use a global optimization module that starts extending the depth from known surfaces, using the predicted surface normals to guide the shape of the reconstruction, and the predicted occlusion boundaries to maintain the separation between distinct objects.</p>
<table>
<thead>
<tr>
<th><a href="https://1.bp.blogspot.com/-xbiK9-km_BE/XkQ7rJmME0I/AAAAAAAAFSA/2kRSqd3DvlcOpv6ql4qILwn3HdmmryfBACEwYBhgL/s1600/image1.png" target="_blank" rel="noopener"><img src="https://1.bp.blogspot.com/-xbiK9-km_BE/XkQ7rJmME0I/AAAAAAAAFSA/2kRSqd3DvlcOpv6ql4qILwn3HdmmryfBACEwYBhgL/s640/image1.png" alt="img"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td>Overview of our method. The point cloud was generated using the output depth and is colored with its surface normals.</td>
</tr>
</tbody>
</table>
<p>Each of the neural networks was trained on our synthetic dataset and they performed well on real-world transparent objects. However, the surface normal estimations for other surfaces, like walls or fruits, were poor. This is because of the limitations of our synthetic dataset, which contains only transparent objects on a ground plane. To alleviate this issue, we included some real indoor scenes from the <a href="https://niessner.github.io/Matterport/" target="_blank" rel="noopener">Matterport3D</a> and <a href="http://www.scan-net.org/" target="_blank" rel="noopener">ScanNet</a> datasets in the surface normals training loop. By training on both the in-domain synthetic dataset and out-of-domain real word dataset, the model performed well on all surfaces in our test set.</p>
<table>
<thead>
<tr>
<th><a href="https://1.bp.blogspot.com/-QBobUQ2ssow/XkQ7txFy4gI/AAAAAAAAFSI/wIR43QtxSd48ghg9bEOAyN617pqIfvsPwCEwYBhgL/s1600/image7.png" target="_blank" rel="noopener"><img src="https://1.bp.blogspot.com/-QBobUQ2ssow/XkQ7txFy4gI/AAAAAAAAFSI/wIR43QtxSd48ghg9bEOAyN617pqIfvsPwCEwYBhgL/s640/image7.png" alt="img"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td>Surface Normal estimation on real images when trained on a) <a href="https://niessner.github.io/Matterport/" target="_blank" rel="noopener">Matterport3D</a> and <a href="http://www.scan-net.org/" target="_blank" rel="noopener">ScanNet</a> only (MP+SN), b) our synthetic dataset only, and c) MP+SN as well as our synthetic dataset. Note how the model trained on MP+SN fails to detect the transparent objects. The model trained on only synthetic data picks up the real plastic bottles remarkably well, but fails for other objects and surfaces. When trained on both, our model gets the best of both worlds.</td>
</tr>
</tbody>
</table>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a><strong>Results</strong></h2><p>Overall, our <a href="https://arxiv.org/pdf/1910.02550.pdf" target="_blank" rel="noopener">quantitative experiments</a> show that ClearGrasp is able to reconstruct depth for transparent objects with much higher fidelity than <a href="https://arxiv.org/abs/1812.11941" target="_blank" rel="noopener">alternative</a> <a href="https://arxiv.org/abs/1803.09326" target="_blank" rel="noopener">methods</a>. Despite being trained on only synthetic transparent objects, we find our models are able to adapt well to the real-world domain — achieving very similar quantitative reconstruction performance on known objects across domains. Our models also generalize well to novel objects with complex shapes never seen before.</p>
<p>To check the qualitative performance of ClearGrasp, we construct 3D point clouds from the input and output depth images, as shown below (additional examples available on the <a href="https://sites.google.com/view/cleargrasp/results" target="_blank" rel="noopener">project webpage</a>). The resulting estimated 3D surfaces have clean and coherent reconstructed shapes — important for applications, such as 3D mapping and 3D object detection — without the jagged noise seen in monocular depth estimation methods. Our models are robust and perform well in challenging conditions, such as identifying transparent objects situated in a patterned background or differentiating between transparent objects partially occluding one another.</p>
<table>
<thead>
<tr>
<th><a href="https://1.bp.blogspot.com/-iaTKEeKdlTA/XkQ7u6Mv6gI/AAAAAAAAFSE/_59HIFZptyoP9tDllG_0WVy2HKD4edPcwCEwYBhgL/s1600/image9.png" target="_blank" rel="noopener"><img src="https://1.bp.blogspot.com/-iaTKEeKdlTA/XkQ7u6Mv6gI/AAAAAAAAFSE/_59HIFZptyoP9tDllG_0WVy2HKD4edPcwCEwYBhgL/s640/image9.png" alt="img"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td>Qualitative results on real images. <strong>Top two rows:</strong> results on known objects. <strong>Bottom two rows:</strong> results on novel objects. The point clouds, colored with their surface normals, are generated from the corresponding depth images.</td>
</tr>
</tbody>
</table>
<p>Most importantly, the output depth from ClearGrasp can be directly used as input to state-of-the-art manipulation algorithms that use RGB-D images. By using ClearGrasp’s output depth estimates instead of the raw sensor data, our <a href="https://ai.googleblog.com/2019/10/learning-to-assemble-and-to-generalize.html" target="_blank" rel="noopener">grasping algorithm</a> on a <a href="https://www.universal-robots.com/products/ur5-robot/" target="_blank" rel="noopener">UR5 robot arm</a> saw significant improvements in the grasping success rates of transparent objects. When using the parallel-jaw gripper, the success rate improved from a baseline of 12% to 74%, and from 64% to 86% with suction.</p>
<table>
<thead>
<tr>
<th><a href="https://1.bp.blogspot.com/-fHSXEBQAp_8/XkQ7uzz9jzI/AAAAAAAAFSI/FKoaYnuntS8vA9IhQCUduBrRyrEnjtVhQCEwYBhgL/s1600/image8.gif" target="_blank" rel="noopener"><img src="https://1.bp.blogspot.com/-fHSXEBQAp_8/XkQ7uzz9jzI/AAAAAAAAFSI/FKoaYnuntS8vA9IhQCUduBrRyrEnjtVhQCEwYBhgL/s640/image8.gif" alt="img"></a></th>
</tr>
</thead>
<tbody>
<tr>
<td>Manipulation of novel transparent objects using ClearGrasp. Note the challenging conditions: textureless background, complex object shapes and the directional light causing confusing shadows and <a href="https://en.wikipedia.org/wiki/Caustic_(optics" target="_blank" rel="noopener">caustics</a>) (the patterns of light that occur when light rays are reflected or refracted from a surface).</td>
</tr>
</tbody>
</table>
<h2 id="Limitations-amp-Future-Work"><a href="#Limitations-amp-Future-Work" class="headerlink" title="Limitations &amp; Future Work"></a><strong>Limitations &amp; Future Work</strong></h2><p>A limitation of our synthetic dataset is that it does not represent accurate <a href="https://en.wikipedia.org/wiki/Caustic_(optics" target="_blank" rel="noopener">caustics</a>), due to the limitations of rendering with traditional path-tracing algorithms. As a result, our models confuse bright caustics coupled with shadows to be independent transparent objects. Despite these drawbacks, our work with ClearGrasp shows that synthetic data remains a viable approach to achieve competent results for learning-based depth reconstruction methods. A promising direction for future work is improving the domain transfer to real-world images by generating renders with physically-correct caustics and surface imperfections such as fingerprints.</p>
<h1 id="运行项目"><a href="#运行项目" class="headerlink" title="运行项目"></a>运行项目</h1><p><a href="https://github.com/Shreeyak/cleargrasp" target="_blank" rel="noopener">https://github.com/Shreeyak/cleargrasp</a></p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python eval_depth_completion.py -c <span class="built_in">config</span>/<span class="built_in">config</span>.yaml</span><br></pre></td></tr></table></figure>
<h1 id="bug"><a href="#bug" class="headerlink" title="bug"></a>bug</h1><p><strong>无法定位软件包 libhdf5-10</strong></p>
<p><strong>无法定位软件包 libhdf5-cpp-11</strong></p>
<p>因为Ubuntu18.04源没有对应包，只有Ubuntu16.04有，所以在</p>
<p>/etc/apt/sources.list 文件中像下面这样添加一行:</p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">deb <span class="string">http:</span><span class="comment">//security.ubuntu.com/ubuntu xenial-security main universe</span></span><br></pre></td></tr></table></figure>
<p><a href="https://packages.ubuntu.com/xenial/amd64/libhdf5-10/download" target="_blank" rel="noopener">用在 AMD64 上 libhdf5-10_1.8.16+docs-4ubuntu1.1_amd64.deb 的下载页面</a></p>
<p><strong>fatal error: GL/glu.h: 没有那个文件或目录</strong></p>
<p><strong>include &lt;GL/glu.h&gt;</strong></p>
<figure class="highlight q"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="built_in">get</span>  install libglu-<span class="built_in">dev</span></span><br></pre></td></tr></table></figure>
<p><strong>g++: warning: /usr/include/hdf5/serial/ linker input file unused because linking notdone</strong></p>
<figure class="highlight gauss"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo <span class="built_in">make</span> clean</span><br><span class="line">sudo <span class="built_in">make</span></span><br></pre></td></tr></table></figure>
<p><strong>depth2depth.cpp:11:10: fatal error: hdf5.h: 没有那个文件或目录#include “hdf5.h”</strong></p>
<p>确认环境配置好。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd api/depth2depth/gaps</span><br><span class="line">export CPATH="/usr/include/hdf5/serial/"  # Ensure this path is same as read from output of `find /usr -iname "*hdf5.h*"`</span><br><span class="line">echo $CPATH</span><br></pre></td></tr></table></figure>
<h1 id="附加"><a href="#附加" class="headerlink" title="附加"></a>附加</h1><p>depth2depth algorithm-<a href="http://deepcompletion.cs.princeton.edu/" target="_blank" rel="noopener">Deep Depth Completion of a Single RGB-D Image</a></p>

      
    </div>
    
    
    

    <div>
    
        
<div style="text-align:center;color: #ccc;font-size:14px;">----------------  本文结束  ----------------</div>
<br>
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>
  
  <!-- JS库 sweetalert 可修改路径 -->
  <script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script>
  <script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script>
  <link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css">

  <p><span>本文标题:</span>paperUnderstanding-ClearGrasp</p>
  <p><span>文章作者:</span>Pabebe</p>
  <p><span>发布时间:</span>2020年06月28日 - 13:32:16</p>
  <p><span>最后更新:</span>2020年09月09日 - 20:57:09</p>
  <p><span>原始链接:</span><a href="/article/2d17733a/" title="paperUnderstanding-ClearGrasp">https://pabebezz.github.io/article/2d17733a/</a>
    <span class="copy-path" title="点击复制文章链接"><i class="fa fa-clipboard" data-clipboard-text="https://pabebezz.github.io/article/2d17733a/" aria-label="复制成功！"></i></span>
  </p>
  <p><span>许可协议:</span><i class="fa fa-creative-commons"></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)">署名-非商业性使用-禁止演绎 4.0 国际</a> 转载请保留原文链接及作者。</p>  
</div>
<script> 
    var clipboard = new Clipboard('.fa-clipboard');
    clipboard.on('success', $(function(){
      $(".fa-clipboard").click(function(){
        swal({   
          title: "",   
          text: '复制成功',   
          html: false,
          timer: 500,   
          showConfirmButton: false
        });
      });
    }));  
</script>


    
</div>


    

    

    
    
   <!-- 添加版权声明 -->
    <div>
    
        
     
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/paper/" rel="tag"><i class="menu-item-icon fa fa-fw fa-tags"></i>&nbsp;paper</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/article/79acc1fa/" rel="next" title="Ubuntu各种命令详解">
                <i class="fa fa-chevron-left"></i> Ubuntu各种命令详解
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/article/e0fdde43/" rel="prev" title="安装Ubuntu18.04后的优化">
                安装Ubuntu18.04后的优化 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="Pabebe">
            
              <p class="site-author-name" itemprop="name">Pabebe</p>
              <p class="site-description motion-element" itemprop="description">I have the soulful eyes of a cow</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">68</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">18</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/Pabebezz" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://blog.csdn.net/Pabebe" target="_blank" title="CSDN">
                      
                        <i class="fa fa-fw fa-globe"></i>CSDN</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zezuwang@qq.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
            </div>
          

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://example.com/" title="Title" target="_blank">Title</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Learning-to-See-Transparent-Objects"><span class="nav-number">1.</span> <span class="nav-text">Learning to See Transparent Objects</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#A-Visual-Dataset-of-Transparent-Objects"><span class="nav-number">1.1.</span> <span class="nav-text">A Visual Dataset of Transparent Objects</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Challenge"><span class="nav-number">1.2.</span> <span class="nav-text">The Challenge</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-ClearGrasp-Algorithm"><span class="nav-number">1.3.</span> <span class="nav-text">The ClearGrasp Algorithm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Results"><span class="nav-number">1.4.</span> <span class="nav-text">Results</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Limitations-amp-Future-Work"><span class="nav-number">1.5.</span> <span class="nav-text">Limitations &amp; Future Work</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#运行项目"><span class="nav-number">2.</span> <span class="nav-text">运行项目</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#bug"><span class="nav-number">3.</span> <span class="nav-text">bug</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#附加"><span class="nav-number">4.</span> <span class="nav-text">附加</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Pabebe</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    
      <span class="post-meta-item-text">本站总字数&#58;</span>
    
    <span title="本站总字数">138.8k</span> 

    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-heart"></i>
    </span>
    <span id="busuanzi_container_site_pv" style="display:none">
    总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
    </span>
    <span class="post-meta-item-icon">
      <i class="fa fa-user-md"></i>
    </span>
    <span id="busuanzi_container_site_uv" style="display:none">
    有<span id="busuanzi_value_site_uv"></span>人到访
    </span>

  
</div>

<div>

</div>

<!--  
  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div> 



   <span class="post-meta-divider">|</span>
 

 
 <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Mist</a> v5.1.4</div> 
 -->




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
          <span id="scrollpercent"><span>0</span>%</span>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>

<!-- 页面点击出现文字 -->
 <script type="text/javascript" src="/js/src/myLittleWits.js"></script>
 
</html>
